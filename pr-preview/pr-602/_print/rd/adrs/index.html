<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><link rel=canonical type=text/html href=https://zaba505.github.io/infra/pr-preview/pr-602/rd/adrs/><link rel=alternate type=application/rss+xml href=https://zaba505.github.io/infra/pr-preview/pr-602/rd/adrs/index.xml><meta name=robots content="noindex, nofollow"><link rel="shortcut icon" href=/infra/pr-preview/pr-602/favicons/favicon.ico><link rel=apple-touch-icon href=/infra/pr-preview/pr-602/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=icon type=image/png href=/infra/pr-preview/pr-602/favicons/favicon-16x16.png sizes=16x16><link rel=icon type=image/png href=/infra/pr-preview/pr-602/favicons/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/infra/pr-preview/pr-602/favicons/android-36x36.png sizes=36x36><link rel=icon type=image/png href=/infra/pr-preview/pr-602/favicons/android-48x48.png sizes=48x48><link rel=icon type=image/png href=/infra/pr-preview/pr-602/favicons/android-72x72.png sizes=72x72><link rel=icon type=image/png href=/infra/pr-preview/pr-602/favicons/android-96x96.png sizes=96x96><link rel=icon type=image/png href=/infra/pr-preview/pr-602/favicons/android-144x144.png sizes=144x144><link rel=icon type=image/png href=/infra/pr-preview/pr-602/favicons/android-192x192.png sizes=192x192><title>Architecture Decision Records | Zaba505's Home Lab</title><meta name=description content="Documentation of architectural decisions made using MADR 4.0.0 standard"><meta property="og:url" content="https://zaba505.github.io/infra/pr-preview/pr-602/rd/adrs/"><meta property="og:site_name" content="Zaba505's Home Lab"><meta property="og:title" content="Architecture Decision Records"><meta property="og:description" content="Documentation of architectural decisions made using MADR 4.0.0 standard"><meta property="og:locale" content="en_us"><meta property="og:type" content="website"><meta itemprop=name content="Architecture Decision Records"><meta itemprop=description content="Documentation of architectural decisions made using MADR 4.0.0 standard"><meta itemprop=dateModified content="2025-11-17T05:21:16+00:00"><meta itemprop=wordCount content="195"><meta name=twitter:card content="summary"><meta name=twitter:title content="Architecture Decision Records"><meta name=twitter:description content="Documentation of architectural decisions made using MADR 4.0.0 standard"><link rel=preload href=/infra/pr-preview/pr-602/scss/main.min.74eef40c5172b0e2f11bd9c3ea40dba66c2dc642ac5294c208f5dc9ff772c0e9.css as=style integrity="sha256-dO70DFFysOLxG9nD6kDbpmwtxkKsUpTCCPXcn/dywOk=" crossorigin=anonymous><link href=/infra/pr-preview/pr-602/scss/main.min.74eef40c5172b0e2f11bd9c3ea40dba66c2dc642ac5294c208f5dc9ff772c0e9.css rel=stylesheet integrity="sha256-dO70DFFysOLxG9nD6kDbpmwtxkKsUpTCCPXcn/dywOk=" crossorigin=anonymous><script src=https://code.jquery.com/jquery-3.7.1.min.js integrity="sha512-v2CJ7UaYy4JwqLDIrZUI/4hqeoQieOmAZNXBeQyjo21dadnwR+8ZaIJVT8EE2iyI61OV8e6M8PP2/4hpQINQ/g==" crossorigin=anonymous></script><script defer src=https://unpkg.com/lunr@2.3.9/lunr.min.js integrity=sha384-203J0SNzyqHby3iU6hzvzltrWi/M41wOP5Gu+BiJMz5nwKykbkUx8Kp7iti0Lpli crossorigin=anonymous></script></head><body class=td-section><header><nav class="td-navbar js-navbar-scroll" data-bs-theme=dark><div class="container-fluid flex-column flex-md-row"><a class=navbar-brand href=/infra/pr-preview/pr-602/><span class="navbar-brand__logo navbar-logo"></span><span class=navbar-brand__name>Zaba505's Home Lab</span></a><div class="td-navbar-nav-scroll ms-md-auto" id=main_navbar><ul class=navbar-nav><li class="td-light-dark-menu nav-item dropdown"><svg class="d-none"><symbol id="check2" viewBox="0 0 16 16"><path d="M13.854 3.646a.5.5.0 010 .708l-7 7a.5.5.0 01-.708.0l-3.5-3.5a.5.5.0 11.708-.708L6.5 10.293l6.646-6.647a.5.5.0 01.708.0z"/></symbol><symbol id="circle-half" viewBox="0 0 16 16"><path d="M8 15A7 7 0 108 1v14zm0 1A8 8 0 118 0a8 8 0 010 16z"/></symbol><symbol id="moon-stars-fill" viewBox="0 0 16 16"><path d="M6 .278a.768.768.0 01.08.858 7.208 7.208.0 00-.878 3.46c0 4.021 3.278 7.277 7.318 7.277.527.0 1.04-.055 1.533-.16a.787.787.0 01.81.316.733.733.0 01-.031.893A8.349 8.349.0 018.344 16C3.734 16 0 12.286.0 7.71.0 4.266 2.114 1.312 5.124.06A.752.752.0 016 .278z"/><path d="M10.794 3.148a.217.217.0 01.412.0l.387 1.162c.173.518.579.924 1.097 1.097l1.162.387a.217.217.0 010 .412l-1.162.387A1.734 1.734.0 0011.593 7.69l-.387 1.162a.217.217.0 01-.412.0l-.387-1.162A1.734 1.734.0 009.31 6.593l-1.162-.387a.217.217.0 010-.412l1.162-.387a1.734 1.734.0 001.097-1.097l.387-1.162zM13.863.099a.145.145.0 01.274.0l.258.774c.115.346.386.617.732.732l.774.258a.145.145.0 010 .274l-.774.258a1.156 1.156.0 00-.732.732l-.258.774a.145.145.0 01-.274.0l-.258-.774a1.156 1.156.0 00-.732-.732l-.774-.258a.145.145.0 010-.274l.774-.258c.346-.115.617-.386.732-.732L13.863.1z"/></symbol><symbol id="sun-fill" viewBox="0 0 16 16"><path d="M8 12a4 4 0 100-8 4 4 0 000 8zM8 0a.5.5.0 01.5.5v2a.5.5.0 01-1 0v-2A.5.5.0 018 0zm0 13a.5.5.0 01.5.5v2a.5.5.0 01-1 0v-2A.5.5.0 018 13zm8-5a.5.5.0 01-.5.5h-2a.5.5.0 010-1h2a.5.5.0 01.5.5zM3 8a.5.5.0 01-.5.5h-2a.5.5.0 010-1h2A.5.5.0 013 8zm10.657-5.657a.5.5.0 010 .707l-1.414 1.415a.5.5.0 11-.707-.708l1.414-1.414a.5.5.0 01.707.0zm-9.193 9.193a.5.5.0 010 .707L3.05 13.657a.5.5.0 01-.707-.707l1.414-1.414a.5.5.0 01.707.0zm9.193 2.121a.5.5.0 01-.707.0l-1.414-1.414a.5.5.0 01.707-.707l1.414 1.414a.5.5.0 010 .707zM4.464 4.465a.5.5.0 01-.707.0L2.343 3.05a.5.5.0 11.707-.707l1.414 1.414a.5.5.0 010 .708z"/></symbol></svg>
<button class="btn btn-link nav-link dropdown-toggle d-flex align-items-center" id=bd-theme type=button aria-expanded=false data-bs-toggle=dropdown data-bs-display=static aria-label="Toggle theme (auto)">
<svg class="bi my-1 theme-icon-active"><use href="#circle-half"/></svg></button><ul class="dropdown-menu dropdown-menu-end" aria-labelledby=bd-theme-text><li><button type=button class="dropdown-item d-flex align-items-center" data-bs-theme-value=light aria-pressed=false>
<svg class="bi me-2 opacity-50"><use href="#sun-fill"/></svg>
Light
<svg class="bi ms-auto d-none"><use href="#check2"/></svg></button></li><li><button type=button class="dropdown-item d-flex align-items-center" data-bs-theme-value=dark aria-pressed=false>
<svg class="bi me-2 opacity-50"><use href="#moon-stars-fill"/></svg>
Dark
<svg class="bi ms-auto d-none"><use href="#check2"/></svg></button></li><li><button type=button class="dropdown-item d-flex align-items-center active" data-bs-theme-value=auto aria-pressed=true>
<svg class="bi me-2 opacity-50"><use href="#circle-half"/></svg>
Auto
<svg class="bi ms-auto d-none"><use href="#check2"/></svg></button></li></ul></li></ul></div><div class="d-none d-lg-block"><div class="td-search td-search--offline"><div class=td-search__icon></div><input type=search class="td-search__input form-control" placeholder="Search this site…" aria-label="Search this site…" autocomplete=off data-offline-search-index-json-src=/infra/pr-preview/pr-602/offline-search-index.eb2046a9a6470803491e973c05ca2bd0.json data-offline-search-base-href=/ data-offline-search-max-results=10></div></div></div></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><main class="col-12 col-md-9 col-xl-8 ps-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>This is the multi-page printable view of this section.
<a href=# onclick="return print(),!1">Click here to print</a>.</p><p><a href=/infra/pr-preview/pr-602/rd/adrs/>Return to the regular view of this page</a>.</p></div><h1 class=title>Architecture Decision Records</h1><div class=lead>Documentation of architectural decisions made using MADR 4.0.0 standard</div><ul><li>1: <a href=#pg-be14bb13e79709979af80fa8e61452c5>[0001] Use MADR for Architecture Decision Records</a></li><li>2: <a href=#pg-79e3b41ba15ba3a179ad4b5df5f3f2fe>[0002] Network Boot Architecture for Home Lab</a></li><li>3: <a href=#pg-853f5ec590d44937c1ebd528cd046965>[0003] Cloud Provider Selection for Network Boot Infrastructure</a></li><li>4: <a href=#pg-dab026fd06f98a03a459f97073d21662>[0004] Server Operating System Selection</a></li><li>5: <a href=#pg-be9e21cdab9183bad0c60fa6e3ba225b>[0005] Network Boot Infrastructure Implementation on Google Cloud</a></li></ul><div class=content><h2 id=architecture-decision-records-adrs>Architecture Decision Records (ADRs)</h2><p>This section contains architectural decision records that document the key design choices made. Each ADR follows the MADR 4.0.0 format and includes:</p><ul><li>Context and problem statement</li><li>Decision drivers and constraints</li><li>Considered options with pros and cons</li><li>Decision outcome and rationale</li><li>Consequences (positive and negative)</li><li>Confirmation methods</li></ul><h3 id=adr-categories>ADR Categories</h3><p>ADRs are classified into three categories:</p><ul><li><strong>Strategic</strong> - High-level architectural decisions affecting the entire system (frameworks, authentication strategies, cross-cutting patterns). Use for foundational technology choices.</li><li><strong>User Journey</strong> - Decisions solving specific user journey problems. More tactical than strategic, but still architectural. Use when evaluating approaches to implement user-facing features.</li><li><strong>API Design</strong> - API endpoint implementation decisions (pagination, filtering, bulk operations). Use for significant API design trade-offs that warrant documentation.</li></ul><h3 id=status-values>Status Values</h3><p>Each ADR has a status that reflects its current state:</p><ul><li><code>proposed</code> - Decision is under consideration</li><li><code>accepted</code> - Decision has been approved and should be implemented</li><li><code>rejected</code> - Decision was considered but not approved</li><li><code>deprecated</code> - Decision is no longer relevant or has been superseded</li><li><code>superseded by ADR-XXXX</code> - Decision has been replaced by a newer ADR</li></ul><p>These records provide historical context for architectural decisions and help ensure consistency across the platform.</p></div></div><div class=td-content style=page-break-before:always><h1 id=pg-be14bb13e79709979af80fa8e61452c5>1 - [0001] Use MADR for Architecture Decision Records</h1><div class=lead>Adopt Markdown Architectural Decision Records (MADR) as the standard format for documenting architectural decisions in the project.</div><h2 id=context-and-problem-statement>Context and Problem Statement</h2><p>As the project grows, architectural decisions are made that have long-term impacts on the system&rsquo;s design, maintainability, and scalability. Without a structured way to document these decisions, we risk losing the context and rationale behind important choices, making it difficult for current and future team members to understand why certain approaches were taken.</p><p>How should we document architectural decisions in a way that is accessible, maintainable, and provides sufficient context for future reference?</p><h2 id=decision-drivers>Decision Drivers</h2><ul><li>Need for clear documentation of architectural decisions and their rationale</li><li>Easy accessibility and searchability of past decisions</li><li>Low barrier to entry for creating and maintaining decision records</li><li>Integration with existing documentation workflow</li><li>Version control friendly format</li><li>Industry-standard approach that team members may already be familiar with</li></ul><h2 id=considered-options>Considered Options</h2><ul><li>MADR (Markdown Architectural Decision Records)</li><li>ADR using custom format</li><li>Wiki-based documentation</li><li>No formal ADR process</li></ul><h2 id=decision-outcome>Decision Outcome</h2><p>Chosen option: &ldquo;MADR (Markdown Architectural Decision Records)&rdquo;, because it provides a well-established, standardized format that is lightweight, version-controlled, and integrates seamlessly with our existing documentation structure. MADR 4.0.0 offers a clear template that captures all necessary information while remaining flexible enough for different types of decisions.</p><h3 id=consequences>Consequences</h3><ul><li>Good, because MADR is a widely adopted standard with clear documentation and examples</li><li>Good, because markdown files are easy to create, edit, and review through pull requests</li><li>Good, because ADRs will be version-controlled alongside code, maintaining historical context</li><li>Good, because the format is flexible enough to accommodate strategic, user-journey, and API design decisions</li><li>Good, because team members can easily search and reference past decisions</li><li>Neutral, because requires discipline to maintain and update ADR status as decisions evolve</li><li>Bad, because team members need to learn and follow the MADR format conventions</li></ul><h3 id=confirmation>Confirmation</h3><p>Compliance will be confirmed through:</p><ul><li>Code reviews ensuring new architectural decisions are documented as ADRs</li><li>ADRs are stored in <code>docs/content/r&amp;d/adrs/</code> following the naming convention <code>NNNN-title-with-dashes.md</code></li><li>Regular reviews during architecture discussions to reference and update existing ADRs</li></ul><h2 id=pros-and-cons-of-the-options>Pros and Cons of the Options</h2><h3 id=madr-markdown-architectural-decision-records>MADR (Markdown Architectural Decision Records)</h3><p>MADR 4.0.0 is a standardized format for documenting architectural decisions using markdown.</p><ul><li>Good, because it&rsquo;s a well-established standard with extensive documentation</li><li>Good, because markdown is simple, portable, and version-control friendly</li><li>Good, because it provides a clear structure while remaining flexible</li><li>Good, because it integrates with static site generators and documentation tools</li><li>Good, because it&rsquo;s lightweight and doesn&rsquo;t require special tools</li><li>Neutral, because it requires some initial learning of the format</li><li>Neutral, because maintaining consistency requires discipline</li></ul><h3 id=adr-using-custom-format>ADR using custom format</h3><p>Create our own custom format for architectural decision records.</p><ul><li>Good, because we can tailor it exactly to our needs</li><li>Bad, because it requires defining and maintaining our own standard</li><li>Bad, because new team members won&rsquo;t be familiar with the format</li><li>Bad, because we lose the benefits of community knowledge and tooling</li><li>Bad, because it may evolve inconsistently over time</li></ul><h3 id=wiki-based-documentation>Wiki-based documentation</h3><p>Use a wiki system (like Confluence, Notion, or GitHub Wiki) to document decisions.</p><ul><li>Good, because wikis provide easy editing and hyperlinking</li><li>Good, because some team members may be familiar with wiki tools</li><li>Neutral, because it may or may not integrate with version control</li><li>Bad, because content may not be version-controlled alongside code</li><li>Bad, because it creates a separate system to maintain</li><li>Bad, because it&rsquo;s harder to review changes through standard PR process</li><li>Bad, because portability and long-term accessibility may be concerns</li></ul><h3 id=no-formal-adr-process>No formal ADR process</h3><p>Continue without a structured approach to documenting architectural decisions.</p><ul><li>Good, because it requires no additional overhead</li><li>Bad, because context and rationale for decisions are lost over time</li><li>Bad, because new team members struggle to understand why decisions were made</li><li>Bad, because it leads to repeated discussions of previously settled questions</li><li>Bad, because it makes it difficult to track when decisions should be revisited</li></ul><h2 id=more-information>More Information</h2><ul><li>MADR 4.0.0 specification: <a href=https://adr.github.io/madr/>https://adr.github.io/madr/</a></li><li>ADRs will be categorized as: strategic, user-journey, or api-design</li><li>ADR status values: proposed | accepted | rejected | deprecated | superseded by ADR-XXXX</li><li>All ADRs are stored in <code>docs/content/r&amp;d/adrs/</code> directory</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-79e3b41ba15ba3a179ad4b5df5f3f2fe>2 - [0002] Network Boot Architecture for Home Lab</h1><div class=lead>Evaluate options for network booting servers in a home lab environment, considering local vs cloud-hosted boot servers.</div><h2 id=context-and-problem-statement>Context and Problem Statement</h2><p>When setting up a home lab infrastructure, servers need to be provisioned and booted over the network using PXE (Preboot Execution Environment). This requires a TFTP/HTTP server to serve boot files to requesting machines. The question is: where should this boot server be hosted to balance security, reliability, cost, and operational complexity?</p><h2 id=decision-drivers>Decision Drivers</h2><ul><li><strong>Security</strong>: Minimize attack surface and ensure only authorized servers receive boot files</li><li><strong>Reliability</strong>: Boot process should be resilient and not dependent on external network connectivity</li><li><strong>Cost</strong>: Minimize ongoing infrastructure costs</li><li><strong>Complexity</strong>: Keep the operational burden manageable</li><li><strong>Trust Model</strong>: Clear verification of requesting server identity</li></ul><h2 id=considered-options>Considered Options</h2><ul><li>Option 1: TFTP/HTTP server locally on home lab network</li><li>Option 2: TFTP/HTTP server on public cloud (without VPN)</li><li>Option 3: TFTP/HTTP server on public cloud (with VPN)</li></ul><h2 id=decision-outcome>Decision Outcome</h2><p>Chosen option: &ldquo;Option 3: TFTP/HTTP server on public cloud (with VPN)&rdquo;, because:</p><ol><li><strong>No local machine management</strong>: Unlike Option 1, this avoids the need to maintain dedicated local hardware for the boot server, reducing operational overhead</li><li><strong>Secure protocol support</strong>: The VPN tunnel encrypts all traffic, allowing unsecured protocols like TFTP to be used without risk of data exposure over public internet routes (unlike Option 2)</li><li><strong>Cost-effective VPN</strong>: The UDM Pro natively supports WireGuard, enabling a self-managed VPN solution that avoids expensive managed VPN services (~$180-300/year vs ~$540-900/year)</li></ol><h3 id=consequences>Consequences</h3><ul><li>Good, because all traffic is encrypted through WireGuard VPN tunnel</li><li>Good, because boot server is not exposed to public internet (no public attack surface)</li><li>Good, because trust model is simple - subnet validation similar to local option</li><li>Good, because centralized cloud management reduces local maintenance burden</li><li>Good, because boot server remains available even if home lab storage fails</li><li>Good, because UDM Pro&rsquo;s native WireGuard support keeps costs at ~$180-300/year</li><li>Bad, because boot process depends on both internet connectivity and VPN availability</li><li>Bad, because VPN adds latency to boot file transfers</li><li>Bad, because VPN gateway becomes an additional failure point</li><li>Bad, because higher ongoing cost compared to local-only option (~$180-300/year vs ~$10/year)</li></ul><h3 id=confirmation>Confirmation</h3><p>The implementation will be confirmed by:</p><ul><li>Successfully network booting a test server using the chosen architecture</li><li>Validating the trust model prevents unauthorized boot requests</li><li>Measuring actual costs against estimates</li></ul><h2 id=pros-and-cons-of-the-options>Pros and Cons of the Options</h2><h3 id=option-1-tftphttp-server-locally-on-home-lab-network>Option 1: TFTP/HTTP server locally on home lab network</h3><p>Run the boot server on local infrastructure (e.g., Raspberry Pi, dedicated VM, or container) within the home lab network.</p><h4 id=boot-flow-sequence>Boot Flow Sequence</h4><pre class=mermaid>sequenceDiagram
    participant Server as Home Lab Server
    participant DHCP as Local DHCP Server
    participant Boot as Local TFTP/HTTP Server

    Server-&gt;&gt;DHCP: PXE Boot Request (DHCP Discover)
    DHCP-&gt;&gt;Server: DHCP Offer with Boot Server IP
    Server-&gt;&gt;Boot: TFTP Request for Boot File
    Boot-&gt;&gt;Boot: Verify MAC/IP against allowlist
    Boot-&gt;&gt;Server: Send iPXE/Boot Loader
    Server-&gt;&gt;Boot: HTTP Request for Kernel/Initrd
    Boot-&gt;&gt;Server: Send Boot Files
    Server-&gt;&gt;Server: Boot into OS</pre><h4 id=trust-model>Trust Model</h4><ul><li><strong>MAC Address Allowlist</strong>: Maintain a list of known server MAC addresses</li><li><strong>Network Isolation</strong>: Boot server only accessible from home lab VLAN</li><li><strong>No external exposure</strong>: Traffic never leaves local network</li><li><strong>Physical security</strong>: Relies on physical access control to home lab</li></ul><h4 id=cost-estimate>Cost Estimate</h4><ul><li><strong>Hardware</strong>: ~$50-100 one-time (Raspberry Pi or repurposed hardware)</li><li><strong>Power</strong>: ~$5-10/year (low power consumption)</li><li><strong>Total</strong>: ~$55-110 initial + ~$10/year ongoing</li></ul><h4 id=pros-and-cons>Pros and Cons</h4><ul><li>Good, because no dependency on internet connectivity for booting</li><li>Good, because lowest latency for boot file transfers</li><li>Good, because all data stays within local network (maximum privacy)</li><li>Good, because lowest ongoing cost</li><li>Good, because simple trust model based on network isolation</li><li>Neutral, because requires dedicated local hardware or resources</li><li>Bad, because single point of failure if boot server goes down</li><li>Bad, because requires local maintenance and updates</li></ul><h3 id=option-2-tftphttp-server-on-public-cloud-without-vpn>Option 2: TFTP/HTTP server on public cloud (without VPN)</h3><p>Host the boot server on a cloud provider (AWS, GCP, Azure) and expose it directly to the internet.</p><h4 id=boot-flow-sequence-1>Boot Flow Sequence</h4><pre class=mermaid>sequenceDiagram
    participant Server as Home Lab Server
    participant DHCP as Local DHCP Server
    participant Router as Home Router/NAT
    participant Internet as Internet
    participant Boot as Cloud TFTP/HTTP Server

    Server-&gt;&gt;DHCP: PXE Boot Request (DHCP Discover)
    DHCP-&gt;&gt;Server: DHCP Offer with Cloud Boot Server IP
    Server-&gt;&gt;Router: TFTP Request
    Router-&gt;&gt;Internet: NAT Translation
    Internet-&gt;&gt;Boot: TFTP Request from Home IP
    Boot-&gt;&gt;Boot: Verify source IP &#43; token/certificate
    Boot-&gt;&gt;Internet: Send iPXE/Boot Loader
    Internet-&gt;&gt;Router: Response
    Router-&gt;&gt;Server: Boot Loader
    Server-&gt;&gt;Router: HTTP Request for Kernel/Initrd
    Router-&gt;&gt;Internet: NAT Translation
    Internet-&gt;&gt;Boot: HTTP Request with auth headers
    Boot-&gt;&gt;Boot: Validate request authenticity
    Boot-&gt;&gt;Internet: Send Boot Files
    Internet-&gt;&gt;Router: Response
    Router-&gt;&gt;Server: Boot Files
    Server-&gt;&gt;Server: Boot into OS</pre><h4 id=trust-model-1>Trust Model</h4><ul><li><strong>Source IP Validation</strong>: Restrict to home lab&rsquo;s public IP (dynamic IP is problematic)</li><li><strong>Certificate/Token Authentication</strong>: Embed certificates in initial bootloader</li><li><strong>TLS for HTTP</strong>: All HTTP traffic encrypted</li><li><strong>Challenge-Response</strong>: Boot server can challenge requesting server</li><li><strong>Risk</strong>: TFTP typically unencrypted, vulnerable to interception</li></ul><h4 id=cost-estimate-1>Cost Estimate</h4><ul><li><strong>Cloud VM (t3.micro or equivalent)</strong>: ~$10-15/month</li><li><strong>Data Transfer</strong>: ~$1-5/month (boot files are typically small)</li><li><strong>Static IP</strong>: ~$3-5/month</li><li><strong>Total</strong>: ~$170-300/year</li></ul><h4 id=pros-and-cons-1>Pros and Cons</h4><ul><li>Good, because boot server remains available even if home lab has issues</li><li>Good, because centralized management in cloud console</li><li>Good, because easy to scale or replicate</li><li>Neutral, because requires internet connectivity for every boot</li><li>Bad, because significantly higher ongoing cost</li><li>Bad, because TFTP protocol is inherently insecure over public internet</li><li>Bad, because complex trust model required (IP validation, certificates)</li><li>Bad, because boot process depends on internet availability</li><li>Bad, because higher latency for boot file transfers</li><li>Bad, because public exposure increases attack surface</li></ul><h3 id=option-3-tftphttp-server-on-public-cloud-with-vpn>Option 3: TFTP/HTTP server on public cloud (with VPN)</h3><p>Host the boot server in the cloud but connect the home lab to the cloud via a site-to-site VPN tunnel.</p><h4 id=boot-flow-sequence-2>Boot Flow Sequence</h4><pre class=mermaid>sequenceDiagram
    participant Server as Home Lab Server
    participant DHCP as Local DHCP Server
    participant VPN as VPN Gateway (Home)
    participant CloudVPN as VPN Gateway (Cloud)
    participant Boot as Cloud TFTP/HTTP Server

    Note over VPN,CloudVPN: Site-to-Site VPN Tunnel Established

    Server-&gt;&gt;DHCP: PXE Boot Request (DHCP Discover)
    DHCP-&gt;&gt;Server: DHCP Offer with Boot Server Private IP
    Server-&gt;&gt;VPN: TFTP Request to Private IP
    VPN-&gt;&gt;CloudVPN: Encrypted VPN Tunnel
    CloudVPN-&gt;&gt;Boot: TFTP Request (appears local)
    Boot-&gt;&gt;Boot: Verify source IP from home lab subnet
    Boot-&gt;&gt;CloudVPN: Send iPXE/Boot Loader
    CloudVPN-&gt;&gt;VPN: Encrypted Response
    VPN-&gt;&gt;Server: Boot Loader
    Server-&gt;&gt;VPN: HTTP Request for Kernel/Initrd
    VPN-&gt;&gt;CloudVPN: Encrypted VPN Tunnel
    CloudVPN-&gt;&gt;Boot: HTTP Request
    Boot-&gt;&gt;Boot: Validate subnet membership
    Boot-&gt;&gt;CloudVPN: Send Boot Files
    CloudVPN-&gt;&gt;VPN: Encrypted Response
    VPN-&gt;&gt;Server: Boot Files
    Server-&gt;&gt;Server: Boot into OS</pre><h4 id=trust-model-2>Trust Model</h4><ul><li><strong>VPN Tunnel Encryption</strong>: All traffic encrypted end-to-end</li><li><strong>Private IP Addressing</strong>: Boot server only accessible via VPN</li><li><strong>Subnet Validation</strong>: Verify requests come from trusted home lab subnet</li><li><strong>VPN Authentication</strong>: Strong auth at tunnel level (certificates, pre-shared keys)</li><li><strong>No public exposure</strong>: Boot server has no public IP</li></ul><h4 id=cost-estimate-2>Cost Estimate</h4><ul><li><strong>Cloud VM (t3.micro or equivalent)</strong>: ~$10-15/month</li><li><strong>Data Transfer (VPN)</strong>: ~$5-10/month</li><li><strong>VPN Gateway Service (if using managed)</strong>: ~$30-50/month OR</li><li><strong>Self-managed VPN (WireGuard/OpenVPN)</strong>: ~$0 additional</li><li><strong>Total (self-managed VPN)</strong>: ~$180-300/year</li><li><strong>Total (managed VPN)</strong>: ~$540-900/year</li></ul><h4 id=pros-and-cons-2>Pros and Cons</h4><ul><li>Good, because all traffic encrypted through VPN tunnel</li><li>Good, because boot server not exposed to public internet</li><li>Good, because trust model similar to local option (subnet validation)</li><li>Good, because centralized cloud management benefits</li><li>Good, because boot server available if home lab storage fails</li><li>Neutral, because moderate complexity (VPN setup and maintenance)</li><li>Bad, because higher cost than local option</li><li>Bad, because boot process still depends on internet + VPN availability</li><li>Bad, because VPN adds latency to boot process</li><li>Bad, because VPN gateway becomes additional failure point</li><li>Bad, because most expensive option if using managed VPN service</li></ul><h2 id=more-information>More Information</h2><h3 id=related-resources>Related Resources</h3><ul><li><a href=https://en.wikipedia.org/wiki/Preboot_Execution_Environment>PXE Boot Specification</a></li><li><a href=https://ipxe.org/>iPXE - Open Source Boot Firmware</a></li><li><a href=https://tools.ietf.org/html/rfc1350>TFTP Protocol (RFC 1350)</a></li></ul><h3 id=key-questions-for-decision>Key Questions for Decision</h3><ol><li>How critical is boot availability during internet outages?</li><li>Is the home lab public IP static or dynamic?</li><li>What is the acceptable boot time latency?</li><li>How many servers need to be supported?</li><li>Is there existing VPN infrastructure?</li></ol><h3 id=related-issues>Related Issues</h3><ul><li><a href=https://github.com/Zaba505/infra/issues/595>Issue #595</a> - story(docs): create adr for network boot architecture</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-853f5ec590d44937c1ebd528cd046965>3 - [0003] Cloud Provider Selection for Network Boot Infrastructure</h1><div class=lead>Evaluate Google Cloud Platform vs Amazon Web Services for hosting network boot server infrastructure as required by ADR-0002.</div><h2 id=context-and-problem-statement>Context and Problem Statement</h2><p><a href=./0002-network-boot-architecture/>ADR-0002</a> established that network boot infrastructure will be hosted on a cloud provider and accessed via VPN (specifically WireGuard from the UDM Pro). The decision to use cloud hosting provides resilience against local hardware failures while maintaining security through encrypted VPN tunnels.</p><p>The question now is: <strong>Which cloud provider should host the network boot infrastructure?</strong></p><p>This decision will affect:</p><ul><li><strong>Cost</strong>: Ongoing monthly/annual infrastructure costs</li><li><strong>Protocol Support</strong>: Ability to serve TFTP, HTTP, and HTTPS boot files</li><li><strong>VPN Integration</strong>: Ease of WireGuard deployment and management</li><li><strong>Operational Complexity</strong>: Management overhead and maintenance burden</li><li><strong>Performance</strong>: Boot file transfer latency and throughput</li><li><strong>Vendor Lock-in</strong>: Future flexibility to migrate or multi-cloud</li></ul><h2 id=decision-drivers>Decision Drivers</h2><ul><li><strong>Cost Efficiency</strong>: Minimize ongoing infrastructure costs for home lab scale</li><li><strong>Protocol Support</strong>: Must support TFTP (UDP/69), HTTP (TCP/80), and HTTPS (TCP/443) for network boot workflows</li><li><strong>WireGuard Compatibility</strong>: Must support self-managed WireGuard VPN with reasonable effort</li><li><strong>UDM Pro Integration</strong>: Should work seamlessly with UniFi Dream Machine Pro&rsquo;s native WireGuard client</li><li><strong>Simplicity</strong>: Minimize operational complexity for a single-person home lab</li><li><strong>Existing Expertise</strong>: Leverage existing team knowledge and infrastructure</li><li><strong>Performance</strong>: Sufficient throughput and low latency for boot file transfers (50-200MB per boot)</li></ul><h2 id=considered-options>Considered Options</h2><ul><li><strong>Option 1</strong>: Google Cloud Platform (GCP)</li><li><strong>Option 2</strong>: Amazon Web Services (AWS)</li></ul><h2 id=decision-outcome>Decision Outcome</h2><p>Chosen option: &ldquo;<strong>Option 1: Google Cloud Platform (GCP)</strong>&rdquo;, because:</p><ol><li><strong>Existing Infrastructure</strong>: The home lab already uses GCP extensively (Cloud Run services, load balancers, mTLS infrastructure per existing codebase), reducing operational overhead and leveraging existing expertise</li><li><strong>Comparable Costs</strong>: Both providers offer similar costs for the required infrastructure (~$6-12/month for compute + VPN), with GCP&rsquo;s e2-micro being sufficient</li><li><strong>Equivalent Protocol Support</strong>: Both support TFTP/HTTP/HTTPS via direct VM access (load balancers unnecessary for single boot server), meeting all protocol requirements</li><li><strong>WireGuard Compatibility</strong>: Both require self-managed WireGuard deployment (neither has native WireGuard support), with nearly identical implementation complexity</li><li><strong>Unified Management</strong>: Consolidating all cloud infrastructure on GCP simplifies monitoring, billing, IAM, and operational workflows</li></ol><p>While AWS would be a viable alternative (especially with t4g.micro ARM instances offering slightly better price/performance), the <strong>existing GCP investment</strong> makes it the pragmatic choice to avoid multi-cloud complexity.</p><h3 id=consequences>Consequences</h3><ul><li>Good, because consolidates all cloud infrastructure on a single provider (reduced operational complexity)</li><li>Good, because leverages existing GCP expertise and IAM configurations</li><li>Good, because unified Cloud Monitoring/Logging across all services</li><li>Good, because single cloud bill simplifies cost tracking</li><li>Good, because existing Terraform modules and patterns can be reused</li><li>Good, because GCP&rsquo;s e2-micro instances (~$6.50/month) are cost-effective for the workload</li><li>Good, because self-managed WireGuard provides flexibility and low cost (~$10/month total)</li><li>Neutral, because both providers have comparable protocol support (TFTP/HTTP/HTTPS via VM)</li><li>Neutral, because both require self-managed WireGuard (no native support)</li><li>Bad, because creates vendor lock-in to GCP (migration would require relearning and reconfiguration)</li><li>Bad, because foregoes AWS&rsquo;s slightly cheaper t4g.micro ARM instances (~$6/month vs GCP&rsquo;s ~$6.50/month)</li><li>Bad, because multi-cloud strategy could provide redundancy (accepted trade-off for simplicity)</li></ul><h3 id=confirmation>Confirmation</h3><p>The implementation will be confirmed by:</p><ul><li>Successfully deploying WireGuard VPN gateway on GCP Compute Engine</li><li>Establishing site-to-site VPN tunnel between UDM Pro and GCP</li><li>Network booting a test server via VPN using TFTP and HTTP protocols</li><li>Measuring actual costs against estimates (~$10-15/month)</li><li>Validating boot performance (transfer time &lt; 30 seconds for typical boot)</li></ul><h2 id=pros-and-cons-of-the-options>Pros and Cons of the Options</h2><h3 id=option-1-google-cloud-platform-gcp>Option 1: Google Cloud Platform (GCP)</h3><p>Host network boot infrastructure on Google Cloud Platform.</p><h4 id=architecture-overview>Architecture Overview</h4><pre class=mermaid>graph TB
    subgraph &#34;Home Lab Network&#34;
        A[Home Lab Servers]
        B[UDM Pro - WireGuard Client]
    end
    
    subgraph &#34;GCP VPC&#34;
        C[WireGuard Gateway VM&lt;br/&gt;e2-micro]
        D[Boot Server VM&lt;br/&gt;e2-micro]
        C --&gt;|VPC Routing| D
    end
    
    A --&gt;|PXE Boot Request| B
    B --&gt;|WireGuard Tunnel| C
    C --&gt;|TFTP/HTTP/HTTPS| D
    D --&gt;|Boot Files| C
    C --&gt;|Encrypted Response| B
    B --&gt;|Boot Files| A</pre><h4 id=implementation-details>Implementation Details</h4><p><strong>Compute</strong>:</p><ul><li><strong>WireGuard Gateway</strong>: e2-micro VM (~$6.50/month) running Ubuntu 22.04<ul><li>Self-managed WireGuard server</li><li>IP forwarding enabled</li><li>Static external IP (~$3.50/month if VM ever stops)</li></ul></li><li><strong>Boot Server</strong>: e2-micro VM (same or consolidated with gateway)<ul><li>TFTP server (<code>tftpd-hpa</code>)</li><li>HTTP server (nginx or simple Python server)</li><li>Optional HTTPS with self-signed cert or Let&rsquo;s Encrypt</li></ul></li></ul><p><strong>Networking</strong>:</p><ul><li><strong>VPC</strong>: Default VPC or custom VPC with private subnets</li><li><strong>Firewall Rules</strong>:<ul><li>Allow UDP/51820 from home lab public IP (WireGuard)</li><li>Allow UDP/69, TCP/80, TCP/443 from VPN subnet (boot protocols)</li></ul></li><li><strong>Routes</strong>: Custom route to direct home lab subnet through WireGuard gateway</li><li><strong>Cloud VPN</strong>: Not used (self-managed WireGuard instead to save ~$65/month)</li></ul><p><strong>WireGuard Setup</strong>:</p><ul><li>Install WireGuard on Compute Engine VM</li><li>Configure <code>wg0</code> interface with PostUp/PostDown iptables rules</li><li>Store private key in Secret Manager</li><li>UDM Pro connects as WireGuard peer</li></ul><p><strong>Cost Breakdown</strong> (US regions):</p><table><thead><tr><th>Component</th><th>Monthly Cost</th></tr></thead><tbody><tr><td>e2-micro VM (WireGuard + Boot)</td><td>~$6.50</td></tr><tr><td>Static External IP (if attached)</td><td>~$3.50</td></tr><tr><td>Egress (10 boots × 150MB)</td><td>~$0.18</td></tr><tr><td><strong>Total</strong></td><td><strong>~$10.18</strong></td></tr><tr><td><strong>Annual</strong></td><td><strong>~$122</strong></td></tr></tbody></table><h4 id=pros-and-cons>Pros and Cons</h4><ul><li>Good, because existing home lab infrastructure already uses GCP extensively</li><li>Good, because consolidates all cloud resources on single provider (unified billing, IAM, monitoring)</li><li>Good, because leverages existing GCP expertise and Terraform modules</li><li>Good, because Cloud Monitoring/Logging already configured for other services</li><li>Good, because Secret Manager integration for WireGuard key storage</li><li>Good, because e2-micro instance size is sufficient for network boot workload</li><li>Good, because low cost (~$10/month for self-managed WireGuard)</li><li>Good, because VPC networking is familiar and well-documented</li><li>Neutral, because requires self-managed WireGuard (no native support, same as AWS)</li><li>Neutral, because TFTP/HTTP/HTTPS served directly from VM (no special GCP features needed)</li><li>Bad, because slightly more expensive than AWS t4g.micro (~$6.50/month vs ~$6/month)</li><li>Bad, because creates vendor lock-in to GCP ecosystem</li><li>Bad, because Cloud VPN (managed IPsec) is expensive (~$73/month), so must use self-managed WireGuard</li></ul><h3 id=option-2-amazon-web-services-aws>Option 2: Amazon Web Services (AWS)</h3><p>Host network boot infrastructure on Amazon Web Services.</p><h4 id=architecture-overview-1>Architecture Overview</h4><pre class=mermaid>graph TB
    subgraph &#34;Home Lab Network&#34;
        A[Home Lab Servers]
        B[UDM Pro - WireGuard Client]
    end
    
    subgraph &#34;AWS VPC&#34;
        C[WireGuard Gateway EC2&lt;br/&gt;t4g.micro]
        D[Boot Server EC2&lt;br/&gt;t4g.micro]
        C --&gt;|VPC Routing| D
    end
    
    A --&gt;|PXE Boot Request| B
    B --&gt;|WireGuard Tunnel| C
    C --&gt;|TFTP/HTTP/HTTPS| D
    D --&gt;|Boot Files| C
    C --&gt;|Encrypted Response| B
    B --&gt;|Boot Files| A</pre><h4 id=implementation-details-1>Implementation Details</h4><p><strong>Compute</strong>:</p><ul><li><strong>WireGuard Gateway</strong>: t4g.micro EC2 (~$6/month, ARM-based Graviton)<ul><li>Self-managed WireGuard server</li><li>Source/Dest check disabled for IP forwarding</li><li>Elastic IP (free when attached to running instance)</li></ul></li><li><strong>Boot Server</strong>: t4g.micro EC2 (same or consolidated with gateway)<ul><li>TFTP server (<code>tftpd-hpa</code>)</li><li>HTTP server (nginx)</li><li>Optional HTTPS with Let&rsquo;s Encrypt or self-signed cert</li></ul></li></ul><p><strong>Networking</strong>:</p><ul><li><strong>VPC</strong>: Default VPC or custom VPC with private subnets</li><li><strong>Security Groups</strong>:<ul><li>WireGuard SG: Allow UDP/51820 from home lab public IP</li><li>Boot Server SG: Allow UDP/69, TCP/80, TCP/443 from WireGuard SG</li></ul></li><li><strong>Route Table</strong>: Add route for home lab subnet via WireGuard instance</li><li><strong>Site-to-Site VPN</strong>: Not used (self-managed WireGuard saves ~$30/month)</li></ul><p><strong>WireGuard Setup</strong>:</p><ul><li>Install WireGuard on Ubuntu 22.04 or Amazon Linux 2023 EC2</li><li>Configure <code>wg0</code> with iptables MASQUERADE</li><li>Store private key in Secrets Manager</li><li>UDM Pro connects as WireGuard peer</li></ul><p><strong>Cost Breakdown</strong> (US East):</p><table><thead><tr><th>Component</th><th>Monthly Cost</th></tr></thead><tbody><tr><td>t4g.micro EC2 (WireGuard + Boot)</td><td>~$6.00</td></tr><tr><td>Elastic IP (attached)</td><td>$0.00</td></tr><tr><td>Egress (10 boots × 150MB)</td><td>~$0.09</td></tr><tr><td><strong>Total (On-Demand)</strong></td><td><strong>~$6.09</strong></td></tr><tr><td><strong>Total (1-yr Reserved)</strong></td><td><strong>~$3.59</strong></td></tr><tr><td><strong>Annual (On-Demand)</strong></td><td><strong>~$73</strong></td></tr><tr><td><strong>Annual (Reserved)</strong></td><td><strong>~$43</strong></td></tr></tbody></table><h4 id=pros-and-cons-1>Pros and Cons</h4><ul><li>Good, because t4g.micro ARM instances offer best price/performance (~$6/month on-demand)</li><li>Good, because Reserved Instances provide significant savings (~40% with 1-year commitment)</li><li>Good, because Elastic IP is free when attached to running instance</li><li>Good, because AWS has extensive documentation and community support</li><li>Good, because potential for future multi-cloud strategy</li><li>Good, because ACM provides free SSL certificates (if public domain used)</li><li>Good, because Secrets Manager for WireGuard key storage</li><li>Good, because low cost (~$6/month on-demand, ~$3.50/month with RI)</li><li>Neutral, because requires self-managed WireGuard (no native support, same as GCP)</li><li>Neutral, because TFTP/HTTP/HTTPS served directly from EC2 (no special AWS features)</li><li>Bad, because introduces multi-cloud complexity (separate billing, IAM, monitoring)</li><li>Bad, because no existing AWS infrastructure in home lab (new learning curve)</li><li>Bad, because requires separate monitoring/logging setup (CloudWatch vs Cloud Monitoring)</li><li>Bad, because separate Terraform state and modules needed</li><li>Bad, because Site-to-Site VPN is expensive (~$36/month), so must use self-managed WireGuard</li></ul><h2 id=more-information>More Information</h2><h3 id=detailed-analysis>Detailed Analysis</h3><p>For in-depth analysis of each provider&rsquo;s capabilities:</p><ul><li><p><a href=../analysis/google-cloud/><strong>Google Cloud Platform Analysis</strong></a></p><ul><li><a href=../analysis/google-cloud/network-boot/>Network Boot Protocol Support (TFTP, HTTP, HTTPS)</a></li><li><a href=../analysis/google-cloud/wireguard/>WireGuard VPN Support and Deployment</a></li></ul></li><li><p><a href=../analysis/aws/><strong>Amazon Web Services Analysis</strong></a></p><ul><li><a href=../analysis/aws/network-boot/>Network Boot Protocol Support (TFTP, HTTP, HTTPS)</a></li><li><a href=../analysis/aws/wireguard/>WireGuard VPN Support and Deployment</a></li></ul></li></ul><h3 id=key-findings-summary>Key Findings Summary</h3><p>Both providers offer:</p><ul><li>✅ <strong>TFTP Support</strong>: Via direct VM/EC2 access (load balancers don&rsquo;t support TFTP)</li><li>✅ <strong>HTTP/HTTPS Support</strong>: Full support via direct VM/EC2 or load balancers</li><li>✅ <strong>WireGuard Compatibility</strong>: Self-managed deployment on VM/EC2 (neither has native support)</li><li>✅ <strong>UDM Pro Integration</strong>: Native WireGuard client works with both</li><li>✅ <strong>Low Cost</strong>: $6-12/month for compute + VPN infrastructure</li><li>✅ <strong>Sufficient Performance</strong>: 100+ Mbps throughput on smallest instances</li></ul><p>Key differences:</p><ul><li><strong>GCP</strong>: Slightly higher cost (~$10/month), but consolidates with existing infrastructure</li><li><strong>AWS</strong>: Slightly lower cost (~$6/month on-demand, ~$3.50/month Reserved), but introduces multi-cloud complexity</li></ul><h3 id=cost-comparison-table>Cost Comparison Table</h3><table><thead><tr><th>Component</th><th>GCP (e2-micro)</th><th>AWS (t4g.micro On-Demand)</th><th>AWS (t4g.micro 1-yr RI)</th></tr></thead><tbody><tr><td>Compute</td><td>$6.50/month</td><td>$6.00/month</td><td>$3.50/month</td></tr><tr><td>Static IP</td><td>$3.50/month</td><td>$0.00 (Elastic IP free when attached)</td><td>$0.00</td></tr><tr><td>Egress (1.5GB)</td><td>$0.18/month</td><td>$0.09/month</td><td>$0.09/month</td></tr><tr><td><strong>Monthly</strong></td><td><strong>$10.18</strong></td><td><strong>$6.09</strong></td><td><strong>$3.59</strong></td></tr><tr><td><strong>Annual</strong></td><td><strong>$122</strong></td><td><strong>$73</strong></td><td><strong>$43</strong></td></tr></tbody></table><p><strong>Savings Analysis</strong>: AWS is ~$49-79/year cheaper, but introduces operational complexity.</p><h3 id=protocol-support-comparison>Protocol Support Comparison</h3><table><thead><tr><th>Protocol</th><th>GCP Support</th><th>AWS Support</th><th>Implementation</th></tr></thead><tbody><tr><td>TFTP (UDP/69)</td><td>⚠️ Via VM</td><td>⚠️ Via EC2</td><td>Direct VM/EC2 access (no LB support)</td></tr><tr><td>HTTP (TCP/80)</td><td>✅ Full</td><td>✅ Full</td><td>Direct VM/EC2 or Load Balancer</td></tr><tr><td>HTTPS (TCP/443)</td><td>✅ Full</td><td>✅ Full</td><td>Direct VM/EC2 or Load Balancer + cert</td></tr><tr><td>WireGuard</td><td>⚠️ Self-managed</td><td>⚠️ Self-managed</td><td>Install on VM/EC2</td></tr></tbody></table><h3 id=wireguard-deployment-comparison>WireGuard Deployment Comparison</h3><table><thead><tr><th>Aspect</th><th>GCP</th><th>AWS</th></tr></thead><tbody><tr><td><strong>Native Support</strong></td><td>❌ No (IPsec Cloud VPN only)</td><td>❌ No (IPsec Site-to-Site VPN only)</td></tr><tr><td><strong>Self-Managed</strong></td><td>✅ Compute Engine</td><td>✅ EC2</td></tr><tr><td><strong>Setup Complexity</strong></td><td>Similar (install, configure, firewall)</td><td>Similar (install, configure, SG)</td></tr><tr><td><strong>IP Forwarding</strong></td><td>Enable on VM</td><td>Disable Source/Dest check</td></tr><tr><td><strong>Firewall</strong></td><td>VPC Firewall rules</td><td>Security Groups</td></tr><tr><td><strong>Key Storage</strong></td><td>Secret Manager</td><td>Secrets Manager</td></tr><tr><td><strong>Cost</strong></td><td>~$10/month total</td><td>~$6/month total</td></tr></tbody></table><h3 id=trade-offs-analysis>Trade-offs Analysis</h3><p><strong>Choosing GCP</strong>:</p><ul><li><strong>Wins</strong>: Operational simplicity, unified infrastructure, existing expertise</li><li><strong>Loses</strong>: ~$50-80/year higher cost, vendor lock-in</li></ul><p><strong>Choosing AWS</strong>:</p><ul><li><strong>Wins</strong>: Lower cost, Reserved Instance savings, multi-cloud optionality</li><li><strong>Loses</strong>: Multi-cloud complexity, separate monitoring/billing, new tooling</li></ul><p>For a home lab prioritizing <strong>simplicity over cost optimization</strong>, GCP&rsquo;s consolidation benefits outweigh the modest cost difference.</p><h3 id=related-adrs>Related ADRs</h3><ul><li><a href=./0002-network-boot-architecture/>ADR-0002: Network Boot Architecture</a> - Established requirement for cloud-hosted boot server with VPN</li><li><a href=./0001-use-madr-for-architecture-decision-records/>ADR-0001: Use MADR for Architecture Decision Records</a> - MADR format used for this ADR</li></ul><h3 id=future-considerations>Future Considerations</h3><ol><li><strong>Cost Reevaluation</strong>: If annual costs become significant, reconsider AWS Reserved Instances</li><li><strong>Multi-Cloud</strong>: If multi-cloud strategy emerges, migrate boot server to AWS</li><li><strong>Managed WireGuard</strong>: If GCP or AWS adds native WireGuard support, reevaluate managed option</li><li><strong>High Availability</strong>: If HA required, evaluate multi-region deployment costs on both providers</li></ol><h3 id=related-issues>Related Issues</h3><ul><li><a href=https://github.com/Zaba505/infra/issues/597>Issue #597</a> - story(docs): create adr for cloud provider selection</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-dab026fd06f98a03a459f97073d21662>4 - [0004] Server Operating System Selection</h1><div class=lead>Evaluate operating systems for homelab server infrastructure with focus on Kubernetes cluster setup and maintenance.</div><h2 id=context-and-problem-statement>Context and Problem Statement</h2><p>The homelab infrastructure requires a server operating system to run Kubernetes clusters for container workloads. The choice of operating system significantly impacts ease of cluster initialization, ongoing maintenance burden, security posture, and operational complexity.</p><p>The question is: <strong>Which operating system should be used for homelab Kubernetes servers?</strong></p><p>This decision will affect:</p><ul><li><strong>Cluster Initialization</strong>: Complexity and time required to bootstrap Kubernetes</li><li><strong>Maintenance Burden</strong>: Frequency and complexity of OS updates, Kubernetes upgrades, and patching</li><li><strong>Security Posture</strong>: Attack surface, built-in security features, and hardening requirements</li><li><strong>Resource Efficiency</strong>: RAM, CPU, and disk overhead</li><li><strong>Operational Complexity</strong>: Day-to-day management, troubleshooting, and debugging</li><li><strong>Learning Curve</strong>: Time required for team to become proficient</li></ul><h2 id=decision-drivers>Decision Drivers</h2><ul><li><strong>Ease of Kubernetes Setup</strong>: Minimize steps and complexity for cluster initialization</li><li><strong>Maintenance Simplicity</strong>: Reduce ongoing operational burden for updates and upgrades</li><li><strong>Security-First Design</strong>: Minimal attack surface and strong security defaults</li><li><strong>Resource Efficiency</strong>: Low RAM/CPU/disk overhead for cost-effective homelab</li><li><strong>Learning Curve</strong>: Reasonable adoption time for single-person homelab</li><li><strong>Community Support</strong>: Strong documentation and active community</li><li><strong>Immutability</strong>: Prefer declarative, version-controlled configuration (GitOps-friendly)</li><li><strong>Purpose-Built</strong>: OS optimized specifically for Kubernetes vs general-purpose</li></ul><h2 id=considered-options>Considered Options</h2><ul><li><strong>Option 1</strong>: Ubuntu Server with k3s</li><li><strong>Option 2</strong>: Fedora Server with kubeadm</li><li><strong>Option 3</strong>: Talos Linux (purpose-built Kubernetes OS)</li><li><strong>Option 4</strong>: Harvester HCI (hyperconverged platform)</li></ul><h2 id=decision-outcome>Decision Outcome</h2><p>Chosen option: &ldquo;<strong>Option 3: Talos Linux</strong>&rdquo;, because:</p><ol><li><strong>Minimal Attack Surface</strong>: No SSH, shell, or package manager eliminates entire classes of vulnerabilities, providing the strongest security posture</li><li><strong>Built-in Kubernetes</strong>: No separate installation or configuration complexity - Kubernetes is included and optimized</li><li><strong>Declarative Configuration</strong>: API-driven, immutable infrastructure aligns with GitOps principles and prevents configuration drift</li><li><strong>Lowest Resource Overhead</strong>: ~768MB RAM vs 1-2GB+ for traditional distros, maximizing homelab hardware efficiency</li><li><strong>Simplified Maintenance</strong>: Declarative upgrades (<code>talosctl upgrade</code>) for both OS and Kubernetes reduce operational burden</li><li><strong>Security by Default</strong>: Immutable filesystem, no shell, KSPP compliance - secure without manual hardening</li></ol><p>While the learning curve is steeper than traditional Linux distributions, the benefits of purpose-built Kubernetes infrastructure, minimal maintenance, and superior security outweigh the initial learning investment for a dedicated Kubernetes homelab.</p><h3 id=consequences>Consequences</h3><ul><li>Good, because minimal attack surface (no SSH/shell) provides strongest security posture</li><li>Good, because declarative configuration enables GitOps workflows and prevents drift</li><li>Good, because lowest resource overhead (~768MB RAM) maximizes homelab efficiency</li><li>Good, because built-in Kubernetes eliminates installation complexity</li><li>Good, because immutable infrastructure prevents configuration drift</li><li>Good, because simplified upgrades (single command for OS + K8s) reduce maintenance burden</li><li>Good, because smallest disk footprint (~500MB) vs 10GB+ for traditional distros</li><li>Good, because secure by default (no manual hardening required)</li><li>Good, because purpose-built design optimized specifically for Kubernetes</li><li>Good, because API-driven management (talosctl) enables automation</li><li>Neutral, because steeper learning curve (paradigm shift from shell-based management)</li><li>Neutral, because smaller community than Ubuntu/Fedora (but active and helpful)</li><li>Bad, because limited to Kubernetes workloads only (not general-purpose)</li><li>Bad, because no shell access requires different troubleshooting approach</li><li>Bad, because newer platform (less mature than Ubuntu/Fedora)</li><li>Bad, because no escape hatch for manual intervention when needed</li></ul><h3 id=confirmation>Confirmation</h3><p>The implementation will be confirmed by:</p><ul><li>Successfully bootstrapping a Talos cluster using talosctl</li><li>Deploying test workloads and validating functionality</li><li>Performing declarative OS and Kubernetes upgrades</li><li>Measuring actual resource usage (RAM &lt; 1GB per node)</li><li>Validating security posture (no SSH/shell, immutable filesystem)</li><li>Testing GitOps workflow (machine configs in version control)</li></ul><h2 id=pros-and-cons-of-the-options>Pros and Cons of the Options</h2><h3 id=option-1-ubuntu-server-with-k3s>Option 1: Ubuntu Server with k3s</h3><p>Host Kubernetes using Ubuntu Server 24.04 LTS with k3s lightweight Kubernetes distribution.</p><h4 id=architecture-overview>Architecture Overview</h4><pre class=mermaid>sequenceDiagram
    participant Admin
    participant Server as Ubuntu Server
    participant K3s as k3s Components
    
    Admin-&gt;&gt;Server: Install Ubuntu 24.04 LTS
    Server-&gt;&gt;Server: Configure network (static IP)
    Admin-&gt;&gt;Server: Update system
    Admin-&gt;&gt;Server: curl -sfL https://get.k3s.io | sh -
    Server-&gt;&gt;K3s: Download k3s binary
    K3s-&gt;&gt;Server: Configure containerd
    K3s-&gt;&gt;Server: Start k3s service
    K3s-&gt;&gt;Server: Initialize etcd (embedded)
    K3s-&gt;&gt;Server: Start API server
    K3s-&gt;&gt;Server: Deploy built-in CNI (Flannel)
    K3s--&gt;&gt;Admin: Control plane ready
    Admin-&gt;&gt;Server: Retrieve node token
    Admin-&gt;&gt;Server: Install k3s agent on workers
    K3s-&gt;&gt;Server: Join workers to cluster
    K3s--&gt;&gt;Admin: Cluster ready (5-10 minutes)</pre><h4 id=implementation-details>Implementation Details</h4><p><strong>Installation</strong>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Single-command k3s install</span>
</span></span><span style=display:flex><span>curl -sfL https://get.k3s.io <span style=color:#000;font-weight:700>|</span> sh -
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Get token for workers</span>
</span></span><span style=display:flex><span>sudo cat /var/lib/rancher/k3s/server/node-token
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Install on workers</span>
</span></span><span style=display:flex><span>curl -sfL https://get.k3s.io <span style=color:#000;font-weight:700>|</span> <span style=color:#000>K3S_URL</span><span style=color:#ce5c00;font-weight:700>=</span>https://control-plane:6443 <span style=color:#000>K3S_TOKEN</span><span style=color:#ce5c00;font-weight:700>=</span>&lt;token&gt; sh -
</span></span></code></pre></div><p><strong>Resource Requirements</strong>:</p><ul><li><strong>RAM</strong>: 1GB total (512MB OS + 512MB k3s)</li><li><strong>CPU</strong>: 1-2 cores</li><li><strong>Disk</strong>: 20GB (10GB OS + 10GB containers)</li></ul><p><strong>Maintenance</strong>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># OS updates</span>
</span></span><span style=display:flex><span>sudo apt update <span style=color:#ce5c00;font-weight:700>&amp;&amp;</span> sudo apt upgrade
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># k3s upgrade</span>
</span></span><span style=display:flex><span>curl -sfL https://get.k3s.io <span style=color:#000;font-weight:700>|</span> <span style=color:#000>INSTALL_K3S_VERSION</span><span style=color:#ce5c00;font-weight:700>=</span>v1.32.0+k3s1 sh -
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Or automatic via system-upgrade-controller</span>
</span></span></code></pre></div><h4 id=pros-and-cons>Pros and Cons</h4><ul><li>Good, because most familiar Linux distribution (easy adoption)</li><li>Good, because 5-year LTS support (10 years with Ubuntu Pro)</li><li>Good, because k3s provides single-command setup</li><li>Good, because extensive documentation and community support</li><li>Good, because compatible with all Kubernetes tooling</li><li>Good, because automatic security updates available</li><li>Good, because general-purpose (can run non-K8s workloads)</li><li>Good, because low learning curve</li><li>Neutral, because moderate resource overhead (1GB RAM)</li><li>Bad, because general-purpose OS has larger attack surface</li><li>Bad, because requires manual OS updates and reboots</li><li>Bad, because managing OS + Kubernetes lifecycle separately</li><li>Bad, because imperative configuration (not GitOps-native)</li><li>Bad, because mutable filesystem (configuration drift possible)</li></ul><h3 id=option-2-fedora-server-with-kubeadm>Option 2: Fedora Server with kubeadm</h3><p>Host Kubernetes using Fedora Server with kubeadm (official Kubernetes tool) and CRI-O container runtime.</p><h4 id=architecture-overview-1>Architecture Overview</h4><pre class=mermaid>sequenceDiagram
    participant Admin
    participant Server as Fedora Server
    participant K8s as Kubernetes Components
    
    Admin-&gt;&gt;Server: Install Fedora 41
    Server-&gt;&gt;Server: Configure network
    Admin-&gt;&gt;Server: Update system (dnf update)
    Admin-&gt;&gt;Server: Install CRI-O
    Server-&gt;&gt;Server: Configure CRI-O runtime
    Admin-&gt;&gt;Server: Install kubeadm/kubelet/kubectl
    Server-&gt;&gt;Server: Disable swap, load kernel modules
    Server-&gt;&gt;Server: Configure SELinux
    Admin-&gt;&gt;K8s: kubeadm init --cri-socket=unix:///var/run/crio/crio.sock
    K8s-&gt;&gt;Server: Generate certificates
    K8s-&gt;&gt;Server: Start etcd
    K8s-&gt;&gt;Server: Start API server
    K8s--&gt;&gt;Admin: Control plane ready
    Admin-&gt;&gt;K8s: kubectl apply CNI
    K8s-&gt;&gt;Server: Deploy CNI pods
    Admin-&gt;&gt;K8s: kubeadm join (workers)
    K8s--&gt;&gt;Admin: Cluster ready (15-20 minutes)</pre><h4 id=implementation-details-1>Implementation Details</h4><p><strong>Installation</strong>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Install CRI-O</span>
</span></span><span style=display:flex><span>sudo dnf install -y cri-o
</span></span><span style=display:flex><span>sudo systemctl <span style=color:#204a87>enable</span> --now crio
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Install kubeadm components</span>
</span></span><span style=display:flex><span>sudo dnf install -y kubelet kubeadm kubectl
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Initialize cluster</span>
</span></span><span style=display:flex><span>sudo kubeadm init --pod-network-cidr<span style=color:#ce5c00;font-weight:700>=</span>10.244.0.0/16 --cri-socket<span style=color:#ce5c00;font-weight:700>=</span>unix:///var/run/crio/crio.sock
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Install CNI</span>
</span></span><span style=display:flex><span>kubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.28.0/manifests/calico.yaml
</span></span></code></pre></div><p><strong>Resource Requirements</strong>:</p><ul><li><strong>RAM</strong>: 2.2GB total (700MB OS + 1.5GB Kubernetes)</li><li><strong>CPU</strong>: 2+ cores</li><li><strong>Disk</strong>: 35GB (15GB OS + 20GB containers)</li></ul><p><strong>Maintenance</strong>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># OS updates (every 13 months major upgrade)</span>
</span></span><span style=display:flex><span>sudo dnf update -y
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Kubernetes upgrade</span>
</span></span><span style=display:flex><span>sudo dnf update -y kubeadm
</span></span><span style=display:flex><span>sudo kubeadm upgrade apply v1.32.0
</span></span><span style=display:flex><span>sudo dnf update -y kubelet kubectl
</span></span></code></pre></div><h4 id=pros-and-cons-1>Pros and Cons</h4><ul><li>Good, because SELinux enabled by default (stronger than AppArmor)</li><li>Good, because latest kernel and packages (bleeding edge)</li><li>Good, because native CRI-O support (OpenShift compatibility)</li><li>Good, because upstream for RHEL (enterprise patterns)</li><li>Good, because kubeadm provides full control over cluster</li><li>Neutral, because faster release cycle (latest features, but more upgrades)</li><li>Bad, because short support cycle (13 months per release)</li><li>Bad, because bleeding-edge can introduce instability</li><li>Bad, because complex kubeadm setup (many manual steps)</li><li>Bad, because higher resource overhead (2.2GB RAM)</li><li>Bad, because SELinux configuration for Kubernetes is complex</li><li>Bad, because frequent OS upgrades required (every 13 months)</li><li>Bad, because managing OS + Kubernetes separately</li><li>Bad, because imperative configuration (not GitOps-native)</li></ul><h3 id=option-3-talos-linux-purpose-built-kubernetes-os>Option 3: Talos Linux (purpose-built Kubernetes OS)</h3><p>Use Talos Linux, an immutable, API-driven operating system designed specifically for Kubernetes with built-in cluster management.</p><h4 id=architecture-overview-2>Architecture Overview</h4><pre class=mermaid>sequenceDiagram
    participant Admin
    participant Server as Bare Metal Server
    participant Talos as Talos Linux
    participant K8s as Kubernetes Components
    
    Admin-&gt;&gt;Server: Boot Talos ISO (PXE or USB)
    Server-&gt;&gt;Talos: Start in maintenance mode
    Talos--&gt;&gt;Admin: API endpoint ready
    Admin-&gt;&gt;Admin: Generate configs (talosctl gen config)
    Admin-&gt;&gt;Talos: talosctl apply-config (controlplane.yaml)
    Talos-&gt;&gt;Server: Install Talos to disk
    Server-&gt;&gt;Server: Reboot from disk
    Talos-&gt;&gt;K8s: Start kubelet
    Talos-&gt;&gt;K8s: Start etcd
    Talos-&gt;&gt;K8s: Start API server
    Admin-&gt;&gt;Talos: talosctl bootstrap
    Talos-&gt;&gt;K8s: Initialize cluster
    K8s-&gt;&gt;Talos: Start controller-manager
    K8s--&gt;&gt;Admin: Control plane ready
    Admin-&gt;&gt;K8s: Apply CNI
    Admin-&gt;&gt;Talos: Apply worker configs
    Talos-&gt;&gt;K8s: Join workers
    K8s--&gt;&gt;Admin: Cluster ready (10-15 minutes)</pre><h4 id=implementation-details-2>Implementation Details</h4><p><strong>Installation</strong>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Generate machine configs</span>
</span></span><span style=display:flex><span>talosctl gen config homelab https://192.168.1.10:6443
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Apply config to control plane (booted from ISO)</span>
</span></span><span style=display:flex><span>talosctl apply-config --insecure --nodes 192.168.1.10 --file controlplane.yaml
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Bootstrap Kubernetes</span>
</span></span><span style=display:flex><span>talosctl bootstrap --nodes 192.168.1.10 --endpoints 192.168.1.10
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Get kubeconfig</span>
</span></span><span style=display:flex><span>talosctl kubeconfig --nodes 192.168.1.10
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Add workers</span>
</span></span><span style=display:flex><span>talosctl apply-config --insecure --nodes 192.168.1.11 --file worker.yaml
</span></span></code></pre></div><p><strong>Machine Configuration</strong> (declarative YAML):</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#204a87;font-weight:700>version</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline> </span><span style=color:#000>v1alpha1</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline></span><span style=color:#204a87;font-weight:700>machine</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>  </span><span style=color:#204a87;font-weight:700>type</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline> </span><span style=color:#000>controlplane</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>  </span><span style=color:#204a87;font-weight:700>install</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>    </span><span style=color:#204a87;font-weight:700>disk</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline> </span><span style=color:#000>/dev/sda</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>  </span><span style=color:#204a87;font-weight:700>network</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>    </span><span style=color:#204a87;font-weight:700>hostname</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline> </span><span style=color:#000>control-plane-1</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>    </span><span style=color:#204a87;font-weight:700>interfaces</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>      </span>- <span style=color:#204a87;font-weight:700>interface</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline> </span><span style=color:#000>eth0</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>        </span><span style=color:#204a87;font-weight:700>addresses</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>          </span>- <span style=color:#0000cf;font-weight:700>192.168.1.10</span><span style=color:#000>/24</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline></span><span style=color:#204a87;font-weight:700>cluster</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>  </span><span style=color:#204a87;font-weight:700>clusterName</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline> </span><span style=color:#000>homelab</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>  </span><span style=color:#204a87;font-weight:700>controlPlane</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>    </span><span style=color:#204a87;font-weight:700>endpoint</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline> </span><span style=color:#000>https://192.168.1.10:6443</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>  </span><span style=color:#204a87;font-weight:700>network</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>    </span><span style=color:#204a87;font-weight:700>cni</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>      </span><span style=color:#204a87;font-weight:700>name</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline> </span><span style=color:#000>custom</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>      </span><span style=color:#204a87;font-weight:700>urls</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>        </span>- <span style=color:#000>https://raw.githubusercontent.com/projectcalico/calico/v3.28.0/manifests/calico.yaml</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span></code></pre></div><p><strong>Resource Requirements</strong>:</p><ul><li><strong>RAM</strong>: 768MB total (256MB OS + 512MB Kubernetes)</li><li><strong>CPU</strong>: 1-2 cores</li><li><strong>Disk</strong>: 10-15GB (500MB OS + 10GB containers)</li></ul><p><strong>Maintenance</strong>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Upgrade Talos (OS + Kubernetes)</span>
</span></span><span style=display:flex><span>talosctl upgrade --nodes 192.168.1.10 --image ghcr.io/siderolabs/installer:v1.9.0
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Upgrade Kubernetes version</span>
</span></span><span style=display:flex><span>talosctl upgrade-k8s --nodes 192.168.1.10 --to 1.32.0
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Apply config changes</span>
</span></span><span style=display:flex><span>talosctl apply-config --nodes 192.168.1.10 --file controlplane.yaml
</span></span></code></pre></div><h4 id=pros-and-cons-2>Pros and Cons</h4><ul><li>Good, because Kubernetes built-in (no separate installation)</li><li>Good, because minimal attack surface (no SSH, shell, package manager)</li><li>Good, because immutable infrastructure (config drift impossible)</li><li>Good, because API-driven management (GitOps-friendly)</li><li>Good, because lowest resource overhead (~768MB RAM)</li><li>Good, because declarative configuration (YAML in version control)</li><li>Good, because secure by default (no manual hardening)</li><li>Good, because smallest disk footprint (~500MB OS)</li><li>Good, because designed specifically for Kubernetes</li><li>Good, because simple declarative upgrades (OS + K8s)</li><li>Good, because UEFI Secure Boot support</li><li>Neutral, because smaller community (but active and helpful)</li><li>Bad, because steep learning curve (paradigm shift)</li><li>Bad, because limited to Kubernetes workloads only</li><li>Bad, because troubleshooting without shell requires different approach</li><li>Bad, because relatively new (less mature than Ubuntu/Fedora)</li><li>Bad, because no escape hatch for manual intervention</li></ul><h3 id=option-4-harvester-hci-hyperconverged-platform>Option 4: Harvester HCI (hyperconverged platform)</h3><p>Use Harvester, a hyperconverged infrastructure platform built on K3s and KubeVirt for unified VM + container management.</p><h4 id=architecture-overview-3>Architecture Overview</h4><pre class=mermaid>sequenceDiagram
    participant Admin
    participant Server as Bare Metal Server
    participant Harvester as Harvester HCI
    participant K3s as K3s / KubeVirt
    participant Storage as Longhorn Storage
    
    Admin-&gt;&gt;Server: Boot Harvester ISO
    Server-&gt;&gt;Harvester: Installation wizard
    Admin-&gt;&gt;Harvester: Configure cluster (VIP, storage)
    Harvester-&gt;&gt;Server: Install RancherOS 2.0
    Harvester-&gt;&gt;Server: Install K3s
    Server-&gt;&gt;Server: Reboot
    Harvester-&gt;&gt;K3s: Start K3s server
    K3s-&gt;&gt;Storage: Deploy Longhorn
    K3s-&gt;&gt;Server: Deploy KubeVirt
    K3s-&gt;&gt;Server: Deploy multus CNI
    Harvester--&gt;&gt;Admin: Web UI ready
    Admin-&gt;&gt;Harvester: Add nodes
    Harvester-&gt;&gt;K3s: Join cluster
    K3s--&gt;&gt;Admin: Cluster ready (20-30 minutes)</pre><h4 id=implementation-details-3>Implementation Details</h4><p><strong>Installation</strong>: Interactive ISO wizard or cloud-init config</p><p><strong>Resource Requirements</strong>:</p><ul><li><strong>RAM</strong>: 8GB minimum per node (16GB+ recommended)</li><li><strong>CPU</strong>: 4+ cores per node</li><li><strong>Disk</strong>: 250GB+ per node (100GB OS + 150GB storage)</li><li><strong>Nodes</strong>: 3+ for production HA</li></ul><p><strong>Features</strong>:</p><ul><li>Web UI management</li><li>Built-in storage (Longhorn)</li><li>VM support (KubeVirt)</li><li>Live migration</li><li>Rancher integration</li></ul><h4 id=pros-and-cons-3>Pros and Cons</h4><ul><li>Good, because unified VM + container platform</li><li>Good, because built-in K3s (Kubernetes included)</li><li>Good, because web UI simplifies management</li><li>Good, because built-in persistent storage (Longhorn)</li><li>Good, because VM live migration</li><li>Good, because Rancher integration</li><li>Neutral, because immutable OS layer</li><li>Bad, because very heavy resource requirements (8GB+ RAM)</li><li>Bad, because complex architecture (KubeVirt, Longhorn, multus)</li><li>Bad, because overkill for container-only workloads</li><li>Bad, because larger attack surface (web UI, VM layer)</li><li>Bad, because requires 3+ nodes for HA (not single-node friendly)</li><li>Bad, because steep learning curve for full feature set</li></ul><h2 id=more-information>More Information</h2><h3 id=detailed-analysis>Detailed Analysis</h3><p>For in-depth analysis of each operating system:</p><ul><li><p><a href=../analysis/server-os/ubuntu/><strong>Ubuntu Server Analysis</strong></a></p><ul><li>Installation methods (kubeadm, k3s, MicroK8s)</li><li>Cluster initialization sequences</li><li>Maintenance requirements and upgrade procedures</li><li>Resource overhead and security posture</li></ul></li><li><p><a href=../analysis/server-os/fedora/><strong>Fedora Server Analysis</strong></a></p><ul><li>kubeadm with CRI-O installation</li><li>SELinux configuration for Kubernetes</li><li>Rapid release cycle implications</li><li>RHEL ecosystem compatibility</li></ul></li><li><p><a href=../analysis/server-os/talos-linux/><strong>Talos Linux Analysis</strong></a></p><ul><li>API-driven, immutable architecture</li><li>Declarative configuration model</li><li>Security-first design principles</li><li>Production readiness and advanced features</li></ul></li><li><p><a href=../analysis/server-os/harvester/><strong>Harvester HCI Analysis</strong></a></p><ul><li>Hyperconverged infrastructure capabilities</li><li>VM + container unified platform</li><li>KubeVirt and Longhorn integration</li><li>Multi-node cluster requirements</li></ul></li></ul><h3 id=key-findings-summary>Key Findings Summary</h3><p>Resource efficiency comparison:</p><ul><li>✅ <strong>Talos</strong>: 768MB RAM, 500MB disk (most efficient)</li><li>✅ <strong>Ubuntu + k3s</strong>: 1GB RAM, 20GB disk (efficient)</li><li>⚠️ <strong>Fedora + kubeadm</strong>: 2.2GB RAM, 35GB disk (moderate)</li><li>❌ <strong>Harvester</strong>: 8GB+ RAM, 250GB+ disk (heavy)</li></ul><p>Security posture comparison:</p><ul><li>✅ <strong>Talos</strong>: Minimal attack surface (no SSH/shell, immutable)</li><li>✅ <strong>Fedora</strong>: SELinux by default (strong MAC)</li><li>⚠️ <strong>Ubuntu</strong>: AppArmor (moderate security)</li><li>⚠️ <strong>Harvester</strong>: Larger attack surface (web UI, VM layer)</li></ul><p>Operational complexity comparison:</p><ul><li>✅ <strong>Ubuntu + k3s</strong>: Single command install, familiar management</li><li>✅ <strong>Talos</strong>: Declarative, automated (after learning curve)</li><li>⚠️ <strong>Fedora + kubeadm</strong>: Manual kubeadm steps, frequent OS upgrades</li><li>❌ <strong>Harvester</strong>: Complex HCI architecture, heavy requirements</li></ul><h3 id=decision-matrix>Decision Matrix</h3><table><thead><tr><th>Criterion</th><th>Ubuntu + k3s</th><th>Fedora + kubeadm</th><th>Talos Linux</th><th>Harvester</th></tr></thead><tbody><tr><td><strong>Setup Simplicity</strong></td><td>⭐⭐⭐⭐⭐</td><td>⭐⭐⭐</td><td>⭐⭐⭐⭐</td><td>⭐⭐⭐</td></tr><tr><td><strong>Maintenance Burden</strong></td><td>⭐⭐⭐⭐</td><td>⭐⭐</td><td>⭐⭐⭐⭐⭐</td><td>⭐⭐⭐⭐</td></tr><tr><td><strong>Security Posture</strong></td><td>⭐⭐⭐</td><td>⭐⭐⭐⭐</td><td>⭐⭐⭐⭐⭐</td><td>⭐⭐⭐</td></tr><tr><td><strong>Resource Efficiency</strong></td><td>⭐⭐⭐⭐</td><td>⭐⭐⭐</td><td>⭐⭐⭐⭐⭐</td><td>⭐</td></tr><tr><td><strong>Learning Curve</strong></td><td>⭐⭐⭐⭐⭐</td><td>⭐⭐⭐⭐</td><td>⭐⭐</td><td>⭐⭐⭐</td></tr><tr><td><strong>Community Support</strong></td><td>⭐⭐⭐⭐⭐</td><td>⭐⭐⭐⭐</td><td>⭐⭐⭐</td><td>⭐⭐⭐⭐</td></tr><tr><td><strong>Immutability</strong></td><td>⭐</td><td>⭐</td><td>⭐⭐⭐⭐⭐</td><td>⭐⭐⭐⭐</td></tr><tr><td><strong>GitOps-Friendly</strong></td><td>⭐⭐</td><td>⭐⭐</td><td>⭐⭐⭐⭐⭐</td><td>⭐⭐⭐</td></tr><tr><td><strong>Purpose-Built</strong></td><td>⭐⭐</td><td>⭐⭐</td><td>⭐⭐⭐⭐⭐</td><td>⭐⭐⭐⭐</td></tr><tr><td><strong>Overall Score</strong></td><td>29/45</td><td>24/45</td><td>38/45</td><td>28/45</td></tr></tbody></table><p><strong>Talos Linux scores highest</strong> for Kubernetes-dedicated homelab infrastructure prioritizing security, efficiency, and GitOps workflows.</p><h3 id=trade-offs-analysis>Trade-offs Analysis</h3><p><strong>Choosing Talos Linux</strong>:</p><ul><li><strong>Wins</strong>: Best security, lowest overhead, declarative configuration, minimal maintenance</li><li><strong>Loses</strong>: Steeper learning curve, no shell access, smaller community</li></ul><p><strong>Choosing Ubuntu + k3s</strong>:</p><ul><li><strong>Wins</strong>: Easiest adoption, largest community, general-purpose flexibility</li><li><strong>Loses</strong>: Higher attack surface, manual OS management, imperative config</li></ul><p><strong>Choosing Fedora + kubeadm</strong>:</p><ul><li><strong>Wins</strong>: Latest features, SELinux, enterprise compatibility</li><li><strong>Loses</strong>: Frequent OS upgrades, complex setup, higher overhead</li></ul><p><strong>Choosing Harvester</strong>:</p><ul><li><strong>Wins</strong>: VM + container unified platform, web UI</li><li><strong>Loses</strong>: Heavy resources, complex architecture, overkill for K8s-only</li></ul><p>For a <strong>Kubernetes-dedicated homelab prioritizing security and efficiency</strong>, Talos Linux&rsquo;s benefits outweigh the learning curve investment.</p><h3 id=related-adrs>Related ADRs</h3><ul><li><a href=./0001-use-madr-for-architecture-decision-records/>ADR-0001: Use MADR for Architecture Decision Records</a> - MADR format used for this ADR</li><li><a href=./0002-network-boot-architecture/>ADR-0002: Network Boot Architecture</a> - Server provisioning architecture</li><li><a href=./0003-cloud-provider-selection/>ADR-0003: Cloud Provider Selection</a> - Cloud infrastructure decisions</li></ul><h3 id=future-considerations>Future Considerations</h3><ol><li><strong>Team Growth</strong>: If team grows beyond single person, reassess Ubuntu for familiarity</li><li><strong>VM Requirements</strong>: If VM workloads emerge, consider Harvester or KubeVirt on Talos</li><li><strong>Enterprise Patterns</strong>: If RHEL compatibility needed, reconsider Fedora/CentOS Stream</li><li><strong>Maintenance Burden</strong>: If Talos learning curve proves too steep, fallback to k3s</li><li><strong>Talos Maturity</strong>: Monitor Talos ecosystem growth and production adoption</li></ol><h3 id=related-issues>Related Issues</h3><ul><li><a href=https://github.com/Zaba505/infra/issues/598>Issue #598</a> - story(docs): create adr for server operating system</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-be9e21cdab9183bad0c60fa6e3ba225b>5 - [0005] Network Boot Infrastructure Implementation on Google Cloud</h1><div class=lead>Evaluate implementation approaches for deploying network boot (PXE/iPXE) infrastructure on Google Cloud Platform, comparing custom server implementation versus Matchbox-based solution.</div><h2 id=context-and-problem-statement>Context and Problem Statement</h2><p><a href=./0002-network-boot-architecture/>ADR-0002</a> established that network boot infrastructure will be hosted on a cloud provider accessed via WireGuard VPN. <a href=./0003-cloud-provider-selection/>ADR-0003</a> selected Google Cloud Platform as the hosting provider to consolidate infrastructure and leverage existing expertise.</p><p>The remaining question is: <strong>How should the network boot server itself be implemented?</strong></p><p>This decision affects:</p><ul><li><strong>Development Effort</strong>: Time required to build, test, and maintain the solution</li><li><strong>Feature Completeness</strong>: Capabilities for boot image management, machine mapping, and provisioning workflows</li><li><strong>Operational Complexity</strong>: Deployment, monitoring, and troubleshooting burden</li><li><strong>Security</strong>: Boot image integrity, access control, and audit capabilities</li><li><strong>Scalability</strong>: Ability to grow from single home lab to multiple environments</li></ul><p>The boot server must handle:</p><ol><li><strong>TFTP requests</strong> for initial PXE boot loader (iPXE binary)</li><li><strong>HTTP/HTTPS requests</strong> for iPXE scripts, kernels, initrd images, and cloud-init configurations</li><li><strong>Machine-to-image mapping</strong> to serve appropriate boot files based on MAC address, hardware profile, or tags</li><li><strong>Boot image lifecycle management</strong> including upload, versioning, and rollback capabilities</li></ol><h2 id=decision-drivers>Decision Drivers</h2><ul><li><strong>Time to Production</strong>: Minimize time to get a working network boot infrastructure</li><li><strong>Feature Requirements</strong>: Must support machine-specific boot configurations, image versioning, and cloud-init integration</li><li><strong>Maintenance Burden</strong>: Prefer solutions that minimize ongoing maintenance and updates</li><li><strong>GCP Integration</strong>: Should leverage GCP services (Cloud Storage, Secret Manager, IAM)</li><li><strong>Security</strong>: Boot images must be served securely with access control and integrity verification</li><li><strong>Observability</strong>: Comprehensive logging and monitoring for troubleshooting boot failures</li><li><strong>Cost</strong>: Minimize infrastructure costs while meeting functional requirements</li><li><strong>Future Flexibility</strong>: Ability to extend or customize as needs evolve</li></ul><h2 id=considered-options>Considered Options</h2><ul><li><strong>Option 1</strong>: Custom server implementation (Go-based)</li><li><strong>Option 2</strong>: Matchbox-based solution</li></ul><h2 id=decision-outcome>Decision Outcome</h2><p><strong>Status</strong>: Proposed (requires implementation proof-of-concept for final decision)</p><p><strong>Preliminary Recommendation</strong>: &ldquo;<strong>Option 2: Matchbox-based solution</strong>&rdquo;, because:</p><ol><li><strong>Production-Ready</strong>: Matchbox is battle-tested in production environments (CoreOS, Flatcar Linux deployments)</li><li><strong>Feature Complete</strong>: Provides HTTP/gRPC APIs, machine grouping, templating, and ignition/cloud-init support out of the box</li><li><strong>Time Savings</strong>: Avoids 2-4 weeks of development time for custom implementation</li><li><strong>GCP Compatible</strong>: Runs as containerized service on Cloud Run or Compute Engine with Cloud Storage backend</li><li><strong>Proven Architecture</strong>: Established patterns for boot image lifecycle and machine matching</li></ol><p>However, a proof-of-concept should validate:</p><ul><li>GCP Cloud Storage integration for boot assets</li><li>WireGuard VPN compatibility from UDM Pro</li><li>Performance for home lab scale (&lt; 50ms boot file request latency)</li><li>Operational complexity of running Matchbox vs custom Go service</li></ul><h3 id=consequences>Consequences</h3><ul><li>Good, because Matchbox provides production-ready boot server with minimal development</li><li>Good, because extensive documentation and community support available</li><li>Good, because supports advanced features (machine grouping, templating, ignition/cloud-init)</li><li>Good, because gRPC API enables programmatic boot configuration management</li><li>Good, because HTTP API provides observability into boot requests and machine states</li><li>Good, because runs as container, compatible with Cloud Run or GKE</li><li>Good, because reduces development time from weeks to days</li><li>Neutral, because introduces external dependency (Matchbox project maintenance)</li><li>Neutral, because requires learning Matchbox configuration patterns</li><li>Bad, because less control over boot server implementation details</li><li>Bad, because may include features unnecessary for home lab scale</li><li>Bad, because dependency on upstream Matchbox project for security patches</li></ul><h3 id=confirmation>Confirmation</h3><p>The implementation will be confirmed by:</p><ul><li>Deploying proof-of-concept Matchbox server on GCP Cloud Run or Compute Engine</li><li>Successfully network booting a test server via WireGuard VPN</li><li>Validating boot image upload and versioning workflows</li><li>Measuring boot file request latency (target: &lt; 50ms for TFTP, &lt; 100ms for HTTP)</li><li>Confirming Cloud Storage integration for boot asset storage</li><li>Testing machine-to-image mapping based on MAC address</li><li>Evaluating operational complexity vs custom implementation</li></ul><h2 id=pros-and-cons-of-the-options>Pros and Cons of the Options</h2><h3 id=option-1-custom-server-implementation-go-based>Option 1: Custom Server Implementation (Go-based)</h3><p>Build a custom network boot server in Go, leveraging the existing <code>z5labs/humus</code> framework for HTTP services.</p><h4 id=architecture-overview>Architecture Overview</h4><pre class=mermaid>architecture-beta
    group gcp(cloud)[GCP VPC]
    
    service wg_nlb(internet)[Network LB] in gcp
    service wireguard(server)[WireGuard Gateway] in gcp
    service https_lb(internet)[HTTPS LB] in gcp
    service compute(server)[Compute Engine] in gcp
    service storage(database)[Cloud Storage] in gcp
    service secrets(disk)[Secret Manager] in gcp
    service monitoring(internet)[Cloud Monitoring] in gcp
    
    group homelab(cloud)[Home Lab]
    service udm(server)[UDM Pro] in homelab
    service servers(server)[Bare Metal Servers] in homelab
    
    servers:L -- R:udm
    udm:R -- L:wg_nlb
    wg_nlb:R -- L:wireguard
    wireguard:R -- L:https_lb
    https_lb:R -- L:compute
    compute:B --&gt; T:storage
    compute:R --&gt; L:secrets
    compute:T --&gt; B:monitoring</pre><p><strong>Components</strong>:</p><ul><li><strong>Boot Server</strong>: Go service deployed to Cloud Run or Compute Engine VM<ul><li>TFTP server (using <code>pin/tftp</code> or similar Go library)</li><li>HTTP/HTTPS server (using <code>z5labs/humus</code> framework with OpenAPI)</li><li>gRPC admin API for boot configuration management</li></ul></li><li><strong>Cloud Storage</strong>: Buckets for boot images, iPXE scripts, kernels, initrd files</li><li><strong>Firestore/Datastore</strong>: Machine-to-image mapping database (MAC → boot profile)</li><li><strong>Secret Manager</strong>: WireGuard keys, TLS certificates</li><li><strong>Cloud Monitoring</strong>: Metrics for boot requests, success/failure rates, latency</li></ul><h4 id=boot-image-lifecycle>Boot Image Lifecycle</h4><pre class=mermaid>sequenceDiagram
    participant Admin
    participant API as Boot Server API
    participant Storage as Cloud Storage
    participant DB as Firestore
    participant Monitor as Cloud Monitoring

    Note over Admin,Monitor: Upload Boot Image
    Admin-&gt;&gt;API: POST /api/v1/images (kernel, initrd, metadata)
    API-&gt;&gt;API: Validate image integrity (checksum)
    API-&gt;&gt;Storage: Upload kernel to gs://boot-images/kernels/
    API-&gt;&gt;Storage: Upload initrd to gs://boot-images/initrd/
    API-&gt;&gt;DB: Store metadata (version, checksum, tags)
    API-&gt;&gt;Monitor: Log upload event
    API-&gt;&gt;Admin: 201 Created (image ID)

    Note over Admin,Monitor: Map Machine to Image
    Admin-&gt;&gt;API: POST /api/v1/machines (MAC, image_id, profile)
    API-&gt;&gt;DB: Store machine mapping
    API-&gt;&gt;Admin: 201 Created

    Note over Admin,Monitor: PXE Boot Request
    participant Server as Home Lab Server
    Server-&gt;&gt;API: TFTP GET /pxelinux.0 (via WireGuard VPN)
    API-&gt;&gt;Monitor: Log TFTP request (MAC, timestamp)
    API-&gt;&gt;Storage: Fetch iPXE binary from gs://boot-images/ipxe/
    API-&gt;&gt;Server: Send iPXE binary (via TFTP)
    
    Server-&gt;&gt;API: HTTP GET /boot?mac=aa:bb:cc:dd:ee:ff
    API-&gt;&gt;DB: Query machine mapping by MAC
    API-&gt;&gt;API: Generate iPXE script (kernel, initrd URLs)
    API-&gt;&gt;Monitor: Log boot script request
    API-&gt;&gt;Server: Send iPXE script
    
    Server-&gt;&gt;API: HTTP GET /kernels/ubuntu-22.04.img
    API-&gt;&gt;Storage: Fetch kernel from Cloud Storage
    API-&gt;&gt;Monitor: Log kernel download (size, duration)
    API-&gt;&gt;Server: Stream kernel file
    
    Server-&gt;&gt;API: HTTP GET /initrd/ubuntu-22.04.img
    API-&gt;&gt;Storage: Fetch initrd from Cloud Storage
    API-&gt;&gt;Monitor: Log initrd download
    API-&gt;&gt;Server: Stream initrd file
    
    Server-&gt;&gt;Server: Boot into OS
    
    Note over Admin,Monitor: Rollback Image Version
    Admin-&gt;&gt;API: POST /api/v1/machines/{mac}/rollback
    API-&gt;&gt;DB: Update machine mapping to previous image_id
    API-&gt;&gt;Monitor: Log rollback event
    API-&gt;&gt;Admin: 200 OK</pre><h4 id=implementation-details>Implementation Details</h4><p><strong>Development Stack</strong>:</p><ul><li><strong>Language</strong>: Go 1.24 (leverage existing Go expertise)</li><li><strong>HTTP Framework</strong>: <code>z5labs/humus</code> (consistent with existing services)</li><li><strong>TFTP Library</strong>: <code>pin/tftp</code> or <code>pack.ag/tftp</code></li><li><strong>Storage Client</strong>: <code>cloud.google.com/go/storage</code></li><li><strong>Database</strong>: Firestore for machine mappings (or simple JSON config in Cloud Storage)</li><li><strong>Observability</strong>: OpenTelemetry (metrics, traces, logs to Cloud Monitoring/Trace)</li></ul><p><strong>Deployment</strong>:</p><ul><li><strong>Compute Engine VM</strong> (Cloud Run doesn&rsquo;t support UDP for TFTP):<ul><li>e2-micro instance ($6.50/month)</li><li>Container-Optimized OS with Docker</li><li>systemd service for boot server</li><li>Health checks: <code>/health/startup</code>, <code>/health/liveness</code></li></ul></li><li><strong>Networking</strong>:<ul><li>VPC firewall: Allow UDP/69 (TFTP), TCP/80, TCP/443 from WireGuard subnet</li><li>Static internal IP for boot server</li><li>Cloud NAT for outbound connectivity (Cloud Storage access)</li></ul></li></ul><p><strong>Configuration Management</strong>:</p><ul><li>Machine mappings stored in Firestore or Cloud Storage JSON files</li><li>Boot profiles defined in YAML (similar to Matchbox groups):<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#204a87;font-weight:700>profiles</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>  </span>- <span style=color:#204a87;font-weight:700>name</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline> </span><span style=color:#000>ubuntu-22.04-server</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>    </span><span style=color:#204a87;font-weight:700>kernel</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline> </span><span style=color:#000>gs://boot-images/kernels/ubuntu-22.04.img</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>    </span><span style=color:#204a87;font-weight:700>initrd</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline> </span><span style=color:#000>gs://boot-images/initrd/ubuntu-22.04.img</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>    </span><span style=color:#204a87;font-weight:700>cmdline</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline> </span><span style=color:#4e9a06>&#34;console=tty0 console=ttyS0&#34;</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>    </span><span style=color:#204a87;font-weight:700>cloud_init</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline> </span><span style=color:#000>gs://boot-images/cloud-init/ubuntu-base.yaml</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline></span><span style=color:#204a87;font-weight:700>machines</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>  </span>- <span style=color:#204a87;font-weight:700>mac</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline> </span><span style=color:#4e9a06>&#34;aa:bb:cc:dd:ee:ff&#34;</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>    </span><span style=color:#204a87;font-weight:700>profile</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline> </span><span style=color:#000>ubuntu-22.04-server</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>    </span><span style=color:#204a87;font-weight:700>hostname</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline> </span><span style=color:#000>node-01</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span></code></pre></div></li></ul><p><strong>Cost Breakdown</strong>:</p><table><thead><tr><th>Component</th><th>Monthly Cost</th></tr></thead><tbody><tr><td>e2-micro VM (boot server)</td><td>$6.50</td></tr><tr><td>Cloud Storage (50GB boot images)</td><td>$1.00</td></tr><tr><td>Firestore (minimal reads/writes)</td><td>$0.50</td></tr><tr><td>Egress (10 boots × 150MB)</td><td>$0.18</td></tr><tr><td><strong>Total</strong></td><td><strong>~$8.18</strong></td></tr></tbody></table><h4 id=pros-and-cons>Pros and Cons</h4><ul><li>Good, because full control over boot server implementation and features</li><li>Good, because leverages existing Go expertise and <code>z5labs/humus</code> framework patterns</li><li>Good, because seamless GCP integration (Cloud Storage, Firestore, Secret Manager, IAM)</li><li>Good, because minimal dependencies (no external projects to track)</li><li>Good, because customizable to specific home lab requirements</li><li>Good, because OpenTelemetry observability built-in from existing patterns</li><li>Good, because can optimize for home lab scale (&lt; 20 machines)</li><li>Good, because lightweight implementation (no unnecessary features)</li><li>Neutral, because development effort required (2-4 weeks for MVP)</li><li>Neutral, because requires ongoing maintenance and security updates</li><li>Bad, because reinvents established boot server patterns (TFTP, HTTP serving, machine matching)</li><li>Bad, because testing complexity (TFTP, PXE workflows, network boot scenarios)</li><li>Bad, because potential for bugs in custom TFTP/HTTP implementation</li><li>Bad, because no community support or established best practices</li><li>Bad, because delayed time to production (weeks vs days)</li></ul><h3 id=option-2-matchbox-based-solution>Option 2: Matchbox-Based Solution</h3><p>Deploy <a href=https://matchbox.psdn.io/>Matchbox</a>, an open-source network boot server developed by CoreOS (now part of Red Hat), to handle PXE/iPXE boot workflows.</p><h4 id=architecture-overview-1>Architecture Overview</h4><pre class=mermaid>architecture-beta
    group gcp(cloud)[GCP VPC]
    
    service wg_nlb(internet)[Network LB] in gcp
    service wireguard(server)[WireGuard Gateway] in gcp
    service https_lb(internet)[HTTPS LB] in gcp
    service compute(server)[Compute Engine] in gcp
    service storage(database)[Cloud Storage] in gcp
    service secrets(disk)[Secret Manager] in gcp
    service monitoring(internet)[Cloud Monitoring] in gcp
    
    group homelab(cloud)[Home Lab]
    service udm(server)[UDM Pro] in homelab
    service servers(server)[Bare Metal Servers] in homelab
    
    servers:L -- R:udm
    udm:R -- L:wg_nlb
    wg_nlb:R -- L:wireguard
    wireguard:R -- L:https_lb
    https_lb:R -- L:compute
    compute:B --&gt; T:storage
    compute:R --&gt; L:secrets
    compute:T --&gt; B:monitoring</pre><p><strong>Components</strong>:</p><ul><li><strong>Matchbox Server</strong>: Container deployed to Compute Engine VM<ul><li>HTTP/gRPC APIs for boot workflows and configuration</li><li>TFTP support via built-in server</li><li>Machine grouping and profile templating</li><li>Ignition, Cloud-Init, and generic boot support</li></ul></li><li><strong>Cloud Storage</strong>: Backend for boot assets (mounted via gcsfuse or synced periodically)</li><li><strong>Local Storage</strong>: <code>/var/lib/matchbox</code> for assets and configuration (synced from Cloud Storage)</li><li><strong>Secret Manager</strong>: WireGuard keys, Matchbox TLS certificates</li><li><strong>Cloud Monitoring</strong>: Logs from Matchbox container, custom metrics via log parsing</li></ul><h4 id=boot-image-lifecycle-1>Boot Image Lifecycle</h4><pre class=mermaid>sequenceDiagram
    participant Admin
    participant CLI as matchbox CLI / API
    participant Matchbox as Matchbox Server
    participant Storage as Cloud Storage
    participant Monitor as Cloud Monitoring

    Note over Admin,Monitor: Upload Boot Image
    Admin-&gt;&gt;CLI: Upload kernel/initrd via gRPC API
    CLI-&gt;&gt;Matchbox: gRPC CreateAsset(kernel, initrd)
    Matchbox-&gt;&gt;Matchbox: Validate asset integrity
    Matchbox-&gt;&gt;Matchbox: Store to /var/lib/matchbox/assets/
    Matchbox-&gt;&gt;Storage: Sync to gs://boot-assets/ (via sidecar script)
    Matchbox-&gt;&gt;Monitor: Log asset upload event
    Matchbox-&gt;&gt;CLI: Asset ID, checksum

    Note over Admin,Monitor: Create Boot Profile
    Admin-&gt;&gt;CLI: Create profile YAML (kernel, initrd, cmdline)
    CLI-&gt;&gt;Matchbox: gRPC CreateProfile(profile.yaml)
    Matchbox-&gt;&gt;Matchbox: Store to /var/lib/matchbox/profiles/
    Matchbox-&gt;&gt;Storage: Sync profiles to gs://boot-config/
    Matchbox-&gt;&gt;CLI: Profile ID

    Note over Admin,Monitor: Create Machine Group
    Admin-&gt;&gt;CLI: Create group YAML (MAC selector, profile mapping)
    CLI-&gt;&gt;Matchbox: gRPC CreateGroup(group.yaml)
    Matchbox-&gt;&gt;Matchbox: Store to /var/lib/matchbox/groups/
    Matchbox-&gt;&gt;Storage: Sync groups to gs://boot-config/
    Matchbox-&gt;&gt;CLI: Group ID

    Note over Admin,Monitor: PXE Boot Request
    participant Server as Home Lab Server
    Server-&gt;&gt;Matchbox: TFTP GET /undionly.kpxe (via WireGuard VPN)
    Matchbox-&gt;&gt;Monitor: Log TFTP request
    Matchbox-&gt;&gt;Server: Send iPXE binary
    
    Server-&gt;&gt;Matchbox: HTTP GET /boot.ipxe?mac=aa:bb:cc:dd:ee:ff
    Matchbox-&gt;&gt;Matchbox: Match MAC to group
    Matchbox-&gt;&gt;Matchbox: Render iPXE template with profile
    Matchbox-&gt;&gt;Monitor: Log boot request (MAC, group, profile)
    Matchbox-&gt;&gt;Server: Send iPXE script
    
    Server-&gt;&gt;Matchbox: HTTP GET /assets/ubuntu-22.04-kernel.img
    Matchbox-&gt;&gt;Matchbox: Serve from /var/lib/matchbox/assets/
    Matchbox-&gt;&gt;Monitor: Log asset download (size, duration)
    Matchbox-&gt;&gt;Server: Stream kernel file
    
    Server-&gt;&gt;Matchbox: HTTP GET /assets/ubuntu-22.04-initrd.img
    Matchbox-&gt;&gt;Matchbox: Serve from /var/lib/matchbox/assets/
    Matchbox-&gt;&gt;Monitor: Log asset download
    Matchbox-&gt;&gt;Server: Stream initrd file
    
    Server-&gt;&gt;Server: Boot into OS
    
    Note over Admin,Monitor: Rollback Machine Group
    Admin-&gt;&gt;CLI: Update group YAML (change profile reference)
    CLI-&gt;&gt;Matchbox: gRPC UpdateGroup(group.yaml)
    Matchbox-&gt;&gt;Matchbox: Update /var/lib/matchbox/groups/
    Matchbox-&gt;&gt;Storage: Sync updated group config
    Matchbox-&gt;&gt;Monitor: Log group update
    Matchbox-&gt;&gt;CLI: Success</pre><h4 id=implementation-details-1>Implementation Details</h4><p><strong>Matchbox Deployment</strong>:</p><ul><li><strong>Container</strong>: <code>quay.io/poseidon/matchbox:latest</code> (official image)</li><li><strong>Compute Engine VM</strong>: e2-small instance ($14/month, 2GB RAM recommended for Matchbox)</li><li><strong>Storage</strong>:<ul><li><code>/var/lib/matchbox</code>: Persistent disk (10GB SSD, $1.70/month)</li><li>Cloud Storage sync: Periodic backup of assets/profiles/groups to <code>gs://matchbox-config/</code></li><li>Option: Use <code>gcsfuse</code> to mount Cloud Storage directly (adds latency but simplifies backups)</li></ul></li></ul><p><strong>Configuration Structure</strong>:</p><pre tabindex=0><code>/var/lib/matchbox/
├── assets/           # Boot images (kernels, initrds, ISOs)
│   ├── ubuntu-22.04-kernel.img
│   ├── ubuntu-22.04-initrd.img
│   └── flatcar-stable.img.gz
├── profiles/         # Boot profiles (YAML)
│   ├── ubuntu-server.yaml
│   └── flatcar-container.yaml
└── groups/           # Machine groups (YAML)
    ├── default.yaml
    ├── node-01.yaml
    └── storage-nodes.yaml
</code></pre><p><strong>Example Profile</strong> (<code>profiles/ubuntu-server.yaml</code>):</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#204a87;font-weight:700>id</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline> </span><span style=color:#000>ubuntu-22.04-server</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline></span><span style=color:#204a87;font-weight:700>name</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline> </span><span style=color:#000>Ubuntu 22.04 LTS Server</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline></span><span style=color:#204a87;font-weight:700>boot</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>  </span><span style=color:#204a87;font-weight:700>kernel</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline> </span><span style=color:#000>/assets/ubuntu-22.04-kernel.img</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>  </span><span style=color:#204a87;font-weight:700>initrd</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>    </span>- <span style=color:#000>/assets/ubuntu-22.04-initrd.img</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>  </span><span style=color:#204a87;font-weight:700>args</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>    </span>- <span style=color:#000>console=tty0</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>    </span>- <span style=color:#000>console=ttyS0</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>    </span>- <span style=color:#000>ip=dhcp</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline></span><span style=color:#204a87;font-weight:700>ignition_id</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline> </span><span style=color:#000>ubuntu-base.yaml</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span></code></pre></div><p><strong>Example Group</strong> (<code>groups/node-01.yaml</code>):</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#204a87;font-weight:700>id</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline> </span><span style=color:#000>node-01</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline></span><span style=color:#204a87;font-weight:700>name</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline> </span><span style=color:#000>Node 01 - Ubuntu Server</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline></span><span style=color:#204a87;font-weight:700>profile</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline> </span><span style=color:#000>ubuntu-22.04-server</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline></span><span style=color:#204a87;font-weight:700>selector</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>  </span><span style=color:#204a87;font-weight:700>mac</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline> </span><span style=color:#4e9a06>&#34;aa:bb:cc:dd:ee:ff&#34;</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline></span><span style=color:#204a87;font-weight:700>metadata</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>  </span><span style=color:#204a87;font-weight:700>hostname</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline> </span><span style=color:#000>node-01.homelab.local</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>  </span><span style=color:#204a87;font-weight:700>ssh_authorized_keys</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>    </span>- <span style=color:#4e9a06>&#34;ssh-ed25519 AAAA...&#34;</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span></code></pre></div><p><strong>GCP Integration</strong>:</p><ul><li><strong>Cloud Storage Sync</strong>: Cron job or sidecar container to sync <code>/var/lib/matchbox</code> to Cloud Storage<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Sync every 5 minutes</span>
</span></span><span style=display:flex><span>*/5 * * * * gsutil -m rsync -r /var/lib/matchbox gs://matchbox-config/
</span></span></code></pre></div></li><li><strong>Secret Manager</strong>: Store Matchbox TLS certificates for gRPC API authentication</li><li><strong>Cloud Monitoring</strong>: Ship Matchbox logs to Cloud Logging, parse for metrics:<ul><li>Boot request count by MAC/group</li><li>Asset download success/failure rates</li><li>TFTP vs HTTP request distribution</li></ul></li></ul><p><strong>Networking</strong>:</p><ul><li>VPC firewall: Allow UDP/69 (TFTP), TCP/8080 (HTTP), TCP/8081 (gRPC) from WireGuard subnet</li><li>Optional: Internal load balancer if high availability required (adds ~$18/month)</li></ul><p><strong>Cost Breakdown</strong>:</p><table><thead><tr><th>Component</th><th>Monthly Cost</th></tr></thead><tbody><tr><td>e2-small VM (Matchbox server)</td><td>$14.00</td></tr><tr><td>Persistent SSD (10GB)</td><td>$1.70</td></tr><tr><td>Cloud Storage (50GB backups)</td><td>$1.00</td></tr><tr><td>Egress (10 boots × 150MB)</td><td>$0.18</td></tr><tr><td><strong>Total</strong></td><td><strong>~$16.88</strong></td></tr></tbody></table><p><strong>Note</strong>: Higher cost than custom implementation due to Matchbox&rsquo;s recommended 2GB RAM (e2-small vs e2-micro).</p><h4 id=pros-and-cons-1>Pros and Cons</h4><ul><li>Good, because production-ready boot server with extensive real-world usage</li><li>Good, because feature-complete with machine grouping, templating, and multi-OS support</li><li>Good, because gRPC API for programmatic boot configuration management</li><li>Good, because supports Ignition (Flatcar, CoreOS), Cloud-Init, and generic boot workflows</li><li>Good, because well-documented with established best practices</li><li>Good, because active community and upstream maintenance (Red Hat/CoreOS)</li><li>Good, because reduces development time to days (deploy + configure vs weeks of coding)</li><li>Good, because avoids reinventing network boot patterns (TFTP, iPXE, machine matching)</li><li>Good, because proven security model (TLS for gRPC, asset integrity checks)</li><li>Neutral, because requires learning Matchbox configuration patterns (YAML profiles/groups)</li><li>Neutral, because containerized deployment (Docker on Compute Engine or Cloud Run)</li><li>Bad, because introduces external dependency (Matchbox project maintenance)</li><li>Bad, because higher resource requirements (2GB RAM recommended = e2-small at $14/month vs e2-micro at $6.50/month)</li><li>Bad, because some features unnecessary for home lab scale (large-scale provisioning, etcd backend)</li><li>Bad, because less control over implementation details (limited customization)</li><li>Bad, because Cloud Storage integration requires custom sync scripts (Matchbox doesn&rsquo;t natively support GCS backend)</li><li>Bad, because dependency on upstream for security patches and bug fixes</li></ul><h2 id=detailed-comparison>Detailed Comparison</h2><h3 id=feature-comparison>Feature Comparison</h3><table><thead><tr><th>Feature</th><th>Custom Implementation</th><th>Matchbox</th></tr></thead><tbody><tr><td><strong>TFTP Support</strong></td><td>✅ Via Go library</td><td>✅ Built-in</td></tr><tr><td><strong>HTTP/HTTPS Boot</strong></td><td>✅ Via z5labs/humus</td><td>✅ Built-in</td></tr><tr><td><strong>iPXE Scripting</strong></td><td>✅ Custom templates</td><td>✅ Go templates</td></tr><tr><td><strong>Machine-to-Image Mapping</strong></td><td>✅ Firestore/JSON</td><td>✅ YAML groups with selectors</td></tr><tr><td><strong>Boot Profile Management</strong></td><td>✅ Custom API</td><td>✅ gRPC API + YAML</td></tr><tr><td><strong>Cloud-Init Support</strong></td><td>⚠️ Requires implementation</td><td>✅ Native support</td></tr><tr><td><strong>Ignition Support</strong></td><td>❌ Not planned</td><td>✅ Native support (Flatcar, CoreOS)</td></tr><tr><td><strong>Asset Versioning</strong></td><td>⚠️ Requires implementation</td><td>⚠️ Manual (via Cloud Storage versioning)</td></tr><tr><td><strong>Rollback Capability</strong></td><td>⚠️ Requires implementation</td><td>✅ Update group to previous profile</td></tr><tr><td><strong>OpenTelemetry Observability</strong></td><td>✅ Built-in</td><td>⚠️ Logs only (requires parsing)</td></tr><tr><td><strong>GCP Cloud Storage Integration</strong></td><td>✅ Native SDK</td><td>⚠️ Requires sync scripts</td></tr><tr><td><strong>gRPC Admin API</strong></td><td>⚠️ Requires implementation</td><td>✅ Built-in</td></tr><tr><td><strong>Multi-Environment Support</strong></td><td>⚠️ Requires implementation</td><td>✅ Groups + metadata</td></tr></tbody></table><h3 id=development-effort-comparison>Development Effort Comparison</h3><table><thead><tr><th>Task</th><th>Custom Implementation</th><th>Matchbox</th></tr></thead><tbody><tr><td><strong>Initial Setup</strong></td><td>1-2 days (project scaffolding)</td><td>4-8 hours (deployment + config)</td></tr><tr><td><strong>TFTP Server</strong></td><td>3-5 days (library integration, testing)</td><td>✅ Included</td></tr><tr><td><strong>HTTP Boot API</strong></td><td>2-3 days (z5labs/humus endpoints)</td><td>✅ Included</td></tr><tr><td><strong>Machine Matching Logic</strong></td><td>2-3 days (database queries, selectors)</td><td>✅ Included</td></tr><tr><td><strong>Boot Profile Templates</strong></td><td>2-3 days (iPXE templating)</td><td>✅ Included</td></tr><tr><td><strong>Cloud-Init Support</strong></td><td>3-5 days (parsing, injection)</td><td>✅ Included</td></tr><tr><td><strong>Asset Management</strong></td><td>2-3 days (upload, storage)</td><td>✅ Included</td></tr><tr><td><strong>gRPC Admin API</strong></td><td>3-5 days (proto definitions, server)</td><td>✅ Included</td></tr><tr><td><strong>Testing</strong></td><td>5-7 days (unit, integration, E2E)</td><td>2-3 days (integration only)</td></tr><tr><td><strong>Documentation</strong></td><td>2-3 days</td><td>1 day (reference existing docs)</td></tr><tr><td><strong>Total Effort</strong></td><td><strong>3-4 weeks</strong></td><td><strong>1 week</strong></td></tr></tbody></table><h3 id=operational-complexity>Operational Complexity</h3><table><thead><tr><th>Aspect</th><th>Custom Implementation</th><th>Matchbox</th></tr></thead><tbody><tr><td><strong>Deployment</strong></td><td>Docker container on Compute Engine</td><td>Docker container on Compute Engine</td></tr><tr><td><strong>Configuration Updates</strong></td><td>API calls or Terraform updates</td><td>YAML file updates + API/filesystem sync</td></tr><tr><td><strong>Monitoring</strong></td><td>OpenTelemetry metrics to Cloud Monitoring</td><td>Log parsing + custom metrics</td></tr><tr><td><strong>Troubleshooting</strong></td><td>Full access to code, custom logging</td><td>Matchbox logs + gRPC API inspection</td></tr><tr><td><strong>Security Patches</strong></td><td>Manual code updates</td><td>Upstream container image updates</td></tr><tr><td><strong>Dependency Updates</strong></td><td>Manual Go module updates</td><td>Upstream Matchbox updates</td></tr><tr><td><strong>Backup/Restore</strong></td><td>Cloud Storage + Firestore backups</td><td>Sync <code>/var/lib/matchbox</code> to Cloud Storage</td></tr></tbody></table><h3 id=cost-comparison-summary>Cost Comparison Summary</h3><table><thead><tr><th>Item</th><th>Custom</th><th>Matchbox</th><th>Difference</th></tr></thead><tbody><tr><td><strong>Compute</strong></td><td>e2-micro ($6.50/month)</td><td>e2-small ($14/month)</td><td>+$7.50/month</td></tr><tr><td><strong>Storage</strong></td><td>Cloud Storage only ($1/month)</td><td>Persistent disk + Cloud Storage ($2.70/month)</td><td>+$1.70/month</td></tr><tr><td><strong>Development</strong></td><td>3-4 weeks @ $100/hour = $12,000-16,000</td><td>1 week @ $100/hour = $4,000</td><td>-$8,000-12,000</td></tr><tr><td><strong>Annual Infrastructure</strong></td><td>~$98</td><td>~$203</td><td>+$105/year</td></tr><tr><td><strong>TCO (Year 1)</strong></td><td>~$12,098-16,098</td><td>~$4,203</td><td><strong>-$7,895-11,895</strong></td></tr><tr><td><strong>TCO (Year 3)</strong></td><td>~$12,294-16,294</td><td>~$4,609</td><td><strong>-$7,685-11,685</strong></td></tr></tbody></table><p><strong>Key Insight</strong>: Even with higher ongoing infrastructure costs, Matchbox&rsquo;s total cost of ownership is significantly lower due to reduced development time.</p><h3 id=risk-analysis>Risk Analysis</h3><table><thead><tr><th>Risk</th><th>Custom Implementation</th><th>Matchbox</th><th>Mitigation</th></tr></thead><tbody><tr><td><strong>Security Vulnerabilities</strong></td><td>High (custom TFTP/HTTP code)</td><td>Medium (upstream dependency)</td><td>Matchbox: Monitor upstream releases, automated updates</td></tr><tr><td><strong>Boot Failures</strong></td><td>High (untested edge cases)</td><td>Low (battle-tested)</td><td>Custom: Comprehensive E2E testing</td></tr><tr><td><strong>Maintenance Burden</strong></td><td>High (ongoing code maintenance)</td><td>Low (upstream handles updates)</td><td>Both: Automated deployment pipelines</td></tr><tr><td><strong>GCP Integration Issues</strong></td><td>Low (native SDK)</td><td>Medium (sync scripts)</td><td>Matchbox: Robust sync with error handling</td></tr><tr><td><strong>Scalability Limits</strong></td><td>Unknown (requires load testing)</td><td>Known (handles thousands of nodes)</td><td>Both: Monitor boot request latency</td></tr><tr><td><strong>Dependency Abandonment</strong></td><td>N/A (no external deps)</td><td>Low (Red Hat backing)</td><td>Matchbox: Can fork if necessary</td></tr></tbody></table><h2 id=proof-of-concept-plan>Proof-of-Concept Plan</h2><p>Before final decision, implement a basic proof-of-concept for both options:</p><h3 id=custom-implementation-poc-1-week>Custom Implementation POC (1 week)</h3><ol><li>Basic TFTP server serving iPXE binary (2 days)</li><li>HTTP endpoint serving iPXE script with static kernel/initrd URLs (1 day)</li><li>Simple MAC-based machine matching (1 day)</li><li>Deploy to Compute Engine VM, test via WireGuard VPN (1 day)</li><li>Measure: boot latency, development complexity, code quality</li></ol><h3 id=matchbox-poc-3-days>Matchbox POC (3 days)</h3><ol><li>Deploy Matchbox container to Compute Engine (1 day)</li><li>Configure profile and group YAML for Ubuntu boot (1 day)</li><li>Test boot via WireGuard VPN, validate Cloud Storage sync (1 day)</li><li>Measure: boot latency, configuration complexity, operational overhead</li></ol><h3 id=poc-success-criteria>POC Success Criteria</h3><ul><li>✅ Successfully boot test server via PXE/iPXE</li><li>✅ Boot latency &lt; 50ms for TFTP, &lt; 100ms for HTTP</li><li>✅ Machine-to-image mapping works correctly</li><li>✅ Cloud Storage integration functional (upload, retrieve boot assets)</li><li>✅ Logs available in Cloud Monitoring for troubleshooting</li><li>✅ Configuration update workflow clear and documented</li></ul><h2 id=more-information>More Information</h2><h3 id=related-resources>Related Resources</h3><ul><li><a href=https://matchbox.psdn.io/>Matchbox Documentation</a></li><li><a href=https://github.com/poseidon/matchbox>Matchbox GitHub Repository</a></li><li><a href=https://ipxe.org/howto/chainloading>iPXE Boot Process</a></li><li><a href=https://en.wikipedia.org/wiki/Preboot_Execution_Environment>PXE Boot Specification</a></li><li><a href=https://www.flatcar.org/docs/latest/provisioning/network-boot/>Flatcar Linux Provisioning with Matchbox</a></li><li><a href=https://coreos.github.io/ignition/>CoreOS Ignition Specification</a></li><li><a href=https://cloudinit.readthedocs.io/>Cloud-Init Documentation</a></li></ul><h3 id=related-adrs>Related ADRs</h3><ul><li><a href=./0002-network-boot-architecture/>ADR-0002: Network Boot Architecture</a> - Established cloud-hosted boot server with VPN</li><li><a href=./0003-cloud-provider-selection/>ADR-0003: Cloud Provider Selection</a> - Selected GCP as hosting provider</li><li><a href=./0001-use-madr-for-architecture-decision-records/>ADR-0001: Use MADR for Architecture Decision Records</a> - MADR format</li></ul><h3 id=future-considerations>Future Considerations</h3><ol><li><strong>High Availability</strong>: If boot server uptime becomes critical, evaluate multi-region deployment or failover strategies</li><li><strong>Multi-Cloud</strong>: If multi-cloud strategy emerges, custom implementation may provide better portability</li><li><strong>Enterprise Features</strong>: If advanced provisioning workflows required (bare metal Kubernetes, etc.), Matchbox provides stronger foundation</li><li><strong>Integration with Existing Services</strong>: Custom implementation could leverage existing <code>z5labs/humus</code> patterns and shared infrastructure (monitoring, secrets, etc.)</li></ol><h3 id=related-issues>Related Issues</h3><ul><li><a href=https://github.com/Zaba505/infra/issues/601>Issue #601</a> - story(docs): create adr for network boot infrastructure on google cloud</li><li><a href=https://github.com/Zaba505/infra/issues/595>Issue #595</a> - story(docs): create adr for network boot architecture</li><li><a href=https://github.com/Zaba505/infra/issues/597>Issue #597</a> - story(docs): create adr for cloud provider selection</li></ul></div></main></div></div><footer class="td-footer row d-print-none"><div class=container-fluid><div class="row mx-md-2"><div class="td-footer__left col-6 col-sm-4 order-sm-1"></div><div class="td-footer__right col-6 col-sm-4 order-sm-3"><ul class=td-footer__links-list><li class=td-footer__links-item data-bs-toggle=tooltip title=GitHub aria-label=GitHub><a target=_blank rel=noopener href=https://github.com/Zaba505/infra aria-label=GitHub><i class="fab fa-github"></i></a></li></ul></div><div class="td-footer__center col-12 col-sm-4 py-2 order-sm-2"><span class=td-footer__copyright>&copy;
2024&ndash;2025
<span class=td-footer__authors>Zaba505 | <a href=https://creativecommons.org/licenses/by/4.0>CC BY 4.0</a> |</span></span><span class=td-footer__all_rights_reserved>All Rights Reserved</span></div></div></div></footer></div><script src=/infra/pr-preview/pr-602/js/main.min.eb40505784d893e4b5c8dbd67b59c353e735d847f4ffbfe9d6921dec08dbacba.js integrity="sha256-60BQV4TYk+S1yNvWe1nDU+c12Ef0/7/p1pId7AjbrLo=" crossorigin=anonymous></script><script defer src=/infra/pr-preview/pr-602/js/click-to-copy.min.73478a7d4807698aed7e355eb23f9890ca18fea3158604c8471746d046702bad.js integrity="sha256-c0eKfUgHaYrtfjVesj+YkMoY/qMVhgTIRxdG0EZwK60=" crossorigin=anonymous></script><script src=/infra/pr-preview/pr-602/js/tabpane-persist.js></script></body></html>