<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><link rel=canonical type=text/html href=https://zaba505.github.io/infra/pr-preview/pr-626/rd/adrs/><link rel=alternate type=application/rss+xml href=https://zaba505.github.io/infra/pr-preview/pr-626/rd/adrs/index.xml><meta name=robots content="noindex, nofollow"><link rel="shortcut icon" href=/infra/pr-preview/pr-626/favicons/favicon.ico><link rel=apple-touch-icon href=/infra/pr-preview/pr-626/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=icon type=image/png href=/infra/pr-preview/pr-626/favicons/favicon-16x16.png sizes=16x16><link rel=icon type=image/png href=/infra/pr-preview/pr-626/favicons/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/infra/pr-preview/pr-626/favicons/android-36x36.png sizes=36x36><link rel=icon type=image/png href=/infra/pr-preview/pr-626/favicons/android-48x48.png sizes=48x48><link rel=icon type=image/png href=/infra/pr-preview/pr-626/favicons/android-72x72.png sizes=72x72><link rel=icon type=image/png href=/infra/pr-preview/pr-626/favicons/android-96x96.png sizes=96x96><link rel=icon type=image/png href=/infra/pr-preview/pr-626/favicons/android-144x144.png sizes=144x144><link rel=icon type=image/png href=/infra/pr-preview/pr-626/favicons/android-192x192.png sizes=192x192><title>Architecture Decision Records | Zaba505's Home Lab</title><meta name=description content="Documentation of architectural decisions made using MADR 4.0.0 standard"><meta property="og:url" content="https://zaba505.github.io/infra/pr-preview/pr-626/rd/adrs/"><meta property="og:site_name" content="Zaba505's Home Lab"><meta property="og:title" content="Architecture Decision Records"><meta property="og:description" content="Documentation of architectural decisions made using MADR 4.0.0 standard"><meta property="og:locale" content="en_us"><meta property="og:type" content="website"><meta itemprop=name content="Architecture Decision Records"><meta itemprop=description content="Documentation of architectural decisions made using MADR 4.0.0 standard"><meta itemprop=dateModified content="2025-11-23T22:23:13+00:00"><meta itemprop=wordCount content="195"><meta name=twitter:card content="summary"><meta name=twitter:title content="Architecture Decision Records"><meta name=twitter:description content="Documentation of architectural decisions made using MADR 4.0.0 standard"><link rel=preload href=/infra/pr-preview/pr-626/scss/main.min.74eef40c5172b0e2f11bd9c3ea40dba66c2dc642ac5294c208f5dc9ff772c0e9.css as=style integrity="sha256-dO70DFFysOLxG9nD6kDbpmwtxkKsUpTCCPXcn/dywOk=" crossorigin=anonymous><link href=/infra/pr-preview/pr-626/scss/main.min.74eef40c5172b0e2f11bd9c3ea40dba66c2dc642ac5294c208f5dc9ff772c0e9.css rel=stylesheet integrity="sha256-dO70DFFysOLxG9nD6kDbpmwtxkKsUpTCCPXcn/dywOk=" crossorigin=anonymous><script src=https://code.jquery.com/jquery-3.7.1.min.js integrity="sha512-v2CJ7UaYy4JwqLDIrZUI/4hqeoQieOmAZNXBeQyjo21dadnwR+8ZaIJVT8EE2iyI61OV8e6M8PP2/4hpQINQ/g==" crossorigin=anonymous></script><script defer src=https://unpkg.com/lunr@2.3.9/lunr.min.js integrity=sha384-203J0SNzyqHby3iU6hzvzltrWi/M41wOP5Gu+BiJMz5nwKykbkUx8Kp7iti0Lpli crossorigin=anonymous></script></head><body class=td-section><header><nav class="td-navbar js-navbar-scroll" data-bs-theme=dark><div class="container-fluid flex-column flex-md-row"><a class=navbar-brand href=/infra/pr-preview/pr-626/><span class="navbar-brand__logo navbar-logo"></span><span class=navbar-brand__name>Zaba505's Home Lab</span></a><div class="td-navbar-nav-scroll ms-md-auto" id=main_navbar><ul class=navbar-nav><li class="td-light-dark-menu nav-item dropdown"><svg class="d-none"><symbol id="check2" viewBox="0 0 16 16"><path d="M13.854 3.646a.5.5.0 010 .708l-7 7a.5.5.0 01-.708.0l-3.5-3.5a.5.5.0 11.708-.708L6.5 10.293l6.646-6.647a.5.5.0 01.708.0z"/></symbol><symbol id="circle-half" viewBox="0 0 16 16"><path d="M8 15A7 7 0 108 1v14zm0 1A8 8 0 118 0a8 8 0 010 16z"/></symbol><symbol id="moon-stars-fill" viewBox="0 0 16 16"><path d="M6 .278a.768.768.0 01.08.858 7.208 7.208.0 00-.878 3.46c0 4.021 3.278 7.277 7.318 7.277.527.0 1.04-.055 1.533-.16a.787.787.0 01.81.316.733.733.0 01-.031.893A8.349 8.349.0 018.344 16C3.734 16 0 12.286.0 7.71.0 4.266 2.114 1.312 5.124.06A.752.752.0 016 .278z"/><path d="M10.794 3.148a.217.217.0 01.412.0l.387 1.162c.173.518.579.924 1.097 1.097l1.162.387a.217.217.0 010 .412l-1.162.387A1.734 1.734.0 0011.593 7.69l-.387 1.162a.217.217.0 01-.412.0l-.387-1.162A1.734 1.734.0 009.31 6.593l-1.162-.387a.217.217.0 010-.412l1.162-.387a1.734 1.734.0 001.097-1.097l.387-1.162zM13.863.099a.145.145.0 01.274.0l.258.774c.115.346.386.617.732.732l.774.258a.145.145.0 010 .274l-.774.258a1.156 1.156.0 00-.732.732l-.258.774a.145.145.0 01-.274.0l-.258-.774a1.156 1.156.0 00-.732-.732l-.774-.258a.145.145.0 010-.274l.774-.258c.346-.115.617-.386.732-.732L13.863.1z"/></symbol><symbol id="sun-fill" viewBox="0 0 16 16"><path d="M8 12a4 4 0 100-8 4 4 0 000 8zM8 0a.5.5.0 01.5.5v2a.5.5.0 01-1 0v-2A.5.5.0 018 0zm0 13a.5.5.0 01.5.5v2a.5.5.0 01-1 0v-2A.5.5.0 018 13zm8-5a.5.5.0 01-.5.5h-2a.5.5.0 010-1h2a.5.5.0 01.5.5zM3 8a.5.5.0 01-.5.5h-2a.5.5.0 010-1h2A.5.5.0 013 8zm10.657-5.657a.5.5.0 010 .707l-1.414 1.415a.5.5.0 11-.707-.708l1.414-1.414a.5.5.0 01.707.0zm-9.193 9.193a.5.5.0 010 .707L3.05 13.657a.5.5.0 01-.707-.707l1.414-1.414a.5.5.0 01.707.0zm9.193 2.121a.5.5.0 01-.707.0l-1.414-1.414a.5.5.0 01.707-.707l1.414 1.414a.5.5.0 010 .707zM4.464 4.465a.5.5.0 01-.707.0L2.343 3.05a.5.5.0 11.707-.707l1.414 1.414a.5.5.0 010 .708z"/></symbol></svg>
<button class="btn btn-link nav-link dropdown-toggle d-flex align-items-center" id=bd-theme type=button aria-expanded=false data-bs-toggle=dropdown data-bs-display=static aria-label="Toggle theme (auto)">
<svg class="bi my-1 theme-icon-active"><use href="#circle-half"/></svg></button><ul class="dropdown-menu dropdown-menu-end" aria-labelledby=bd-theme-text><li><button type=button class="dropdown-item d-flex align-items-center" data-bs-theme-value=light aria-pressed=false>
<svg class="bi me-2 opacity-50"><use href="#sun-fill"/></svg>
Light
<svg class="bi ms-auto d-none"><use href="#check2"/></svg></button></li><li><button type=button class="dropdown-item d-flex align-items-center" data-bs-theme-value=dark aria-pressed=false>
<svg class="bi me-2 opacity-50"><use href="#moon-stars-fill"/></svg>
Dark
<svg class="bi ms-auto d-none"><use href="#check2"/></svg></button></li><li><button type=button class="dropdown-item d-flex align-items-center active" data-bs-theme-value=auto aria-pressed=true>
<svg class="bi me-2 opacity-50"><use href="#circle-half"/></svg>
Auto
<svg class="bi ms-auto d-none"><use href="#check2"/></svg></button></li></ul></li></ul></div><div class="d-none d-lg-block"><div class="td-search td-search--offline"><div class=td-search__icon></div><input type=search class="td-search__input form-control" placeholder="Search this site…" aria-label="Search this site…" autocomplete=off data-offline-search-index-json-src=/infra/pr-preview/pr-626/offline-search-index.4c593ab7fffc4e20f3e28b001d0839bd.json data-offline-search-base-href=/ data-offline-search-max-results=10></div></div></div></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><main class="col-12 col-md-9 col-xl-8 ps-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>This is the multi-page printable view of this section.
<a href=# onclick="return print(),!1">Click here to print</a>.</p><p><a href=/infra/pr-preview/pr-626/rd/adrs/>Return to the regular view of this page</a>.</p></div><h1 class=title>Architecture Decision Records</h1><div class=lead>Documentation of architectural decisions made using MADR 4.0.0 standard</div><ul><li>1: <a href=#pg-be14bb13e79709979af80fa8e61452c5>[0001] Use MADR for Architecture Decision Records</a></li><li>2: <a href=#pg-79e3b41ba15ba3a179ad4b5df5f3f2fe>[0002] Network Boot Architecture for Home Lab</a></li><li>3: <a href=#pg-853f5ec590d44937c1ebd528cd046965>[0003] Cloud Provider Selection for Network Boot Infrastructure</a></li><li>4: <a href=#pg-dab026fd06f98a03a459f97073d21662>[0004] Server Operating System Selection</a></li><li>5: <a href=#pg-be9e21cdab9183bad0c60fa6e3ba225b>[0005] Network Boot Infrastructure Implementation on Google Cloud</a></li><li>6: <a href=#pg-d234efec001fb7b9899c0a45bea1ae5d>[0006] Universal Resource Identifier Standard</a></li></ul><div class=content><h2 id=architecture-decision-records-adrs>Architecture Decision Records (ADRs)</h2><p>This section contains architectural decision records that document the key design choices made. Each ADR follows the MADR 4.0.0 format and includes:</p><ul><li>Context and problem statement</li><li>Decision drivers and constraints</li><li>Considered options with pros and cons</li><li>Decision outcome and rationale</li><li>Consequences (positive and negative)</li><li>Confirmation methods</li></ul><h3 id=adr-categories>ADR Categories</h3><p>ADRs are classified into three categories:</p><ul><li><strong>Strategic</strong> - High-level architectural decisions affecting the entire system (frameworks, authentication strategies, cross-cutting patterns). Use for foundational technology choices.</li><li><strong>User Journey</strong> - Decisions solving specific user journey problems. More tactical than strategic, but still architectural. Use when evaluating approaches to implement user-facing features.</li><li><strong>API Design</strong> - API endpoint implementation decisions (pagination, filtering, bulk operations). Use for significant API design trade-offs that warrant documentation.</li></ul><h3 id=status-values>Status Values</h3><p>Each ADR has a status that reflects its current state:</p><ul><li><code>proposed</code> - Decision is under consideration</li><li><code>accepted</code> - Decision has been approved and should be implemented</li><li><code>rejected</code> - Decision was considered but not approved</li><li><code>deprecated</code> - Decision is no longer relevant or has been superseded</li><li><code>superseded by ADR-XXXX</code> - Decision has been replaced by a newer ADR</li></ul><p>These records provide historical context for architectural decisions and help ensure consistency across the platform.</p></div></div><div class=td-content style=page-break-before:always><h1 id=pg-be14bb13e79709979af80fa8e61452c5>1 - [0001] Use MADR for Architecture Decision Records</h1><div class=lead>Adopt Markdown Architectural Decision Records (MADR) as the standard format for documenting architectural decisions in the project.</div><h2 id=context-and-problem-statement>Context and Problem Statement</h2><p>As the project grows, architectural decisions are made that have long-term impacts on the system&rsquo;s design, maintainability, and scalability. Without a structured way to document these decisions, we risk losing the context and rationale behind important choices, making it difficult for current and future team members to understand why certain approaches were taken.</p><p>How should we document architectural decisions in a way that is accessible, maintainable, and provides sufficient context for future reference?</p><h2 id=decision-drivers>Decision Drivers</h2><ul><li>Need for clear documentation of architectural decisions and their rationale</li><li>Easy accessibility and searchability of past decisions</li><li>Low barrier to entry for creating and maintaining decision records</li><li>Integration with existing documentation workflow</li><li>Version control friendly format</li><li>Industry-standard approach that team members may already be familiar with</li></ul><h2 id=considered-options>Considered Options</h2><ul><li>MADR (Markdown Architectural Decision Records)</li><li>ADR using custom format</li><li>Wiki-based documentation</li><li>No formal ADR process</li></ul><h2 id=decision-outcome>Decision Outcome</h2><p>Chosen option: &ldquo;MADR (Markdown Architectural Decision Records)&rdquo;, because it provides a well-established, standardized format that is lightweight, version-controlled, and integrates seamlessly with our existing documentation structure. MADR 4.0.0 offers a clear template that captures all necessary information while remaining flexible enough for different types of decisions.</p><h3 id=consequences>Consequences</h3><ul><li>Good, because MADR is a widely adopted standard with clear documentation and examples</li><li>Good, because markdown files are easy to create, edit, and review through pull requests</li><li>Good, because ADRs will be version-controlled alongside code, maintaining historical context</li><li>Good, because the format is flexible enough to accommodate strategic, user-journey, and API design decisions</li><li>Good, because team members can easily search and reference past decisions</li><li>Neutral, because requires discipline to maintain and update ADR status as decisions evolve</li><li>Bad, because team members need to learn and follow the MADR format conventions</li></ul><h3 id=confirmation>Confirmation</h3><p>Compliance will be confirmed through:</p><ul><li>Code reviews ensuring new architectural decisions are documented as ADRs</li><li>ADRs are stored in <code>docs/content/r&amp;d/adrs/</code> following the naming convention <code>NNNN-title-with-dashes.md</code></li><li>Regular reviews during architecture discussions to reference and update existing ADRs</li></ul><h2 id=pros-and-cons-of-the-options>Pros and Cons of the Options</h2><h3 id=madr-markdown-architectural-decision-records>MADR (Markdown Architectural Decision Records)</h3><p>MADR 4.0.0 is a standardized format for documenting architectural decisions using markdown.</p><ul><li>Good, because it&rsquo;s a well-established standard with extensive documentation</li><li>Good, because markdown is simple, portable, and version-control friendly</li><li>Good, because it provides a clear structure while remaining flexible</li><li>Good, because it integrates with static site generators and documentation tools</li><li>Good, because it&rsquo;s lightweight and doesn&rsquo;t require special tools</li><li>Neutral, because it requires some initial learning of the format</li><li>Neutral, because maintaining consistency requires discipline</li></ul><h3 id=adr-using-custom-format>ADR using custom format</h3><p>Create our own custom format for architectural decision records.</p><ul><li>Good, because we can tailor it exactly to our needs</li><li>Bad, because it requires defining and maintaining our own standard</li><li>Bad, because new team members won&rsquo;t be familiar with the format</li><li>Bad, because we lose the benefits of community knowledge and tooling</li><li>Bad, because it may evolve inconsistently over time</li></ul><h3 id=wiki-based-documentation>Wiki-based documentation</h3><p>Use a wiki system (like Confluence, Notion, or GitHub Wiki) to document decisions.</p><ul><li>Good, because wikis provide easy editing and hyperlinking</li><li>Good, because some team members may be familiar with wiki tools</li><li>Neutral, because it may or may not integrate with version control</li><li>Bad, because content may not be version-controlled alongside code</li><li>Bad, because it creates a separate system to maintain</li><li>Bad, because it&rsquo;s harder to review changes through standard PR process</li><li>Bad, because portability and long-term accessibility may be concerns</li></ul><h3 id=no-formal-adr-process>No formal ADR process</h3><p>Continue without a structured approach to documenting architectural decisions.</p><ul><li>Good, because it requires no additional overhead</li><li>Bad, because context and rationale for decisions are lost over time</li><li>Bad, because new team members struggle to understand why decisions were made</li><li>Bad, because it leads to repeated discussions of previously settled questions</li><li>Bad, because it makes it difficult to track when decisions should be revisited</li></ul><h2 id=more-information>More Information</h2><ul><li>MADR 4.0.0 specification: <a href=https://adr.github.io/madr/>https://adr.github.io/madr/</a></li><li>ADRs will be categorized as: strategic, user-journey, or api-design</li><li>ADR status values: proposed | accepted | rejected | deprecated | superseded by ADR-XXXX</li><li>All ADRs are stored in <code>docs/content/r&amp;d/adrs/</code> directory</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-79e3b41ba15ba3a179ad4b5df5f3f2fe>2 - [0002] Network Boot Architecture for Home Lab</h1><div class=lead>Evaluate options for network booting servers in a home lab environment, considering local vs cloud-hosted boot servers.</div><h2 id=context-and-problem-statement>Context and Problem Statement</h2><p>When setting up a home lab infrastructure, servers need to be provisioned and booted over the network using PXE (Preboot Execution Environment). This requires a TFTP/HTTP server to serve boot files to requesting machines. The question is: where should this boot server be hosted to balance security, reliability, cost, and operational complexity?</p><h2 id=decision-drivers>Decision Drivers</h2><ul><li><strong>Security</strong>: Minimize attack surface and ensure only authorized servers receive boot files</li><li><strong>Reliability</strong>: Boot process should be resilient and not dependent on external network connectivity</li><li><strong>Cost</strong>: Minimize ongoing infrastructure costs</li><li><strong>Complexity</strong>: Keep the operational burden manageable</li><li><strong>Trust Model</strong>: Clear verification of requesting server identity</li></ul><h2 id=considered-options>Considered Options</h2><ul><li>Option 1: TFTP/HTTP server locally on home lab network</li><li>Option 2: TFTP/HTTP server on public cloud (without VPN)</li><li>Option 3: TFTP/HTTP server on public cloud (with VPN)</li></ul><h2 id=decision-outcome>Decision Outcome</h2><p>Chosen option: &ldquo;Option 3: TFTP/HTTP server on public cloud (with VPN)&rdquo;, because:</p><ol><li><strong>No local machine management</strong>: Unlike Option 1, this avoids the need to maintain dedicated local hardware for the boot server, reducing operational overhead</li><li><strong>Secure protocol support</strong>: The VPN tunnel encrypts all traffic, allowing unsecured protocols like TFTP to be used without risk of data exposure over public internet routes (unlike Option 2)</li><li><strong>Cost-effective VPN</strong>: The UDM Pro natively supports WireGuard, enabling a self-managed VPN solution that avoids expensive managed VPN services (~$180-300/year vs ~$540-900/year)</li></ol><h3 id=consequences>Consequences</h3><ul><li>Good, because all traffic is encrypted through WireGuard VPN tunnel</li><li>Good, because boot server is not exposed to public internet (no public attack surface)</li><li>Good, because trust model is simple - subnet validation similar to local option</li><li>Good, because centralized cloud management reduces local maintenance burden</li><li>Good, because boot server remains available even if home lab storage fails</li><li>Good, because UDM Pro&rsquo;s native WireGuard support keeps costs at ~$180-300/year</li><li>Bad, because boot process depends on both internet connectivity and VPN availability</li><li>Bad, because VPN adds latency to boot file transfers</li><li>Bad, because VPN gateway becomes an additional failure point</li><li>Bad, because higher ongoing cost compared to local-only option (~$180-300/year vs ~$10/year)</li></ul><h3 id=confirmation>Confirmation</h3><p>The implementation will be confirmed by:</p><ul><li>Successfully network booting a test server using the chosen architecture</li><li>Validating the trust model prevents unauthorized boot requests</li><li>Measuring actual costs against estimates</li></ul><h2 id=pros-and-cons-of-the-options>Pros and Cons of the Options</h2><h3 id=option-1-tftphttp-server-locally-on-home-lab-network>Option 1: TFTP/HTTP server locally on home lab network</h3><p>Run the boot server on local infrastructure (e.g., Raspberry Pi, dedicated VM, or container) within the home lab network.</p><h4 id=boot-flow-sequence>Boot Flow Sequence</h4><pre class=mermaid>sequenceDiagram
    participant Server as Home Lab Server
    participant DHCP as Local DHCP Server
    participant Boot as Local TFTP/HTTP Server

    Server-&gt;&gt;DHCP: PXE Boot Request (DHCP Discover)
    DHCP-&gt;&gt;Server: DHCP Offer with Boot Server IP
    Server-&gt;&gt;Boot: TFTP Request for Boot File
    Boot-&gt;&gt;Boot: Verify MAC/IP against allowlist
    Boot-&gt;&gt;Server: Send iPXE/Boot Loader
    Server-&gt;&gt;Boot: HTTP Request for Kernel/Initrd
    Boot-&gt;&gt;Server: Send Boot Files
    Server-&gt;&gt;Server: Boot into OS</pre><h4 id=trust-model>Trust Model</h4><ul><li><strong>MAC Address Allowlist</strong>: Maintain a list of known server MAC addresses</li><li><strong>Network Isolation</strong>: Boot server only accessible from home lab VLAN</li><li><strong>No external exposure</strong>: Traffic never leaves local network</li><li><strong>Physical security</strong>: Relies on physical access control to home lab</li></ul><h4 id=cost-estimate>Cost Estimate</h4><ul><li><strong>Hardware</strong>: ~$50-100 one-time (Raspberry Pi or repurposed hardware)</li><li><strong>Power</strong>: ~$5-10/year (low power consumption)</li><li><strong>Total</strong>: ~$55-110 initial + ~$10/year ongoing</li></ul><h4 id=pros-and-cons>Pros and Cons</h4><ul><li>Good, because no dependency on internet connectivity for booting</li><li>Good, because lowest latency for boot file transfers</li><li>Good, because all data stays within local network (maximum privacy)</li><li>Good, because lowest ongoing cost</li><li>Good, because simple trust model based on network isolation</li><li>Neutral, because requires dedicated local hardware or resources</li><li>Bad, because single point of failure if boot server goes down</li><li>Bad, because requires local maintenance and updates</li></ul><h3 id=option-2-tftphttp-server-on-public-cloud-without-vpn>Option 2: TFTP/HTTP server on public cloud (without VPN)</h3><p>Host the boot server on a cloud provider (AWS, GCP, Azure) and expose it directly to the internet.</p><h4 id=boot-flow-sequence-1>Boot Flow Sequence</h4><pre class=mermaid>sequenceDiagram
    participant Server as Home Lab Server
    participant DHCP as Local DHCP Server
    participant Router as Home Router/NAT
    participant Internet as Internet
    participant Boot as Cloud TFTP/HTTP Server

    Server-&gt;&gt;DHCP: PXE Boot Request (DHCP Discover)
    DHCP-&gt;&gt;Server: DHCP Offer with Cloud Boot Server IP
    Server-&gt;&gt;Router: TFTP Request
    Router-&gt;&gt;Internet: NAT Translation
    Internet-&gt;&gt;Boot: TFTP Request from Home IP
    Boot-&gt;&gt;Boot: Verify source IP &#43; token/certificate
    Boot-&gt;&gt;Internet: Send iPXE/Boot Loader
    Internet-&gt;&gt;Router: Response
    Router-&gt;&gt;Server: Boot Loader
    Server-&gt;&gt;Router: HTTP Request for Kernel/Initrd
    Router-&gt;&gt;Internet: NAT Translation
    Internet-&gt;&gt;Boot: HTTP Request with auth headers
    Boot-&gt;&gt;Boot: Validate request authenticity
    Boot-&gt;&gt;Internet: Send Boot Files
    Internet-&gt;&gt;Router: Response
    Router-&gt;&gt;Server: Boot Files
    Server-&gt;&gt;Server: Boot into OS</pre><h4 id=trust-model-1>Trust Model</h4><ul><li><strong>Source IP Validation</strong>: Restrict to home lab&rsquo;s public IP (dynamic IP is problematic)</li><li><strong>Certificate/Token Authentication</strong>: Embed certificates in initial bootloader</li><li><strong>TLS for HTTP</strong>: All HTTP traffic encrypted</li><li><strong>Challenge-Response</strong>: Boot server can challenge requesting server</li><li><strong>Risk</strong>: TFTP typically unencrypted, vulnerable to interception</li></ul><h4 id=cost-estimate-1>Cost Estimate</h4><ul><li><strong>Cloud VM (t3.micro or equivalent)</strong>: ~$10-15/month</li><li><strong>Data Transfer</strong>: ~$1-5/month (boot files are typically small)</li><li><strong>Static IP</strong>: ~$3-5/month</li><li><strong>Total</strong>: ~$170-300/year</li></ul><h4 id=pros-and-cons-1>Pros and Cons</h4><ul><li>Good, because boot server remains available even if home lab has issues</li><li>Good, because centralized management in cloud console</li><li>Good, because easy to scale or replicate</li><li>Neutral, because requires internet connectivity for every boot</li><li>Bad, because significantly higher ongoing cost</li><li>Bad, because TFTP protocol is inherently insecure over public internet</li><li>Bad, because complex trust model required (IP validation, certificates)</li><li>Bad, because boot process depends on internet availability</li><li>Bad, because higher latency for boot file transfers</li><li>Bad, because public exposure increases attack surface</li></ul><h3 id=option-3-tftphttp-server-on-public-cloud-with-vpn>Option 3: TFTP/HTTP server on public cloud (with VPN)</h3><p>Host the boot server in the cloud but connect the home lab to the cloud via a site-to-site VPN tunnel.</p><h4 id=boot-flow-sequence-2>Boot Flow Sequence</h4><pre class=mermaid>sequenceDiagram
    participant Server as Home Lab Server
    participant DHCP as Local DHCP Server
    participant VPN as VPN Gateway (Home)
    participant CloudVPN as VPN Gateway (Cloud)
    participant Boot as Cloud TFTP/HTTP Server

    Note over VPN,CloudVPN: Site-to-Site VPN Tunnel Established

    Server-&gt;&gt;DHCP: PXE Boot Request (DHCP Discover)
    DHCP-&gt;&gt;Server: DHCP Offer with Boot Server Private IP
    Server-&gt;&gt;VPN: TFTP Request to Private IP
    VPN-&gt;&gt;CloudVPN: Encrypted VPN Tunnel
    CloudVPN-&gt;&gt;Boot: TFTP Request (appears local)
    Boot-&gt;&gt;Boot: Verify source IP from home lab subnet
    Boot-&gt;&gt;CloudVPN: Send iPXE/Boot Loader
    CloudVPN-&gt;&gt;VPN: Encrypted Response
    VPN-&gt;&gt;Server: Boot Loader
    Server-&gt;&gt;VPN: HTTP Request for Kernel/Initrd
    VPN-&gt;&gt;CloudVPN: Encrypted VPN Tunnel
    CloudVPN-&gt;&gt;Boot: HTTP Request
    Boot-&gt;&gt;Boot: Validate subnet membership
    Boot-&gt;&gt;CloudVPN: Send Boot Files
    CloudVPN-&gt;&gt;VPN: Encrypted Response
    VPN-&gt;&gt;Server: Boot Files
    Server-&gt;&gt;Server: Boot into OS</pre><h4 id=trust-model-2>Trust Model</h4><ul><li><strong>VPN Tunnel Encryption</strong>: All traffic encrypted end-to-end</li><li><strong>Private IP Addressing</strong>: Boot server only accessible via VPN</li><li><strong>Subnet Validation</strong>: Verify requests come from trusted home lab subnet</li><li><strong>VPN Authentication</strong>: Strong auth at tunnel level (certificates, pre-shared keys)</li><li><strong>No public exposure</strong>: Boot server has no public IP</li></ul><h4 id=cost-estimate-2>Cost Estimate</h4><ul><li><strong>Cloud VM (t3.micro or equivalent)</strong>: ~$10-15/month</li><li><strong>Data Transfer (VPN)</strong>: ~$5-10/month</li><li><strong>VPN Gateway Service (if using managed)</strong>: ~$30-50/month OR</li><li><strong>Self-managed VPN (WireGuard/OpenVPN)</strong>: ~$0 additional</li><li><strong>Total (self-managed VPN)</strong>: ~$180-300/year</li><li><strong>Total (managed VPN)</strong>: ~$540-900/year</li></ul><h4 id=pros-and-cons-2>Pros and Cons</h4><ul><li>Good, because all traffic encrypted through VPN tunnel</li><li>Good, because boot server not exposed to public internet</li><li>Good, because trust model similar to local option (subnet validation)</li><li>Good, because centralized cloud management benefits</li><li>Good, because boot server available if home lab storage fails</li><li>Neutral, because moderate complexity (VPN setup and maintenance)</li><li>Bad, because higher cost than local option</li><li>Bad, because boot process still depends on internet + VPN availability</li><li>Bad, because VPN adds latency to boot process</li><li>Bad, because VPN gateway becomes additional failure point</li><li>Bad, because most expensive option if using managed VPN service</li></ul><h2 id=more-information>More Information</h2><h3 id=related-resources>Related Resources</h3><ul><li><a href=https://en.wikipedia.org/wiki/Preboot_Execution_Environment>PXE Boot Specification</a></li><li><a href=https://ipxe.org/>iPXE - Open Source Boot Firmware</a></li><li><a href=https://tools.ietf.org/html/rfc1350>TFTP Protocol (RFC 1350)</a></li></ul><h3 id=key-questions-for-decision>Key Questions for Decision</h3><ol><li>How critical is boot availability during internet outages?</li><li>Is the home lab public IP static or dynamic?</li><li>What is the acceptable boot time latency?</li><li>How many servers need to be supported?</li><li>Is there existing VPN infrastructure?</li></ol><h3 id=related-issues>Related Issues</h3><ul><li><a href=https://github.com/Zaba505/infra/issues/595>Issue #595</a> - story(docs): create adr for network boot architecture</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-853f5ec590d44937c1ebd528cd046965>3 - [0003] Cloud Provider Selection for Network Boot Infrastructure</h1><div class=lead>Evaluate Google Cloud Platform vs Amazon Web Services for hosting network boot server infrastructure as required by ADR-0002.</div><h2 id=context-and-problem-statement>Context and Problem Statement</h2><p><a href=./0002-network-boot-architecture/>ADR-0002</a> established that network boot infrastructure will be hosted on a cloud provider and accessed via VPN (specifically WireGuard from the UDM Pro). The decision to use cloud hosting provides resilience against local hardware failures while maintaining security through encrypted VPN tunnels.</p><p>The question now is: <strong>Which cloud provider should host the network boot infrastructure?</strong></p><p>This decision will affect:</p><ul><li><strong>Cost</strong>: Ongoing monthly/annual infrastructure costs</li><li><strong>Protocol Support</strong>: Ability to serve TFTP, HTTP, and HTTPS boot files</li><li><strong>VPN Integration</strong>: Ease of WireGuard deployment and management</li><li><strong>Operational Complexity</strong>: Management overhead and maintenance burden</li><li><strong>Performance</strong>: Boot file transfer latency and throughput</li><li><strong>Vendor Lock-in</strong>: Future flexibility to migrate or multi-cloud</li></ul><h2 id=decision-drivers>Decision Drivers</h2><ul><li><strong>Cost Efficiency</strong>: Minimize ongoing infrastructure costs for home lab scale</li><li><strong>Protocol Support</strong>: Must support TFTP (UDP/69), HTTP (TCP/80), and HTTPS (TCP/443) for network boot workflows</li><li><strong>WireGuard Compatibility</strong>: Must support self-managed WireGuard VPN with reasonable effort</li><li><strong>UDM Pro Integration</strong>: Should work seamlessly with UniFi Dream Machine Pro&rsquo;s native WireGuard client</li><li><strong>Simplicity</strong>: Minimize operational complexity for a single-person home lab</li><li><strong>Existing Expertise</strong>: Leverage existing team knowledge and infrastructure</li><li><strong>Performance</strong>: Sufficient throughput and low latency for boot file transfers (50-200MB per boot)</li></ul><h2 id=considered-options>Considered Options</h2><ul><li><strong>Option 1</strong>: Google Cloud Platform (GCP)</li><li><strong>Option 2</strong>: Amazon Web Services (AWS)</li></ul><h2 id=decision-outcome>Decision Outcome</h2><p>Chosen option: &ldquo;<strong>Option 1: Google Cloud Platform (GCP)</strong>&rdquo;, because:</p><ol><li><strong>Existing Infrastructure</strong>: The home lab already uses GCP extensively (Cloud Run services, load balancers, mTLS infrastructure per existing codebase), reducing operational overhead and leveraging existing expertise</li><li><strong>Comparable Costs</strong>: Both providers offer similar costs for the required infrastructure (~$6-12/month for compute + VPN), with GCP&rsquo;s e2-micro being sufficient</li><li><strong>Equivalent Protocol Support</strong>: Both support TFTP/HTTP/HTTPS via direct VM access (load balancers unnecessary for single boot server), meeting all protocol requirements</li><li><strong>WireGuard Compatibility</strong>: Both require self-managed WireGuard deployment (neither has native WireGuard support), with nearly identical implementation complexity</li><li><strong>Unified Management</strong>: Consolidating all cloud infrastructure on GCP simplifies monitoring, billing, IAM, and operational workflows</li></ol><p>While AWS would be a viable alternative (especially with t4g.micro ARM instances offering slightly better price/performance), the <strong>existing GCP investment</strong> makes it the pragmatic choice to avoid multi-cloud complexity.</p><h3 id=consequences>Consequences</h3><ul><li>Good, because consolidates all cloud infrastructure on a single provider (reduced operational complexity)</li><li>Good, because leverages existing GCP expertise and IAM configurations</li><li>Good, because unified Cloud Monitoring/Logging across all services</li><li>Good, because single cloud bill simplifies cost tracking</li><li>Good, because existing Terraform modules and patterns can be reused</li><li>Good, because GCP&rsquo;s e2-micro instances (~$6.50/month) are cost-effective for the workload</li><li>Good, because self-managed WireGuard provides flexibility and low cost (~$10/month total)</li><li>Neutral, because both providers have comparable protocol support (TFTP/HTTP/HTTPS via VM)</li><li>Neutral, because both require self-managed WireGuard (no native support)</li><li>Bad, because creates vendor lock-in to GCP (migration would require relearning and reconfiguration)</li><li>Bad, because foregoes AWS&rsquo;s slightly cheaper t4g.micro ARM instances (~$6/month vs GCP&rsquo;s ~$6.50/month)</li><li>Bad, because multi-cloud strategy could provide redundancy (accepted trade-off for simplicity)</li></ul><h3 id=confirmation>Confirmation</h3><p>The implementation will be confirmed by:</p><ul><li>Successfully deploying WireGuard VPN gateway on GCP Compute Engine</li><li>Establishing site-to-site VPN tunnel between UDM Pro and GCP</li><li>Network booting a test server via VPN using TFTP and HTTP protocols</li><li>Measuring actual costs against estimates (~$10-15/month)</li><li>Validating boot performance (transfer time &lt; 30 seconds for typical boot)</li></ul><h2 id=pros-and-cons-of-the-options>Pros and Cons of the Options</h2><h3 id=option-1-google-cloud-platform-gcp>Option 1: Google Cloud Platform (GCP)</h3><p>Host network boot infrastructure on Google Cloud Platform.</p><h4 id=architecture-overview>Architecture Overview</h4><pre class=mermaid>graph TB
    subgraph &#34;Home Lab Network&#34;
        A[Home Lab Servers]
        B[UDM Pro - WireGuard Client]
    end
    
    subgraph &#34;GCP VPC&#34;
        C[WireGuard Gateway VM&lt;br/&gt;e2-micro]
        D[Boot Server VM&lt;br/&gt;e2-micro]
        C --&gt;|VPC Routing| D
    end
    
    A --&gt;|PXE Boot Request| B
    B --&gt;|WireGuard Tunnel| C
    C --&gt;|TFTP/HTTP/HTTPS| D
    D --&gt;|Boot Files| C
    C --&gt;|Encrypted Response| B
    B --&gt;|Boot Files| A</pre><h4 id=implementation-details>Implementation Details</h4><p><strong>Compute</strong>:</p><ul><li><strong>WireGuard Gateway</strong>: e2-micro VM (~$6.50/month) running Ubuntu 22.04<ul><li>Self-managed WireGuard server</li><li>IP forwarding enabled</li><li>Static external IP (~$3.50/month if VM ever stops)</li></ul></li><li><strong>Boot Server</strong>: e2-micro VM (same or consolidated with gateway)<ul><li>TFTP server (<code>tftpd-hpa</code>)</li><li>HTTP server (nginx or simple Python server)</li><li>Optional HTTPS with self-signed cert or Let&rsquo;s Encrypt</li></ul></li></ul><p><strong>Networking</strong>:</p><ul><li><strong>VPC</strong>: Default VPC or custom VPC with private subnets</li><li><strong>Firewall Rules</strong>:<ul><li>Allow UDP/51820 from home lab public IP (WireGuard)</li><li>Allow UDP/69, TCP/80, TCP/443 from VPN subnet (boot protocols)</li></ul></li><li><strong>Routes</strong>: Custom route to direct home lab subnet through WireGuard gateway</li><li><strong>Cloud VPN</strong>: Not used (self-managed WireGuard instead to save ~$65/month)</li></ul><p><strong>WireGuard Setup</strong>:</p><ul><li>Install WireGuard on Compute Engine VM</li><li>Configure <code>wg0</code> interface with PostUp/PostDown iptables rules</li><li>Store private key in Secret Manager</li><li>UDM Pro connects as WireGuard peer</li></ul><p><strong>Cost Breakdown</strong> (US regions):</p><table><thead><tr><th>Component</th><th>Monthly Cost</th></tr></thead><tbody><tr><td>e2-micro VM (WireGuard + Boot)</td><td>~$6.50</td></tr><tr><td>Static External IP (if attached)</td><td>~$3.50</td></tr><tr><td>Egress (10 boots × 150MB)</td><td>~$0.18</td></tr><tr><td><strong>Total</strong></td><td><strong>~$10.18</strong></td></tr><tr><td><strong>Annual</strong></td><td><strong>~$122</strong></td></tr></tbody></table><h4 id=pros-and-cons>Pros and Cons</h4><ul><li>Good, because existing home lab infrastructure already uses GCP extensively</li><li>Good, because consolidates all cloud resources on single provider (unified billing, IAM, monitoring)</li><li>Good, because leverages existing GCP expertise and Terraform modules</li><li>Good, because Cloud Monitoring/Logging already configured for other services</li><li>Good, because Secret Manager integration for WireGuard key storage</li><li>Good, because e2-micro instance size is sufficient for network boot workload</li><li>Good, because low cost (~$10/month for self-managed WireGuard)</li><li>Good, because VPC networking is familiar and well-documented</li><li>Neutral, because requires self-managed WireGuard (no native support, same as AWS)</li><li>Neutral, because TFTP/HTTP/HTTPS served directly from VM (no special GCP features needed)</li><li>Bad, because slightly more expensive than AWS t4g.micro (~$6.50/month vs ~$6/month)</li><li>Bad, because creates vendor lock-in to GCP ecosystem</li><li>Bad, because Cloud VPN (managed IPsec) is expensive (~$73/month), so must use self-managed WireGuard</li></ul><h3 id=option-2-amazon-web-services-aws>Option 2: Amazon Web Services (AWS)</h3><p>Host network boot infrastructure on Amazon Web Services.</p><h4 id=architecture-overview-1>Architecture Overview</h4><pre class=mermaid>graph TB
    subgraph &#34;Home Lab Network&#34;
        A[Home Lab Servers]
        B[UDM Pro - WireGuard Client]
    end
    
    subgraph &#34;AWS VPC&#34;
        C[WireGuard Gateway EC2&lt;br/&gt;t4g.micro]
        D[Boot Server EC2&lt;br/&gt;t4g.micro]
        C --&gt;|VPC Routing| D
    end
    
    A --&gt;|PXE Boot Request| B
    B --&gt;|WireGuard Tunnel| C
    C --&gt;|TFTP/HTTP/HTTPS| D
    D --&gt;|Boot Files| C
    C --&gt;|Encrypted Response| B
    B --&gt;|Boot Files| A</pre><h4 id=implementation-details-1>Implementation Details</h4><p><strong>Compute</strong>:</p><ul><li><strong>WireGuard Gateway</strong>: t4g.micro EC2 (~$6/month, ARM-based Graviton)<ul><li>Self-managed WireGuard server</li><li>Source/Dest check disabled for IP forwarding</li><li>Elastic IP (free when attached to running instance)</li></ul></li><li><strong>Boot Server</strong>: t4g.micro EC2 (same or consolidated with gateway)<ul><li>TFTP server (<code>tftpd-hpa</code>)</li><li>HTTP server (nginx)</li><li>Optional HTTPS with Let&rsquo;s Encrypt or self-signed cert</li></ul></li></ul><p><strong>Networking</strong>:</p><ul><li><strong>VPC</strong>: Default VPC or custom VPC with private subnets</li><li><strong>Security Groups</strong>:<ul><li>WireGuard SG: Allow UDP/51820 from home lab public IP</li><li>Boot Server SG: Allow UDP/69, TCP/80, TCP/443 from WireGuard SG</li></ul></li><li><strong>Route Table</strong>: Add route for home lab subnet via WireGuard instance</li><li><strong>Site-to-Site VPN</strong>: Not used (self-managed WireGuard saves ~$30/month)</li></ul><p><strong>WireGuard Setup</strong>:</p><ul><li>Install WireGuard on Ubuntu 22.04 or Amazon Linux 2023 EC2</li><li>Configure <code>wg0</code> with iptables MASQUERADE</li><li>Store private key in Secrets Manager</li><li>UDM Pro connects as WireGuard peer</li></ul><p><strong>Cost Breakdown</strong> (US East):</p><table><thead><tr><th>Component</th><th>Monthly Cost</th></tr></thead><tbody><tr><td>t4g.micro EC2 (WireGuard + Boot)</td><td>~$6.00</td></tr><tr><td>Elastic IP (attached)</td><td>$0.00</td></tr><tr><td>Egress (10 boots × 150MB)</td><td>~$0.09</td></tr><tr><td><strong>Total (On-Demand)</strong></td><td><strong>~$6.09</strong></td></tr><tr><td><strong>Total (1-yr Reserved)</strong></td><td><strong>~$3.59</strong></td></tr><tr><td><strong>Annual (On-Demand)</strong></td><td><strong>~$73</strong></td></tr><tr><td><strong>Annual (Reserved)</strong></td><td><strong>~$43</strong></td></tr></tbody></table><h4 id=pros-and-cons-1>Pros and Cons</h4><ul><li>Good, because t4g.micro ARM instances offer best price/performance (~$6/month on-demand)</li><li>Good, because Reserved Instances provide significant savings (~40% with 1-year commitment)</li><li>Good, because Elastic IP is free when attached to running instance</li><li>Good, because AWS has extensive documentation and community support</li><li>Good, because potential for future multi-cloud strategy</li><li>Good, because ACM provides free SSL certificates (if public domain used)</li><li>Good, because Secrets Manager for WireGuard key storage</li><li>Good, because low cost (~$6/month on-demand, ~$3.50/month with RI)</li><li>Neutral, because requires self-managed WireGuard (no native support, same as GCP)</li><li>Neutral, because TFTP/HTTP/HTTPS served directly from EC2 (no special AWS features)</li><li>Bad, because introduces multi-cloud complexity (separate billing, IAM, monitoring)</li><li>Bad, because no existing AWS infrastructure in home lab (new learning curve)</li><li>Bad, because requires separate monitoring/logging setup (CloudWatch vs Cloud Monitoring)</li><li>Bad, because separate Terraform state and modules needed</li><li>Bad, because Site-to-Site VPN is expensive (~$36/month), so must use self-managed WireGuard</li></ul><h2 id=more-information>More Information</h2><h3 id=detailed-analysis>Detailed Analysis</h3><p>For in-depth analysis of each provider&rsquo;s capabilities:</p><ul><li><p><a href=../analysis/google-cloud/><strong>Google Cloud Platform Analysis</strong></a></p><ul><li><a href=../analysis/google-cloud/network-boot/>Network Boot Protocol Support (TFTP, HTTP, HTTPS)</a></li><li><a href=../analysis/google-cloud/wireguard/>WireGuard VPN Support and Deployment</a></li></ul></li><li><p><a href=../analysis/aws/><strong>Amazon Web Services Analysis</strong></a></p><ul><li><a href=../analysis/aws/network-boot/>Network Boot Protocol Support (TFTP, HTTP, HTTPS)</a></li><li><a href=../analysis/aws/wireguard/>WireGuard VPN Support and Deployment</a></li></ul></li></ul><h3 id=key-findings-summary>Key Findings Summary</h3><p>Both providers offer:</p><ul><li>✅ <strong>TFTP Support</strong>: Via direct VM/EC2 access (load balancers don&rsquo;t support TFTP)</li><li>✅ <strong>HTTP/HTTPS Support</strong>: Full support via direct VM/EC2 or load balancers</li><li>✅ <strong>WireGuard Compatibility</strong>: Self-managed deployment on VM/EC2 (neither has native support)</li><li>✅ <strong>UDM Pro Integration</strong>: Native WireGuard client works with both</li><li>✅ <strong>Low Cost</strong>: $6-12/month for compute + VPN infrastructure</li><li>✅ <strong>Sufficient Performance</strong>: 100+ Mbps throughput on smallest instances</li></ul><p>Key differences:</p><ul><li><strong>GCP</strong>: Slightly higher cost (~$10/month), but consolidates with existing infrastructure</li><li><strong>AWS</strong>: Slightly lower cost (~$6/month on-demand, ~$3.50/month Reserved), but introduces multi-cloud complexity</li></ul><h3 id=cost-comparison-table>Cost Comparison Table</h3><table><thead><tr><th>Component</th><th>GCP (e2-micro)</th><th>AWS (t4g.micro On-Demand)</th><th>AWS (t4g.micro 1-yr RI)</th></tr></thead><tbody><tr><td>Compute</td><td>$6.50/month</td><td>$6.00/month</td><td>$3.50/month</td></tr><tr><td>Static IP</td><td>$3.50/month</td><td>$0.00 (Elastic IP free when attached)</td><td>$0.00</td></tr><tr><td>Egress (1.5GB)</td><td>$0.18/month</td><td>$0.09/month</td><td>$0.09/month</td></tr><tr><td><strong>Monthly</strong></td><td><strong>$10.18</strong></td><td><strong>$6.09</strong></td><td><strong>$3.59</strong></td></tr><tr><td><strong>Annual</strong></td><td><strong>$122</strong></td><td><strong>$73</strong></td><td><strong>$43</strong></td></tr></tbody></table><p><strong>Savings Analysis</strong>: AWS is ~$49-79/year cheaper, but introduces operational complexity.</p><h3 id=protocol-support-comparison>Protocol Support Comparison</h3><table><thead><tr><th>Protocol</th><th>GCP Support</th><th>AWS Support</th><th>Implementation</th></tr></thead><tbody><tr><td>TFTP (UDP/69)</td><td>⚠️ Via VM</td><td>⚠️ Via EC2</td><td>Direct VM/EC2 access (no LB support)</td></tr><tr><td>HTTP (TCP/80)</td><td>✅ Full</td><td>✅ Full</td><td>Direct VM/EC2 or Load Balancer</td></tr><tr><td>HTTPS (TCP/443)</td><td>✅ Full</td><td>✅ Full</td><td>Direct VM/EC2 or Load Balancer + cert</td></tr><tr><td>WireGuard</td><td>⚠️ Self-managed</td><td>⚠️ Self-managed</td><td>Install on VM/EC2</td></tr></tbody></table><h3 id=wireguard-deployment-comparison>WireGuard Deployment Comparison</h3><table><thead><tr><th>Aspect</th><th>GCP</th><th>AWS</th></tr></thead><tbody><tr><td><strong>Native Support</strong></td><td>❌ No (IPsec Cloud VPN only)</td><td>❌ No (IPsec Site-to-Site VPN only)</td></tr><tr><td><strong>Self-Managed</strong></td><td>✅ Compute Engine</td><td>✅ EC2</td></tr><tr><td><strong>Setup Complexity</strong></td><td>Similar (install, configure, firewall)</td><td>Similar (install, configure, SG)</td></tr><tr><td><strong>IP Forwarding</strong></td><td>Enable on VM</td><td>Disable Source/Dest check</td></tr><tr><td><strong>Firewall</strong></td><td>VPC Firewall rules</td><td>Security Groups</td></tr><tr><td><strong>Key Storage</strong></td><td>Secret Manager</td><td>Secrets Manager</td></tr><tr><td><strong>Cost</strong></td><td>~$10/month total</td><td>~$6/month total</td></tr></tbody></table><h3 id=trade-offs-analysis>Trade-offs Analysis</h3><p><strong>Choosing GCP</strong>:</p><ul><li><strong>Wins</strong>: Operational simplicity, unified infrastructure, existing expertise</li><li><strong>Loses</strong>: ~$50-80/year higher cost, vendor lock-in</li></ul><p><strong>Choosing AWS</strong>:</p><ul><li><strong>Wins</strong>: Lower cost, Reserved Instance savings, multi-cloud optionality</li><li><strong>Loses</strong>: Multi-cloud complexity, separate monitoring/billing, new tooling</li></ul><p>For a home lab prioritizing <strong>simplicity over cost optimization</strong>, GCP&rsquo;s consolidation benefits outweigh the modest cost difference.</p><h3 id=related-adrs>Related ADRs</h3><ul><li><a href=./0002-network-boot-architecture/>ADR-0002: Network Boot Architecture</a> - Established requirement for cloud-hosted boot server with VPN</li><li><a href=./0001-use-madr-for-architecture-decision-records/>ADR-0001: Use MADR for Architecture Decision Records</a> - MADR format used for this ADR</li></ul><h3 id=future-considerations>Future Considerations</h3><ol><li><strong>Cost Reevaluation</strong>: If annual costs become significant, reconsider AWS Reserved Instances</li><li><strong>Multi-Cloud</strong>: If multi-cloud strategy emerges, migrate boot server to AWS</li><li><strong>Managed WireGuard</strong>: If GCP or AWS adds native WireGuard support, reevaluate managed option</li><li><strong>High Availability</strong>: If HA required, evaluate multi-region deployment costs on both providers</li></ol><h3 id=related-issues>Related Issues</h3><ul><li><a href=https://github.com/Zaba505/infra/issues/597>Issue #597</a> - story(docs): create adr for cloud provider selection</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-dab026fd06f98a03a459f97073d21662>4 - [0004] Server Operating System Selection</h1><div class=lead>Evaluate operating systems for homelab server infrastructure with focus on Kubernetes cluster setup and maintenance.</div><h2 id=context-and-problem-statement>Context and Problem Statement</h2><p>The homelab infrastructure requires a server operating system to run Kubernetes clusters for container workloads. The choice of operating system significantly impacts ease of cluster initialization, ongoing maintenance burden, security posture, and operational complexity.</p><p>The question is: <strong>Which operating system should be used for homelab Kubernetes servers?</strong></p><p>This decision will affect:</p><ul><li><strong>Cluster Initialization</strong>: Complexity and time required to bootstrap Kubernetes</li><li><strong>Maintenance Burden</strong>: Frequency and complexity of OS updates, Kubernetes upgrades, and patching</li><li><strong>Security Posture</strong>: Attack surface, built-in security features, and hardening requirements</li><li><strong>Resource Efficiency</strong>: RAM, CPU, and disk overhead</li><li><strong>Operational Complexity</strong>: Day-to-day management, troubleshooting, and debugging</li><li><strong>Learning Curve</strong>: Time required for team to become proficient</li></ul><h2 id=decision-drivers>Decision Drivers</h2><ul><li><strong>Ease of Kubernetes Setup</strong>: Minimize steps and complexity for cluster initialization</li><li><strong>Maintenance Simplicity</strong>: Reduce ongoing operational burden for updates and upgrades</li><li><strong>Security-First Design</strong>: Minimal attack surface and strong security defaults</li><li><strong>Resource Efficiency</strong>: Low RAM/CPU/disk overhead for cost-effective homelab</li><li><strong>Learning Curve</strong>: Reasonable adoption time for single-person homelab</li><li><strong>Community Support</strong>: Strong documentation and active community</li><li><strong>Immutability</strong>: Prefer declarative, version-controlled configuration (GitOps-friendly)</li><li><strong>Purpose-Built</strong>: OS optimized specifically for Kubernetes vs general-purpose</li></ul><h2 id=considered-options>Considered Options</h2><ul><li><strong>Option 1</strong>: Ubuntu Server with k3s</li><li><strong>Option 2</strong>: Fedora Server with kubeadm</li><li><strong>Option 3</strong>: Talos Linux (purpose-built Kubernetes OS)</li><li><strong>Option 4</strong>: Harvester HCI (hyperconverged platform)</li></ul><h2 id=decision-outcome>Decision Outcome</h2><p>Chosen option: &ldquo;<strong>Option 3: Talos Linux</strong>&rdquo;, because:</p><ol><li><strong>Minimal Attack Surface</strong>: No SSH, shell, or package manager eliminates entire classes of vulnerabilities, providing the strongest security posture</li><li><strong>Built-in Kubernetes</strong>: No separate installation or configuration complexity - Kubernetes is included and optimized</li><li><strong>Declarative Configuration</strong>: API-driven, immutable infrastructure aligns with GitOps principles and prevents configuration drift</li><li><strong>Lowest Resource Overhead</strong>: ~768MB RAM vs 1-2GB+ for traditional distros, maximizing homelab hardware efficiency</li><li><strong>Simplified Maintenance</strong>: Declarative upgrades (<code>talosctl upgrade</code>) for both OS and Kubernetes reduce operational burden</li><li><strong>Security by Default</strong>: Immutable filesystem, no shell, KSPP compliance - secure without manual hardening</li></ol><p>While the learning curve is steeper than traditional Linux distributions, the benefits of purpose-built Kubernetes infrastructure, minimal maintenance, and superior security outweigh the initial learning investment for a dedicated Kubernetes homelab.</p><h3 id=consequences>Consequences</h3><ul><li>Good, because minimal attack surface (no SSH/shell) provides strongest security posture</li><li>Good, because declarative configuration enables GitOps workflows and prevents drift</li><li>Good, because lowest resource overhead (~768MB RAM) maximizes homelab efficiency</li><li>Good, because built-in Kubernetes eliminates installation complexity</li><li>Good, because immutable infrastructure prevents configuration drift</li><li>Good, because simplified upgrades (single command for OS + K8s) reduce maintenance burden</li><li>Good, because smallest disk footprint (~500MB) vs 10GB+ for traditional distros</li><li>Good, because secure by default (no manual hardening required)</li><li>Good, because purpose-built design optimized specifically for Kubernetes</li><li>Good, because API-driven management (talosctl) enables automation</li><li>Neutral, because steeper learning curve (paradigm shift from shell-based management)</li><li>Neutral, because smaller community than Ubuntu/Fedora (but active and helpful)</li><li>Bad, because limited to Kubernetes workloads only (not general-purpose)</li><li>Bad, because no shell access requires different troubleshooting approach</li><li>Bad, because newer platform (less mature than Ubuntu/Fedora)</li><li>Bad, because no escape hatch for manual intervention when needed</li></ul><h3 id=confirmation>Confirmation</h3><p>The implementation will be confirmed by:</p><ul><li>Successfully bootstrapping a Talos cluster using talosctl</li><li>Deploying test workloads and validating functionality</li><li>Performing declarative OS and Kubernetes upgrades</li><li>Measuring actual resource usage (RAM &lt; 1GB per node)</li><li>Validating security posture (no SSH/shell, immutable filesystem)</li><li>Testing GitOps workflow (machine configs in version control)</li></ul><h2 id=pros-and-cons-of-the-options>Pros and Cons of the Options</h2><h3 id=option-1-ubuntu-server-with-k3s>Option 1: Ubuntu Server with k3s</h3><p>Host Kubernetes using Ubuntu Server 24.04 LTS with k3s lightweight Kubernetes distribution.</p><h4 id=architecture-overview>Architecture Overview</h4><pre class=mermaid>sequenceDiagram
    participant Admin
    participant Server as Ubuntu Server
    participant K3s as k3s Components
    
    Admin-&gt;&gt;Server: Install Ubuntu 24.04 LTS
    Server-&gt;&gt;Server: Configure network (static IP)
    Admin-&gt;&gt;Server: Update system
    Admin-&gt;&gt;Server: curl -sfL https://get.k3s.io | sh -
    Server-&gt;&gt;K3s: Download k3s binary
    K3s-&gt;&gt;Server: Configure containerd
    K3s-&gt;&gt;Server: Start k3s service
    K3s-&gt;&gt;Server: Initialize etcd (embedded)
    K3s-&gt;&gt;Server: Start API server
    K3s-&gt;&gt;Server: Deploy built-in CNI (Flannel)
    K3s--&gt;&gt;Admin: Control plane ready
    Admin-&gt;&gt;Server: Retrieve node token
    Admin-&gt;&gt;Server: Install k3s agent on workers
    K3s-&gt;&gt;Server: Join workers to cluster
    K3s--&gt;&gt;Admin: Cluster ready (5-10 minutes)</pre><h4 id=implementation-details>Implementation Details</h4><p><strong>Installation</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># Single-command k3s install</span>
</span></span><span class=line><span class=cl>curl -sfL https://get.k3s.io <span class=p>|</span> sh -
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Get token for workers</span>
</span></span><span class=line><span class=cl>sudo cat /var/lib/rancher/k3s/server/node-token
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Install on workers</span>
</span></span><span class=line><span class=cl>curl -sfL https://get.k3s.io <span class=p>|</span> <span class=nv>K3S_URL</span><span class=o>=</span>https://control-plane:6443 <span class=nv>K3S_TOKEN</span><span class=o>=</span>&lt;token&gt; sh -
</span></span></code></pre></div><p><strong>Resource Requirements</strong>:</p><ul><li><strong>RAM</strong>: 1GB total (512MB OS + 512MB k3s)</li><li><strong>CPU</strong>: 1-2 cores</li><li><strong>Disk</strong>: 20GB (10GB OS + 10GB containers)</li></ul><p><strong>Maintenance</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># OS updates</span>
</span></span><span class=line><span class=cl>sudo apt update <span class=o>&amp;&amp;</span> sudo apt upgrade
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># k3s upgrade</span>
</span></span><span class=line><span class=cl>curl -sfL https://get.k3s.io <span class=p>|</span> <span class=nv>INSTALL_K3S_VERSION</span><span class=o>=</span>v1.32.0+k3s1 sh -
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Or automatic via system-upgrade-controller</span>
</span></span></code></pre></div><h4 id=pros-and-cons>Pros and Cons</h4><ul><li>Good, because most familiar Linux distribution (easy adoption)</li><li>Good, because 5-year LTS support (10 years with Ubuntu Pro)</li><li>Good, because k3s provides single-command setup</li><li>Good, because extensive documentation and community support</li><li>Good, because compatible with all Kubernetes tooling</li><li>Good, because automatic security updates available</li><li>Good, because general-purpose (can run non-K8s workloads)</li><li>Good, because low learning curve</li><li>Neutral, because moderate resource overhead (1GB RAM)</li><li>Bad, because general-purpose OS has larger attack surface</li><li>Bad, because requires manual OS updates and reboots</li><li>Bad, because managing OS + Kubernetes lifecycle separately</li><li>Bad, because imperative configuration (not GitOps-native)</li><li>Bad, because mutable filesystem (configuration drift possible)</li></ul><h3 id=option-2-fedora-server-with-kubeadm>Option 2: Fedora Server with kubeadm</h3><p>Host Kubernetes using Fedora Server with kubeadm (official Kubernetes tool) and CRI-O container runtime.</p><h4 id=architecture-overview-1>Architecture Overview</h4><pre class=mermaid>sequenceDiagram
    participant Admin
    participant Server as Fedora Server
    participant K8s as Kubernetes Components
    
    Admin-&gt;&gt;Server: Install Fedora 41
    Server-&gt;&gt;Server: Configure network
    Admin-&gt;&gt;Server: Update system (dnf update)
    Admin-&gt;&gt;Server: Install CRI-O
    Server-&gt;&gt;Server: Configure CRI-O runtime
    Admin-&gt;&gt;Server: Install kubeadm/kubelet/kubectl
    Server-&gt;&gt;Server: Disable swap, load kernel modules
    Server-&gt;&gt;Server: Configure SELinux
    Admin-&gt;&gt;K8s: kubeadm init --cri-socket=unix:///var/run/crio/crio.sock
    K8s-&gt;&gt;Server: Generate certificates
    K8s-&gt;&gt;Server: Start etcd
    K8s-&gt;&gt;Server: Start API server
    K8s--&gt;&gt;Admin: Control plane ready
    Admin-&gt;&gt;K8s: kubectl apply CNI
    K8s-&gt;&gt;Server: Deploy CNI pods
    Admin-&gt;&gt;K8s: kubeadm join (workers)
    K8s--&gt;&gt;Admin: Cluster ready (15-20 minutes)</pre><h4 id=implementation-details-1>Implementation Details</h4><p><strong>Installation</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># Install CRI-O</span>
</span></span><span class=line><span class=cl>sudo dnf install -y cri-o
</span></span><span class=line><span class=cl>sudo systemctl <span class=nb>enable</span> --now crio
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Install kubeadm components</span>
</span></span><span class=line><span class=cl>sudo dnf install -y kubelet kubeadm kubectl
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Initialize cluster</span>
</span></span><span class=line><span class=cl>sudo kubeadm init --pod-network-cidr<span class=o>=</span>10.244.0.0/16 --cri-socket<span class=o>=</span>unix:///var/run/crio/crio.sock
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Install CNI</span>
</span></span><span class=line><span class=cl>kubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.28.0/manifests/calico.yaml
</span></span></code></pre></div><p><strong>Resource Requirements</strong>:</p><ul><li><strong>RAM</strong>: 2.2GB total (700MB OS + 1.5GB Kubernetes)</li><li><strong>CPU</strong>: 2+ cores</li><li><strong>Disk</strong>: 35GB (15GB OS + 20GB containers)</li></ul><p><strong>Maintenance</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># OS updates (every 13 months major upgrade)</span>
</span></span><span class=line><span class=cl>sudo dnf update -y
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Kubernetes upgrade</span>
</span></span><span class=line><span class=cl>sudo dnf update -y kubeadm
</span></span><span class=line><span class=cl>sudo kubeadm upgrade apply v1.32.0
</span></span><span class=line><span class=cl>sudo dnf update -y kubelet kubectl
</span></span></code></pre></div><h4 id=pros-and-cons-1>Pros and Cons</h4><ul><li>Good, because SELinux enabled by default (stronger than AppArmor)</li><li>Good, because latest kernel and packages (bleeding edge)</li><li>Good, because native CRI-O support (OpenShift compatibility)</li><li>Good, because upstream for RHEL (enterprise patterns)</li><li>Good, because kubeadm provides full control over cluster</li><li>Neutral, because faster release cycle (latest features, but more upgrades)</li><li>Bad, because short support cycle (13 months per release)</li><li>Bad, because bleeding-edge can introduce instability</li><li>Bad, because complex kubeadm setup (many manual steps)</li><li>Bad, because higher resource overhead (2.2GB RAM)</li><li>Bad, because SELinux configuration for Kubernetes is complex</li><li>Bad, because frequent OS upgrades required (every 13 months)</li><li>Bad, because managing OS + Kubernetes separately</li><li>Bad, because imperative configuration (not GitOps-native)</li></ul><h3 id=option-3-talos-linux-purpose-built-kubernetes-os>Option 3: Talos Linux (purpose-built Kubernetes OS)</h3><p>Use Talos Linux, an immutable, API-driven operating system designed specifically for Kubernetes with built-in cluster management.</p><h4 id=architecture-overview-2>Architecture Overview</h4><pre class=mermaid>sequenceDiagram
    participant Admin
    participant Server as Bare Metal Server
    participant Talos as Talos Linux
    participant K8s as Kubernetes Components
    
    Admin-&gt;&gt;Server: Boot Talos ISO (PXE or USB)
    Server-&gt;&gt;Talos: Start in maintenance mode
    Talos--&gt;&gt;Admin: API endpoint ready
    Admin-&gt;&gt;Admin: Generate configs (talosctl gen config)
    Admin-&gt;&gt;Talos: talosctl apply-config (controlplane.yaml)
    Talos-&gt;&gt;Server: Install Talos to disk
    Server-&gt;&gt;Server: Reboot from disk
    Talos-&gt;&gt;K8s: Start kubelet
    Talos-&gt;&gt;K8s: Start etcd
    Talos-&gt;&gt;K8s: Start API server
    Admin-&gt;&gt;Talos: talosctl bootstrap
    Talos-&gt;&gt;K8s: Initialize cluster
    K8s-&gt;&gt;Talos: Start controller-manager
    K8s--&gt;&gt;Admin: Control plane ready
    Admin-&gt;&gt;K8s: Apply CNI
    Admin-&gt;&gt;Talos: Apply worker configs
    Talos-&gt;&gt;K8s: Join workers
    K8s--&gt;&gt;Admin: Cluster ready (10-15 minutes)</pre><h4 id=implementation-details-2>Implementation Details</h4><p><strong>Installation</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># Generate machine configs</span>
</span></span><span class=line><span class=cl>talosctl gen config homelab https://192.168.1.10:6443
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Apply config to control plane (booted from ISO)</span>
</span></span><span class=line><span class=cl>talosctl apply-config --insecure --nodes 192.168.1.10 --file controlplane.yaml
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Bootstrap Kubernetes</span>
</span></span><span class=line><span class=cl>talosctl bootstrap --nodes 192.168.1.10 --endpoints 192.168.1.10
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Get kubeconfig</span>
</span></span><span class=line><span class=cl>talosctl kubeconfig --nodes 192.168.1.10
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Add workers</span>
</span></span><span class=line><span class=cl>talosctl apply-config --insecure --nodes 192.168.1.11 --file worker.yaml
</span></span></code></pre></div><p><strong>Machine Configuration</strong> (declarative YAML):</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>version</span><span class=p>:</span><span class=w> </span><span class=l>v1alpha1</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>machine</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>type</span><span class=p>:</span><span class=w> </span><span class=l>controlplane</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>install</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>disk</span><span class=p>:</span><span class=w> </span><span class=l>/dev/sda</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>network</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>hostname</span><span class=p>:</span><span class=w> </span><span class=l>control-plane-1</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>interfaces</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=nt>interface</span><span class=p>:</span><span class=w> </span><span class=l>eth0</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>addresses</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span>- <span class=m>192.168.1.10</span><span class=l>/24</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>cluster</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>clusterName</span><span class=p>:</span><span class=w> </span><span class=l>homelab</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>controlPlane</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>endpoint</span><span class=p>:</span><span class=w> </span><span class=l>https://192.168.1.10:6443</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>network</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>cni</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>custom</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>urls</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span>- <span class=l>https://raw.githubusercontent.com/projectcalico/calico/v3.28.0/manifests/calico.yaml</span><span class=w>
</span></span></span></code></pre></div><p><strong>Resource Requirements</strong>:</p><ul><li><strong>RAM</strong>: 768MB total (256MB OS + 512MB Kubernetes)</li><li><strong>CPU</strong>: 1-2 cores</li><li><strong>Disk</strong>: 10-15GB (500MB OS + 10GB containers)</li></ul><p><strong>Maintenance</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># Upgrade Talos (OS + Kubernetes)</span>
</span></span><span class=line><span class=cl>talosctl upgrade --nodes 192.168.1.10 --image ghcr.io/siderolabs/installer:v1.9.0
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Upgrade Kubernetes version</span>
</span></span><span class=line><span class=cl>talosctl upgrade-k8s --nodes 192.168.1.10 --to 1.32.0
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Apply config changes</span>
</span></span><span class=line><span class=cl>talosctl apply-config --nodes 192.168.1.10 --file controlplane.yaml
</span></span></code></pre></div><h4 id=pros-and-cons-2>Pros and Cons</h4><ul><li>Good, because Kubernetes built-in (no separate installation)</li><li>Good, because minimal attack surface (no SSH, shell, package manager)</li><li>Good, because immutable infrastructure (config drift impossible)</li><li>Good, because API-driven management (GitOps-friendly)</li><li>Good, because lowest resource overhead (~768MB RAM)</li><li>Good, because declarative configuration (YAML in version control)</li><li>Good, because secure by default (no manual hardening)</li><li>Good, because smallest disk footprint (~500MB OS)</li><li>Good, because designed specifically for Kubernetes</li><li>Good, because simple declarative upgrades (OS + K8s)</li><li>Good, because UEFI Secure Boot support</li><li>Neutral, because smaller community (but active and helpful)</li><li>Bad, because steep learning curve (paradigm shift)</li><li>Bad, because limited to Kubernetes workloads only</li><li>Bad, because troubleshooting without shell requires different approach</li><li>Bad, because relatively new (less mature than Ubuntu/Fedora)</li><li>Bad, because no escape hatch for manual intervention</li></ul><h3 id=option-4-harvester-hci-hyperconverged-platform>Option 4: Harvester HCI (hyperconverged platform)</h3><p>Use Harvester, a hyperconverged infrastructure platform built on K3s and KubeVirt for unified VM + container management.</p><h4 id=architecture-overview-3>Architecture Overview</h4><pre class=mermaid>sequenceDiagram
    participant Admin
    participant Server as Bare Metal Server
    participant Harvester as Harvester HCI
    participant K3s as K3s / KubeVirt
    participant Storage as Longhorn Storage
    
    Admin-&gt;&gt;Server: Boot Harvester ISO
    Server-&gt;&gt;Harvester: Installation wizard
    Admin-&gt;&gt;Harvester: Configure cluster (VIP, storage)
    Harvester-&gt;&gt;Server: Install RancherOS 2.0
    Harvester-&gt;&gt;Server: Install K3s
    Server-&gt;&gt;Server: Reboot
    Harvester-&gt;&gt;K3s: Start K3s server
    K3s-&gt;&gt;Storage: Deploy Longhorn
    K3s-&gt;&gt;Server: Deploy KubeVirt
    K3s-&gt;&gt;Server: Deploy multus CNI
    Harvester--&gt;&gt;Admin: Web UI ready
    Admin-&gt;&gt;Harvester: Add nodes
    Harvester-&gt;&gt;K3s: Join cluster
    K3s--&gt;&gt;Admin: Cluster ready (20-30 minutes)</pre><h4 id=implementation-details-3>Implementation Details</h4><p><strong>Installation</strong>: Interactive ISO wizard or cloud-init config</p><p><strong>Resource Requirements</strong>:</p><ul><li><strong>RAM</strong>: 8GB minimum per node (16GB+ recommended)</li><li><strong>CPU</strong>: 4+ cores per node</li><li><strong>Disk</strong>: 250GB+ per node (100GB OS + 150GB storage)</li><li><strong>Nodes</strong>: 3+ for production HA</li></ul><p><strong>Features</strong>:</p><ul><li>Web UI management</li><li>Built-in storage (Longhorn)</li><li>VM support (KubeVirt)</li><li>Live migration</li><li>Rancher integration</li></ul><h4 id=pros-and-cons-3>Pros and Cons</h4><ul><li>Good, because unified VM + container platform</li><li>Good, because built-in K3s (Kubernetes included)</li><li>Good, because web UI simplifies management</li><li>Good, because built-in persistent storage (Longhorn)</li><li>Good, because VM live migration</li><li>Good, because Rancher integration</li><li>Neutral, because immutable OS layer</li><li>Bad, because very heavy resource requirements (8GB+ RAM)</li><li>Bad, because complex architecture (KubeVirt, Longhorn, multus)</li><li>Bad, because overkill for container-only workloads</li><li>Bad, because larger attack surface (web UI, VM layer)</li><li>Bad, because requires 3+ nodes for HA (not single-node friendly)</li><li>Bad, because steep learning curve for full feature set</li></ul><h2 id=more-information>More Information</h2><h3 id=detailed-analysis>Detailed Analysis</h3><p>For in-depth analysis of each operating system:</p><ul><li><p><a href=../analysis/server-os/ubuntu/><strong>Ubuntu Server Analysis</strong></a></p><ul><li>Installation methods (kubeadm, k3s, MicroK8s)</li><li>Cluster initialization sequences</li><li>Maintenance requirements and upgrade procedures</li><li>Resource overhead and security posture</li></ul></li><li><p><a href=../analysis/server-os/fedora/><strong>Fedora Server Analysis</strong></a></p><ul><li>kubeadm with CRI-O installation</li><li>SELinux configuration for Kubernetes</li><li>Rapid release cycle implications</li><li>RHEL ecosystem compatibility</li></ul></li><li><p><a href=../analysis/server-os/talos-linux/><strong>Talos Linux Analysis</strong></a></p><ul><li>API-driven, immutable architecture</li><li>Declarative configuration model</li><li>Security-first design principles</li><li>Production readiness and advanced features</li></ul></li><li><p><a href=../analysis/server-os/harvester/><strong>Harvester HCI Analysis</strong></a></p><ul><li>Hyperconverged infrastructure capabilities</li><li>VM + container unified platform</li><li>KubeVirt and Longhorn integration</li><li>Multi-node cluster requirements</li></ul></li></ul><h3 id=key-findings-summary>Key Findings Summary</h3><p>Resource efficiency comparison:</p><ul><li>✅ <strong>Talos</strong>: 768MB RAM, 500MB disk (most efficient)</li><li>✅ <strong>Ubuntu + k3s</strong>: 1GB RAM, 20GB disk (efficient)</li><li>⚠️ <strong>Fedora + kubeadm</strong>: 2.2GB RAM, 35GB disk (moderate)</li><li>❌ <strong>Harvester</strong>: 8GB+ RAM, 250GB+ disk (heavy)</li></ul><p>Security posture comparison:</p><ul><li>✅ <strong>Talos</strong>: Minimal attack surface (no SSH/shell, immutable)</li><li>✅ <strong>Fedora</strong>: SELinux by default (strong MAC)</li><li>⚠️ <strong>Ubuntu</strong>: AppArmor (moderate security)</li><li>⚠️ <strong>Harvester</strong>: Larger attack surface (web UI, VM layer)</li></ul><p>Operational complexity comparison:</p><ul><li>✅ <strong>Ubuntu + k3s</strong>: Single command install, familiar management</li><li>✅ <strong>Talos</strong>: Declarative, automated (after learning curve)</li><li>⚠️ <strong>Fedora + kubeadm</strong>: Manual kubeadm steps, frequent OS upgrades</li><li>❌ <strong>Harvester</strong>: Complex HCI architecture, heavy requirements</li></ul><h3 id=decision-matrix>Decision Matrix</h3><table><thead><tr><th>Criterion</th><th>Ubuntu + k3s</th><th>Fedora + kubeadm</th><th>Talos Linux</th><th>Harvester</th></tr></thead><tbody><tr><td><strong>Setup Simplicity</strong></td><td>⭐⭐⭐⭐⭐</td><td>⭐⭐⭐</td><td>⭐⭐⭐⭐</td><td>⭐⭐⭐</td></tr><tr><td><strong>Maintenance Burden</strong></td><td>⭐⭐⭐⭐</td><td>⭐⭐</td><td>⭐⭐⭐⭐⭐</td><td>⭐⭐⭐⭐</td></tr><tr><td><strong>Security Posture</strong></td><td>⭐⭐⭐</td><td>⭐⭐⭐⭐</td><td>⭐⭐⭐⭐⭐</td><td>⭐⭐⭐</td></tr><tr><td><strong>Resource Efficiency</strong></td><td>⭐⭐⭐⭐</td><td>⭐⭐⭐</td><td>⭐⭐⭐⭐⭐</td><td>⭐</td></tr><tr><td><strong>Learning Curve</strong></td><td>⭐⭐⭐⭐⭐</td><td>⭐⭐⭐⭐</td><td>⭐⭐</td><td>⭐⭐⭐</td></tr><tr><td><strong>Community Support</strong></td><td>⭐⭐⭐⭐⭐</td><td>⭐⭐⭐⭐</td><td>⭐⭐⭐</td><td>⭐⭐⭐⭐</td></tr><tr><td><strong>Immutability</strong></td><td>⭐</td><td>⭐</td><td>⭐⭐⭐⭐⭐</td><td>⭐⭐⭐⭐</td></tr><tr><td><strong>GitOps-Friendly</strong></td><td>⭐⭐</td><td>⭐⭐</td><td>⭐⭐⭐⭐⭐</td><td>⭐⭐⭐</td></tr><tr><td><strong>Purpose-Built</strong></td><td>⭐⭐</td><td>⭐⭐</td><td>⭐⭐⭐⭐⭐</td><td>⭐⭐⭐⭐</td></tr><tr><td><strong>Overall Score</strong></td><td>29/45</td><td>24/45</td><td>38/45</td><td>28/45</td></tr></tbody></table><p><strong>Talos Linux scores highest</strong> for Kubernetes-dedicated homelab infrastructure prioritizing security, efficiency, and GitOps workflows.</p><h3 id=trade-offs-analysis>Trade-offs Analysis</h3><p><strong>Choosing Talos Linux</strong>:</p><ul><li><strong>Wins</strong>: Best security, lowest overhead, declarative configuration, minimal maintenance</li><li><strong>Loses</strong>: Steeper learning curve, no shell access, smaller community</li></ul><p><strong>Choosing Ubuntu + k3s</strong>:</p><ul><li><strong>Wins</strong>: Easiest adoption, largest community, general-purpose flexibility</li><li><strong>Loses</strong>: Higher attack surface, manual OS management, imperative config</li></ul><p><strong>Choosing Fedora + kubeadm</strong>:</p><ul><li><strong>Wins</strong>: Latest features, SELinux, enterprise compatibility</li><li><strong>Loses</strong>: Frequent OS upgrades, complex setup, higher overhead</li></ul><p><strong>Choosing Harvester</strong>:</p><ul><li><strong>Wins</strong>: VM + container unified platform, web UI</li><li><strong>Loses</strong>: Heavy resources, complex architecture, overkill for K8s-only</li></ul><p>For a <strong>Kubernetes-dedicated homelab prioritizing security and efficiency</strong>, Talos Linux&rsquo;s benefits outweigh the learning curve investment.</p><h3 id=related-adrs>Related ADRs</h3><ul><li><a href=./0001-use-madr-for-architecture-decision-records/>ADR-0001: Use MADR for Architecture Decision Records</a> - MADR format used for this ADR</li><li><a href=./0002-network-boot-architecture/>ADR-0002: Network Boot Architecture</a> - Server provisioning architecture</li><li><a href=./0003-cloud-provider-selection/>ADR-0003: Cloud Provider Selection</a> - Cloud infrastructure decisions</li></ul><h3 id=future-considerations>Future Considerations</h3><ol><li><strong>Team Growth</strong>: If team grows beyond single person, reassess Ubuntu for familiarity</li><li><strong>VM Requirements</strong>: If VM workloads emerge, consider Harvester or KubeVirt on Talos</li><li><strong>Enterprise Patterns</strong>: If RHEL compatibility needed, reconsider Fedora/CentOS Stream</li><li><strong>Maintenance Burden</strong>: If Talos learning curve proves too steep, fallback to k3s</li><li><strong>Talos Maturity</strong>: Monitor Talos ecosystem growth and production adoption</li></ol><h3 id=related-issues>Related Issues</h3><ul><li><a href=https://github.com/Zaba505/infra/issues/598>Issue #598</a> - story(docs): create adr for server operating system</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-be9e21cdab9183bad0c60fa6e3ba225b>5 - [0005] Network Boot Infrastructure Implementation on Google Cloud</h1><div class=lead>Evaluate implementation approaches for deploying network boot infrastructure on Google Cloud Platform using UEFI HTTP boot, comparing custom server implementation versus Matchbox-based solution.</div><h2 id=context-and-problem-statement>Context and Problem Statement</h2><p><a href=./0002-network-boot-architecture/>ADR-0002</a> established that network boot infrastructure will be hosted on a cloud provider accessed via WireGuard VPN. <a href=./0003-cloud-provider-selection/>ADR-0003</a> selected Google Cloud Platform as the hosting provider to consolidate infrastructure and leverage existing expertise.</p><p>The remaining question is: <strong>How should the network boot server itself be implemented?</strong></p><p>This decision affects:</p><ul><li><strong>Development Effort</strong>: Time required to build, test, and maintain the solution</li><li><strong>Feature Completeness</strong>: Capabilities for boot image management, machine mapping, and provisioning workflows</li><li><strong>Operational Complexity</strong>: Deployment, monitoring, and troubleshooting burden</li><li><strong>Security</strong>: Boot image integrity, access control, and audit capabilities</li><li><strong>Scalability</strong>: Ability to grow from single home lab to multiple environments</li></ul><p>The boot server must handle:</p><ol><li><strong>HTTP/HTTPS requests</strong> for UEFI boot scripts, kernels, initrd images, and cloud-init configurations</li><li><strong>Machine-to-image mapping</strong> to serve appropriate boot files based on MAC address, hardware profile, or tags</li><li><strong>Boot image lifecycle management</strong> including upload, versioning, and rollback capabilities</li></ol><h3 id=hardware-specific-context>Hardware-Specific Context</h3><p>The target bare metal servers (HP DL360 Gen 9) have the following network boot capabilities:</p><ul><li><strong>UEFI HTTP Boot</strong>: Supported in iLO 4 firmware v2.40+ (released 2016)</li><li><strong>TLS Support</strong>: Server-side TLS only (no client certificate authentication)</li><li><strong>Boot Process</strong>: Firmware handles initial HTTP requests directly (no PXE/TFTP chain loading required)</li><li><strong>Configuration</strong>: Boot URL configured via iLO RBSU or UEFI System Utilities</li></ul><p><strong>Security Implications</strong>: Since the servers cannot present client certificates for mTLS authentication with Cloudflare, the WireGuard VPN serves as the secure transport layer for boot traffic. The HTTP boot server is only accessible through the VPN tunnel.</p><p><strong>Reference</strong>: <a href=../../analysis/hp_dl360_gen9/network-boot.md>HP DL360 Gen 9 Network Boot Analysis</a></p><h2 id=decision-drivers>Decision Drivers</h2><ul><li><strong>Time to Production</strong>: Minimize time to get a working network boot infrastructure</li><li><strong>Feature Requirements</strong>: Must support machine-specific boot configurations, image versioning, and cloud-init integration</li><li><strong>Maintenance Burden</strong>: Prefer solutions that minimize ongoing maintenance and updates</li><li><strong>GCP Integration</strong>: Should leverage GCP services (Cloud Storage, Secret Manager, IAM)</li><li><strong>Security</strong>: Boot images must be served securely with access control and integrity verification</li><li><strong>Observability</strong>: Comprehensive logging and monitoring for troubleshooting boot failures</li><li><strong>Cost</strong>: Minimize infrastructure costs while meeting functional requirements</li><li><strong>Future Flexibility</strong>: Ability to extend or customize as needs evolve</li></ul><h2 id=considered-options>Considered Options</h2><ul><li><strong>Option 1</strong>: Custom server implementation (Go-based)</li><li><strong>Option 2</strong>: Matchbox-based solution</li></ul><h2 id=decision-outcome>Decision Outcome</h2><p><strong>Chosen option</strong>: &ldquo;<strong>Option 1: Custom implementation</strong>&rdquo;, because:</p><ol><li><strong>UEFI HTTP Boot Simplification</strong>: Elimination of TFTP/PXE dramatically reduces implementation complexity</li><li><strong>Cloud Run Deployment</strong>: HTTP-only boot enables serverless deployment (~$5/month vs $8-17/month)</li><li><strong>Development Time Manageable</strong>: UEFI HTTP boot reduces custom development to 2-3 weeks</li><li><strong>Full Control</strong>: Custom implementation maintains flexibility for future home lab requirements</li><li><strong>GCP Native Integration</strong>: Direct Cloud Storage, Firestore, Secret Manager, and IAM integration</li><li><strong>Existing Framework</strong>: Leverages <code>z5labs/humus</code> patterns already in use across services</li><li><strong>HTTP REST API</strong>: Native HTTP REST admin API via <code>z5labs/humus</code> framework provides better integration with existing tooling</li><li><strong>Microservices Architecture</strong>: Separation into Boot Service and Machine Management Service provides better separation of concerns</li></ol><h3 id=architecture>Architecture</h3><p>The implementation consists of two services:</p><ol><li><p><strong>Boot Service</strong>: Serves UEFI HTTP boot endpoints (<code>/boot.ipxe</code>, <code>/assets/{id}/kernel</code>, <code>/assets/{id}/initrd</code>)</p><ul><li>Accessed by bare metal servers during boot</li><li>Calls Machine Management Service API to resolve machine mappings and profiles</li><li>Streams kernel/initrd data from Machine Management Service</li></ul></li><li><p><strong>Machine Management Service</strong>: Provides REST API for managing profiles and machine mappings</p><ul><li>Stores boot profiles and machine mappings in Firestore</li><li>Manages kernel/initrd blobs in Cloud Storage</li><li>Provides admin API for creating/updating profiles and machines</li><li>Streams kernel/initrd binaries to Boot Service and admin clients</li></ul></li></ol><h3 id=consequences>Consequences</h3><ul><li>Good, because UEFI HTTP boot eliminates TFTP complexity entirely</li><li>Good, because Cloud Run deployment reduces operational overhead and cost</li><li>Good, because leverages existing <code>z5labs/humus</code> framework and Go expertise</li><li>Good, because GCP native integration (Cloud Storage, Firestore, Secret Manager, IAM)</li><li>Good, because full control over implementation enables future customization</li><li>Good, because microservices architecture separates boot operations from management operations</li><li>Good, because Boot Service can scale independently from Machine Management Service</li><li>Good, because simplified testing (HTTP-only, no TFTP/PXE edge cases)</li><li>Good, because OpenTelemetry observability built-in from existing patterns</li><li>Neutral, because requires 2-3 weeks development time vs 1 week for Matchbox setup</li><li>Neutral, because ongoing maintenance responsibility (no upstream project support)</li><li>Neutral, because two services require coordination but provide clearer boundaries</li><li>Bad, because custom implementation may miss edge cases that Matchbox handles</li><li>Bad, because reinvents machine matching and boot configuration patterns</li><li>Bad, because Cloud Run cold start latency needs monitoring (mitigated with min instances = 1)</li><li>Bad, because service-to-service communication adds latency (mitigated by GCP regional networking)</li></ul><h3 id=confirmation>Confirmation</h3><p>The implementation success will be validated by:</p><ul><li>Successfully deploying custom boot server on GCP Cloud Run</li><li>Successfully network booting HP DL360 Gen 9 via UEFI HTTP boot through WireGuard VPN</li><li>Confirming iLO 4 firmware v2.40+ compatibility with HTTP boot workflow</li><li>Validating boot image upload and versioning workflows via HTTP REST API</li><li>Measuring Cloud Run cold start latency for boot requests (target: &lt; 100ms)</li><li>Measuring boot file request latency for kernel/initrd downloads (target: &lt; 100ms)</li><li>Confirming Cloud Storage integration for boot asset storage</li><li>Testing machine-to-image mapping based on MAC address using Firestore</li><li>Validating WireGuard VPN security for boot traffic (compensating for lack of client cert support)</li><li>Verifying OpenTelemetry observability integration with Cloud Monitoring</li></ul><h2 id=pros-and-cons-of-the-options>Pros and Cons of the Options</h2><h3 id=option-1-custom-server-implementation-go-based>Option 1: Custom Server Implementation (Go-based)</h3><p>Build a custom network boot server in Go, leveraging the existing <code>z5labs/humus</code> framework for HTTP services.</p><h4 id=architecture-overview>Architecture Overview</h4><pre class=mermaid>architecture-beta
    group gcp(cloud)[GCP VPC]

    service wg_nlb(internet)[Network LB] in gcp
    service wireguard(server)[WireGuard Gateway] in gcp
    service https_lb(internet)[HTTPS LB] in gcp
    service compute(server)[Compute Engine] in gcp
    service storage(database)[Cloud Storage] in gcp
    service firestore(database)[Firestore] in gcp
    service secrets(disk)[Secret Manager] in gcp
    service monitoring(internet)[Cloud Monitoring] in gcp

    group homelab(cloud)[Home Lab]
    service udm(server)[UDM Pro] in homelab
    service servers(server)[Bare Metal Servers] in homelab

    servers:L -- R:udm
    udm:R -- L:wg_nlb
    wg_nlb:R -- L:wireguard
    wireguard:R -- L:https_lb
    https_lb:R -- L:compute
    compute:B --&gt; T:storage
    compute:B --&gt; T:firestore
    compute:R --&gt; L:secrets
    compute:T --&gt; B:monitoring</pre><p><strong>Components</strong>:</p><ul><li><strong>Boot Server</strong>: Go service deployed to Cloud Run (or Compute Engine VM as fallback)<ul><li>HTTP/HTTPS server (using <code>z5labs/humus</code> framework with OpenAPI)</li><li>UEFI HTTP boot endpoint serving boot scripts and assets</li><li>HTTP REST admin API for boot configuration management</li></ul></li><li><strong>Cloud Storage</strong>: Buckets for boot images, boot scripts, kernels, initrd files</li><li><strong>Firestore/Datastore</strong>: Machine-to-image mapping database (MAC → boot profile)</li><li><strong>Secret Manager</strong>: WireGuard keys, TLS certificates (optional for HTTPS boot)</li><li><strong>Cloud Monitoring</strong>: Metrics for boot requests, success/failure rates, latency</li></ul><h4 id=boot-image-lifecycle>Boot Image Lifecycle</h4><pre class=mermaid>sequenceDiagram
    participant Admin
    participant API as Boot Server API
    participant Storage as Cloud Storage
    participant DB as Firestore
    participant Monitor as Cloud Monitoring

    Note over Admin,Monitor: Upload Boot Image
    Admin-&gt;&gt;API: POST /api/v1/images (kernel, initrd, metadata)
    API-&gt;&gt;API: Validate image integrity (checksum)
    API-&gt;&gt;Storage: Upload kernel to gs://boot-images/kernels/
    API-&gt;&gt;Storage: Upload initrd to gs://boot-images/initrd/
    API-&gt;&gt;DB: Store metadata (version, checksum, tags)
    API-&gt;&gt;Monitor: Log upload event
    API-&gt;&gt;Admin: 201 Created (image ID)

    Note over Admin,Monitor: Map Machine to Image
    Admin-&gt;&gt;API: POST /api/v1/machines (MAC, image_id, profile)
    API-&gt;&gt;DB: Store machine mapping
    API-&gt;&gt;Admin: 201 Created

    Note over Admin,Monitor: UEFI HTTP Boot Request
    participant Server as Home Lab Server
    Note right of Server: iLO 4 firmware v2.40&#43; initiates HTTP request directly
    Server-&gt;&gt;API: HTTP GET /boot?mac=aa:bb:cc:dd:ee:ff (via WireGuard VPN)
    API-&gt;&gt;DB: Query machine mapping by MAC
    API-&gt;&gt;API: Generate iPXE script (kernel, initrd URLs)
    API-&gt;&gt;Monitor: Log boot script request
    API-&gt;&gt;Server: Send iPXE script
    
    Server-&gt;&gt;API: HTTP GET /kernels/ubuntu-22.04.img
    API-&gt;&gt;Storage: Fetch kernel from Cloud Storage
    API-&gt;&gt;Monitor: Log kernel download (size, duration)
    API-&gt;&gt;Server: Stream kernel file
    
    Server-&gt;&gt;API: HTTP GET /initrd/ubuntu-22.04.img
    API-&gt;&gt;Storage: Fetch initrd from Cloud Storage
    API-&gt;&gt;Monitor: Log initrd download
    API-&gt;&gt;Server: Stream initrd file
    
    Server-&gt;&gt;Server: Boot into OS
    
    Note over Admin,Monitor: Rollback Image Version
    Admin-&gt;&gt;API: POST /api/v1/machines/{mac}/rollback
    API-&gt;&gt;DB: Update machine mapping to previous image_id
    API-&gt;&gt;Monitor: Log rollback event
    API-&gt;&gt;Admin: 200 OK</pre><h4 id=implementation-details>Implementation Details</h4><p><strong>Development Stack</strong>:</p><ul><li><strong>Language</strong>: Go 1.24 (leverage existing Go expertise)</li><li><strong>HTTP Framework</strong>: <code>z5labs/humus</code> (consistent with existing services)</li><li><strong>UEFI Boot</strong>: Standard HTTP handlers (no special libraries needed)</li><li><strong>Storage Client</strong>: <code>cloud.google.com/go/storage</code></li><li><strong>Database</strong>: Firestore for machine mappings (or simple JSON config in Cloud Storage)</li><li><strong>Observability</strong>: OpenTelemetry (metrics, traces, logs to Cloud Monitoring/Trace)</li></ul><p><strong>Deployment</strong>:</p><ul><li><strong>Cloud Run</strong> (preferred - HTTP-only boot enables serverless deployment):<ul><li>Min instances: 1 (ensures fast boot response, avoids cold start delays)</li><li>Max instances: 2 (home lab scale)</li><li>Memory: 512MB</li><li>CPU: 1 vCPU</li><li>Health checks: <code>/health/startup</code>, <code>/health/liveness</code></li><li>Concurrency: 10 requests per instance</li></ul></li><li><strong>Alternative - Compute Engine VM</strong> (if Cloud Run latency unacceptable):<ul><li>e2-micro instance ($6.50/month)</li><li>Container-Optimized OS with Docker</li><li>systemd service for boot server</li><li>Health checks: <code>/health/startup</code>, <code>/health/liveness</code></li></ul></li><li><strong>Networking</strong>:<ul><li>VPC firewall: Allow TCP/80, TCP/443 from WireGuard subnet (no UDP/69 needed)</li><li>Static internal IP for boot server (Compute Engine) or HTTPS Load Balancer (Cloud Run)</li><li>Cloud NAT for outbound connectivity (Cloud Storage access)</li></ul></li></ul><p><strong>Configuration Management</strong>:</p><ul><li>Machine mappings stored in Firestore or Cloud Storage JSON files</li><li>Boot profiles defined in YAML (similar to Matchbox groups):<div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>profiles</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>ubuntu-22.04-server</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>kernel</span><span class=p>:</span><span class=w> </span><span class=l>gs://boot-images/kernels/ubuntu-22.04.img</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>initrd</span><span class=p>:</span><span class=w> </span><span class=l>gs://boot-images/initrd/ubuntu-22.04.img</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>cmdline</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;console=tty0 console=ttyS0&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>cloud_init</span><span class=p>:</span><span class=w> </span><span class=l>gs://boot-images/cloud-init/ubuntu-base.yaml</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>machines</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span>- <span class=nt>mac</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;aa:bb:cc:dd:ee:ff&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>profile</span><span class=p>:</span><span class=w> </span><span class=l>ubuntu-22.04-server</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>hostname</span><span class=p>:</span><span class=w> </span><span class=l>node-01</span><span class=w>
</span></span></span></code></pre></div></li></ul><p><strong>Cost Breakdown</strong>:</p><p><strong>Option A: Cloud Run Deployment</strong> (Preferred):</p><table><thead><tr><th>Component</th><th>Monthly Cost</th></tr></thead><tbody><tr><td>Cloud Run (1 min instance, 512MB, always-on)</td><td>$3.50</td></tr><tr><td>Cloud Storage (50GB boot images)</td><td>$1.00</td></tr><tr><td>Firestore (minimal reads/writes)</td><td>$0.50</td></tr><tr><td>Egress (10 boots × 150MB)</td><td>$0.18</td></tr><tr><td><strong>Total</strong></td><td><strong>~$5.18</strong></td></tr></tbody></table><p><strong>Option B: Compute Engine Deployment</strong> (If Cloud Run latency unacceptable):</p><table><thead><tr><th>Component</th><th>Monthly Cost</th></tr></thead><tbody><tr><td>e2-micro VM (boot server)</td><td>$6.50</td></tr><tr><td>Cloud Storage (50GB boot images)</td><td>$1.00</td></tr><tr><td>Firestore (minimal reads/writes)</td><td>$0.50</td></tr><tr><td>Egress (10 boots × 150MB)</td><td>$0.18</td></tr><tr><td><strong>Total</strong></td><td><strong>~$8.18</strong></td></tr></tbody></table><h4 id=pros-and-cons>Pros and Cons</h4><ul><li>Good, because UEFI HTTP boot eliminates TFTP complexity entirely</li><li>Good, because Cloud Run deployment option reduces operational overhead and infrastructure cost</li><li>Good, because full control over boot server implementation and features</li><li>Good, because leverages existing Go expertise and <code>z5labs/humus</code> framework patterns</li><li>Good, because seamless GCP integration (Cloud Storage, Firestore, Secret Manager, IAM)</li><li>Good, because minimal dependencies (no external projects to track)</li><li>Good, because customizable to specific home lab requirements</li><li>Good, because OpenTelemetry observability built-in from existing patterns</li><li>Good, because can optimize for home lab scale (&lt; 20 machines)</li><li>Good, because lightweight implementation (no unnecessary features)</li><li>Good, because simplified testing (HTTP-only, no TFTP/PXE edge cases)</li><li>Good, because standard HTTP serving is well-understood (lower risk than TFTP)</li><li>Neutral, because development effort required (2-3 weeks for MVP, reduced from 3-4 weeks)</li><li>Neutral, because requires ongoing maintenance and security updates</li><li>Neutral, because Cloud Run cold start latency needs validation (POC required)</li><li>Bad, because reinvents machine matching and boot configuration patterns</li><li>Bad, because testing network boot scenarios still requires hardware</li><li>Bad, because potential for bugs in custom implementation</li><li>Bad, because no community support or established best practices</li><li>Bad, because development time still longer than Matchbox (2-3 weeks vs 1 week)</li></ul><h3 id=option-2-matchbox-based-solution>Option 2: Matchbox-Based Solution</h3><p>Deploy <a href=https://matchbox.psdn.io/>Matchbox</a>, an open-source network boot server developed by CoreOS (now part of Red Hat), to handle UEFI HTTP boot workflows.</p><h4 id=architecture-overview-1>Architecture Overview</h4><pre class=mermaid>architecture-beta
    group gcp(cloud)[GCP VPC]
    
    service wg_nlb(internet)[Network LB] in gcp
    service wireguard(server)[WireGuard Gateway] in gcp
    service https_lb(internet)[HTTPS LB] in gcp
    service compute(server)[Compute Engine] in gcp
    service storage(database)[Cloud Storage] in gcp
    service secrets(disk)[Secret Manager] in gcp
    service monitoring(internet)[Cloud Monitoring] in gcp
    
    group homelab(cloud)[Home Lab]
    service udm(server)[UDM Pro] in homelab
    service servers(server)[Bare Metal Servers] in homelab
    
    servers:L -- R:udm
    udm:R -- L:wg_nlb
    wg_nlb:R -- L:wireguard
    wireguard:R -- L:https_lb
    https_lb:R -- L:compute
    compute:B --&gt; T:storage
    compute:R --&gt; L:secrets
    compute:T --&gt; B:monitoring</pre><p><strong>Components</strong>:</p><ul><li><strong>Matchbox Server</strong>: Container deployed to Cloud Run or Compute Engine VM<ul><li>HTTP/gRPC APIs for boot workflows and configuration</li><li>UEFI HTTP boot support (TFTP disabled)</li><li>Machine grouping and profile templating</li><li>Ignition, Cloud-Init, and generic boot support</li></ul></li><li><strong>Cloud Storage</strong>: Backend for boot assets (mounted via gcsfuse or synced periodically)</li><li><strong>Local Storage</strong> (Compute Engine only): <code>/var/lib/matchbox</code> for assets and configuration (synced from Cloud Storage)</li><li><strong>Secret Manager</strong>: WireGuard keys, Matchbox TLS certificates</li><li><strong>Cloud Monitoring</strong>: Logs from Matchbox container, custom metrics via log parsing</li></ul><h4 id=boot-image-lifecycle-1>Boot Image Lifecycle</h4><pre class=mermaid>sequenceDiagram
    participant Admin
    participant CLI as matchbox CLI / API
    participant Matchbox as Matchbox Server
    participant Storage as Cloud Storage
    participant Monitor as Cloud Monitoring

    Note over Admin,Monitor: Upload Boot Image
    Admin-&gt;&gt;CLI: Upload kernel/initrd via gRPC API
    CLI-&gt;&gt;Matchbox: gRPC CreateAsset(kernel, initrd)
    Matchbox-&gt;&gt;Matchbox: Validate asset integrity
    Matchbox-&gt;&gt;Matchbox: Store to /var/lib/matchbox/assets/
    Matchbox-&gt;&gt;Storage: Sync to gs://boot-assets/ (via sidecar script)
    Matchbox-&gt;&gt;Monitor: Log asset upload event
    Matchbox-&gt;&gt;CLI: Asset ID, checksum

    Note over Admin,Monitor: Create Boot Profile
    Admin-&gt;&gt;CLI: Create profile YAML (kernel, initrd, cmdline)
    CLI-&gt;&gt;Matchbox: gRPC CreateProfile(profile.yaml)
    Matchbox-&gt;&gt;Matchbox: Store to /var/lib/matchbox/profiles/
    Matchbox-&gt;&gt;Storage: Sync profiles to gs://boot-config/
    Matchbox-&gt;&gt;CLI: Profile ID

    Note over Admin,Monitor: Create Machine Group
    Admin-&gt;&gt;CLI: Create group YAML (MAC selector, profile mapping)
    CLI-&gt;&gt;Matchbox: gRPC CreateGroup(group.yaml)
    Matchbox-&gt;&gt;Matchbox: Store to /var/lib/matchbox/groups/
    Matchbox-&gt;&gt;Storage: Sync groups to gs://boot-config/
    Matchbox-&gt;&gt;CLI: Group ID

    Note over Admin,Monitor: UEFI HTTP Boot Request
    participant Server as Home Lab Server
    Note right of Server: iLO 4 firmware v2.40&#43; initiates HTTP request directly
    Server-&gt;&gt;Matchbox: HTTP GET /boot.ipxe?mac=aa:bb:cc:dd:ee:ff (via WireGuard VPN)
    Matchbox-&gt;&gt;Matchbox: Match MAC to group
    Matchbox-&gt;&gt;Matchbox: Render iPXE template with profile
    Matchbox-&gt;&gt;Monitor: Log boot request (MAC, group, profile)
    Matchbox-&gt;&gt;Server: Send iPXE script
    
    Server-&gt;&gt;Matchbox: HTTP GET /assets/ubuntu-22.04-kernel.img
    Matchbox-&gt;&gt;Matchbox: Serve from /var/lib/matchbox/assets/
    Matchbox-&gt;&gt;Monitor: Log asset download (size, duration)
    Matchbox-&gt;&gt;Server: Stream kernel file
    
    Server-&gt;&gt;Matchbox: HTTP GET /assets/ubuntu-22.04-initrd.img
    Matchbox-&gt;&gt;Matchbox: Serve from /var/lib/matchbox/assets/
    Matchbox-&gt;&gt;Monitor: Log asset download
    Matchbox-&gt;&gt;Server: Stream initrd file
    
    Server-&gt;&gt;Server: Boot into OS
    
    Note over Admin,Monitor: Rollback Machine Group
    Admin-&gt;&gt;CLI: Update group YAML (change profile reference)
    CLI-&gt;&gt;Matchbox: gRPC UpdateGroup(group.yaml)
    Matchbox-&gt;&gt;Matchbox: Update /var/lib/matchbox/groups/
    Matchbox-&gt;&gt;Storage: Sync updated group config
    Matchbox-&gt;&gt;Monitor: Log group update
    Matchbox-&gt;&gt;CLI: Success</pre><h4 id=implementation-details-1>Implementation Details</h4><p><strong>Matchbox Deployment</strong>:</p><ul><li><strong>Container</strong>: <code>quay.io/poseidon/matchbox:latest</code> (official image)</li><li><strong>Deployment Options</strong>:<ul><li><strong>Cloud Run</strong> (preferred - HTTP-only boot enables serverless deployment):<ul><li>Min instances: 1 (ensures fast boot response)</li><li>Memory: 1GB RAM (Matchbox recommendation)</li><li>CPU: 1 vCPU</li><li>Storage: Cloud Storage for assets/profiles/groups (via HTTP API)</li></ul></li><li><strong>Compute Engine VM</strong> (if persistent local storage preferred):<ul><li>e2-small instance ($14/month, 2GB RAM recommended for Matchbox)</li><li><code>/var/lib/matchbox</code>: Persistent disk (10GB SSD, $1.70/month)</li><li>Cloud Storage sync: Periodic backup of assets/profiles/groups to <code>gs://matchbox-config/</code></li><li>Option: Use <code>gcsfuse</code> to mount Cloud Storage directly (adds latency but simplifies backups)</li></ul></li></ul></li></ul><p><strong>Configuration Structure</strong>:</p><pre tabindex=0><code>/var/lib/matchbox/
├── assets/           # Boot images (kernels, initrds, ISOs)
│   ├── ubuntu-22.04-kernel.img
│   ├── ubuntu-22.04-initrd.img
│   └── flatcar-stable.img.gz
├── profiles/         # Boot profiles (YAML)
│   ├── ubuntu-server.yaml
│   └── flatcar-container.yaml
└── groups/           # Machine groups (YAML)
    ├── default.yaml
    ├── node-01.yaml
    └── storage-nodes.yaml
</code></pre><p><strong>Example Profile</strong> (<code>profiles/ubuntu-server.yaml</code>):</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>id</span><span class=p>:</span><span class=w> </span><span class=l>ubuntu-22.04-server</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Ubuntu 22.04 LTS Server</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>boot</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>kernel</span><span class=p>:</span><span class=w> </span><span class=l>/assets/ubuntu-22.04-kernel.img</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>initrd</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=l>/assets/ubuntu-22.04-initrd.img</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>args</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=l>console=tty0</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=l>console=ttyS0</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=l>ip=dhcp</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>ignition_id</span><span class=p>:</span><span class=w> </span><span class=l>ubuntu-base.yaml</span><span class=w>
</span></span></span></code></pre></div><p><strong>Example Group</strong> (<code>groups/node-01.yaml</code>):</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>id</span><span class=p>:</span><span class=w> </span><span class=l>node-01</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Node 01 - Ubuntu Server</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>profile</span><span class=p>:</span><span class=w> </span><span class=l>ubuntu-22.04-server</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>selector</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>mac</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;aa:bb:cc:dd:ee:ff&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>hostname</span><span class=p>:</span><span class=w> </span><span class=l>node-01.homelab.local</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>ssh_authorized_keys</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=s2>&#34;ssh-ed25519 AAAA...&#34;</span><span class=w>
</span></span></span></code></pre></div><p><strong>GCP Integration</strong>:</p><ul><li><strong>Cloud Storage Sync</strong>: Cron job or sidecar container to sync <code>/var/lib/matchbox</code> to Cloud Storage<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># Sync every 5 minutes</span>
</span></span><span class=line><span class=cl>*/5 * * * * gsutil -m rsync -r /var/lib/matchbox gs://matchbox-config/
</span></span></code></pre></div></li><li><strong>Secret Manager</strong>: Store Matchbox TLS certificates for gRPC API authentication</li><li><strong>Cloud Monitoring</strong>: Ship Matchbox logs to Cloud Logging, parse for metrics:<ul><li>Boot request count by MAC/group</li><li>Asset download success/failure rates</li><li>TFTP vs HTTP request distribution</li></ul></li></ul><p><strong>Networking</strong>:</p><ul><li>VPC firewall: Allow TCP/8080 (HTTP), TCP/8081 (gRPC) from WireGuard subnet (no UDP/69 needed)</li><li>Optional: Internal load balancer if high availability required (adds ~$18/month)</li><li>Note: Cloud Run deployment includes integrated HTTPS load balancing</li></ul><p><strong>Cost Breakdown</strong>:</p><p><strong>Option A: Cloud Run Deployment</strong> (Preferred):</p><table><thead><tr><th>Component</th><th>Monthly Cost</th></tr></thead><tbody><tr><td>Cloud Run (1 min instance, 1GB RAM, always-on)</td><td>$7.00</td></tr><tr><td>Cloud Storage (50GB boot images)</td><td>$1.00</td></tr><tr><td>Egress (10 boots × 150MB)</td><td>$0.18</td></tr><tr><td><strong>Total</strong></td><td><strong>~$8.18</strong></td></tr></tbody></table><p><strong>Option B: Compute Engine Deployment</strong> (If persistent local storage preferred):</p><table><thead><tr><th>Component</th><th>Monthly Cost</th></tr></thead><tbody><tr><td>e2-small VM (Matchbox server)</td><td>$14.00</td></tr><tr><td>Persistent SSD (10GB)</td><td>$1.70</td></tr><tr><td>Cloud Storage (50GB backups)</td><td>$1.00</td></tr><tr><td>Egress (10 boots × 150MB)</td><td>$0.18</td></tr><tr><td><strong>Total</strong></td><td><strong>~$16.88</strong></td></tr></tbody></table><h4 id=pros-and-cons-1>Pros and Cons</h4><ul><li>Good, because HTTP-only boot enables Cloud Run deployment (reduces cost significantly)</li><li>Good, because UEFI HTTP boot eliminates TFTP complexity and potential failure points</li><li>Good, because production-ready boot server with extensive real-world usage</li><li>Good, because feature-complete with machine grouping, templating, and multi-OS support</li><li>Good, because gRPC API for programmatic boot configuration management</li><li>Good, because supports Ignition (Flatcar, CoreOS), Cloud-Init, and generic boot workflows</li><li>Good, because well-documented with established best practices</li><li>Good, because active community and upstream maintenance (Red Hat/CoreOS)</li><li>Good, because reduces development time to days (deploy + configure vs weeks of coding)</li><li>Good, because avoids reinventing network boot patterns (machine matching, boot configuration)</li><li>Good, because proven security model (TLS for gRPC, asset integrity checks)</li><li>Neutral, because requires learning Matchbox configuration patterns (YAML profiles/groups)</li><li>Neutral, because containerized deployment (Docker on Compute Engine or Cloud Run)</li><li>Neutral, because Cloud Run deployment option competitive with custom implementation cost</li><li>Bad, because introduces external dependency (Matchbox project maintenance)</li><li>Bad, because some features unnecessary for home lab scale (large-scale provisioning, etcd backend)</li><li>Bad, because less control over implementation details (limited customization)</li><li>Bad, because Cloud Storage integration requires custom sync scripts (Matchbox doesn&rsquo;t natively support GCS backend)</li><li>Bad, because dependency on upstream for security patches and bug fixes</li></ul><h2 id=uefi-http-boot-architecture>UEFI HTTP Boot Architecture</h2><p>This section documents the UEFI HTTP boot capability that fundamentally changes the network boot infrastructure design.</p><h3 id=boot-process-overview>Boot Process Overview</h3><p><strong>Traditional PXE Boot</strong> (NOT USED - shown for comparison):</p><pre class=mermaid>sequenceDiagram
    participant Server as Bare Metal Server
    participant DHCP as DHCP Server
    participant TFTP as TFTP Server
    participant HTTP as HTTP Server

    Note over Server,HTTP: Traditional PXE Boot Chain (NOT USED)
    Server-&gt;&gt;DHCP: DHCP Discover
    DHCP-&gt;&gt;Server: DHCP Offer (TFTP server, boot filename)
    Server-&gt;&gt;TFTP: TFTP GET /pxelinux.0
    TFTP-&gt;&gt;Server: Send PXE bootloader
    Server-&gt;&gt;TFTP: TFTP GET /ipxe.efi
    TFTP-&gt;&gt;Server: Send iPXE binary
    Server-&gt;&gt;HTTP: HTTP GET /boot.ipxe
    HTTP-&gt;&gt;Server: Send boot script
    Server-&gt;&gt;HTTP: HTTP GET /kernel, /initrd
    HTTP-&gt;&gt;Server: Stream boot files</pre><p><strong>UEFI HTTP Boot</strong> (ACTUAL IMPLEMENTATION):</p><pre class=mermaid>sequenceDiagram
    participant Server as HP DL360 Gen 9&lt;br/&gt;(iLO 4 v2.40&#43;)
    participant DHCP as DHCP Server&lt;br/&gt;(UDM Pro)
    participant VPN as WireGuard VPN
    participant HTTP as HTTP Boot Server&lt;br/&gt;(GCP Cloud Run)

    Note over Server,HTTP: UEFI HTTP Boot (ACTUAL IMPLEMENTATION)
    Server-&gt;&gt;DHCP: DHCP Discover
    DHCP-&gt;&gt;Server: DHCP Offer (boot URL: http://boot.internal/boot.ipxe?mac=...)
    Note right of Server: Firmware initiates HTTP request directly&lt;br/&gt;(no TFTP/PXE chain loading)
    Server-&gt;&gt;VPN: WireGuard tunnel established
    Server-&gt;&gt;HTTP: HTTP GET /boot.ipxe?mac=aa:bb:cc:dd:ee:ff
    HTTP-&gt;&gt;Server: Send boot script with kernel/initrd URLs
    Server-&gt;&gt;HTTP: HTTP GET /assets/talos-kernel.img
    HTTP-&gt;&gt;Server: Stream kernel (via WireGuard)
    Server-&gt;&gt;HTTP: HTTP GET /assets/talos-initrd.img
    HTTP-&gt;&gt;Server: Stream initrd (via WireGuard)
    Server-&gt;&gt;Server: Boot into OS</pre><h3 id=key-differences>Key Differences</h3><table><thead><tr><th>Aspect</th><th>Traditional PXE</th><th>UEFI HTTP Boot</th></tr></thead><tbody><tr><td><strong>Initial Protocol</strong></td><td>TFTP (UDP/69)</td><td>HTTP (TCP/80) or HTTPS (TCP/443)</td></tr><tr><td><strong>Boot Loader</strong></td><td>Requires TFTP transfer of iPXE binary</td><td>Firmware has HTTP client built-in</td></tr><tr><td><strong>Chain Loading</strong></td><td>PXE → TFTP → iPXE → HTTP</td><td>Direct HTTP boot (no chain)</td></tr><tr><td><strong>Firewall Rules</strong></td><td>UDP/69, TCP/80, TCP/443</td><td>TCP/80, TCP/443 only</td></tr><tr><td><strong>Cloud Run Support</strong></td><td>❌ (UDP not supported)</td><td>✅ (HTTP-only)</td></tr><tr><td><strong>Transfer Speed</strong></td><td>~1-5 Mbps (TFTP)</td><td>10-100 Mbps (HTTP)</td></tr><tr><td><strong>Complexity</strong></td><td>High (multiple protocols)</td><td>Low (HTTP-only)</td></tr></tbody></table><h3 id=security-architecture>Security Architecture</h3><p><strong>Challenge</strong>: HP DL360 Gen 9 UEFI HTTP boot does not support client-side TLS certificates (mTLS).</p><p><strong>Solution</strong>: WireGuard VPN provides transport-layer security:</p><pre class=mermaid>flowchart LR
    subgraph homelab[Home Lab]
        server[HP DL360 Gen 9&lt;br/&gt;UEFI HTTP Boot&lt;br/&gt;iLO 4 v2.40&#43;]
        udm[UDM Pro&lt;br/&gt;WireGuard Client]
    end

    subgraph gcp[Google Cloud Platform]
        wg_gw[WireGuard Gateway&lt;br/&gt;Compute Engine]
        cr[Boot Server&lt;br/&gt;Cloud Run]
    end

    server --&gt;|HTTP| udm
    udm --&gt;|Encrypted WireGuard Tunnel| wg_gw
    wg_gw --&gt;|HTTP| cr

    style server fill:#f9f,stroke:#333
    style udm fill:#bbf,stroke:#333
    style wg_gw fill:#bfb,stroke:#333
    style cr fill:#fbb,stroke:#333</pre><p><strong>Why WireGuard instead of Cloudflare mTLS?</strong></p><ul><li><strong>Cloudflare mTLS Limitation</strong>: Requires client certificates at TLS layer</li><li><strong>UEFI Firmware Limitation</strong>: Cannot present client certificates during TLS handshake</li><li><strong>WireGuard Solution</strong>: Provides mutual authentication at network layer (pre-shared keys)</li><li><strong>Security Equivalent</strong>: WireGuard offers same security properties as mTLS:<ul><li>Mutual authentication (both endpoints authenticated)</li><li>Confidentiality (all traffic encrypted)</li><li>Integrity (authenticated encryption via ChaCha20-Poly1305)</li><li>No Internet exposure (boot server only accessible via VPN)</li></ul></li></ul><h3 id=firmware-configuration>Firmware Configuration</h3><p><strong>HP iLO 4 UEFI HTTP Boot Setup</strong>:</p><ol><li><p><strong>Access Configuration</strong>:</p><ul><li>iLO web interface → Remote Console → Power On → Press F9 (RBSU)</li><li>Or: Direct RBSU access during POST (Press F9)</li></ul></li><li><p><strong>Enable UEFI HTTP Boot</strong>:</p><ul><li>Navigate: <code>System Configuration → BIOS/Platform Configuration (RBSU) → Network Options</code></li><li>Set <code>Network Boot</code> to <code>Enabled</code></li><li>Set <code>Boot Mode</code> to <code>UEFI</code> (not Legacy BIOS)</li><li>Enable <code>UEFI HTTP Boot Support</code></li></ul></li><li><p><strong>Configure NIC</strong>:</p><ul><li>Navigate: <code>RBSU → Network Options → [FlexibleLOM/PCIe NIC]</code></li><li>Set <code>Option ROM</code> to <code>Enabled</code> (required for UEFI boot option to appear)</li><li>Set <code>Network Boot</code> to <code>Enabled</code></li><li>Configure IPv4/IPv6 settings (DHCP or static)</li></ul></li><li><p><strong>Set Boot Order</strong>:</p><ul><li>Navigate: <code>RBSU → Boot Options → UEFI Boot Order</code></li><li>Move network device to top priority</li></ul></li><li><p><strong>Configure Boot URL</strong> (via DHCP or static):</p><ul><li>DHCP option 67: <code>http://10.x.x.x/boot.ipxe?mac=${net0/mac}</code></li><li>Or: Static configuration in UEFI System Utilities</li></ul></li></ol><p><strong>Required Firmware Versions</strong>:</p><ul><li><strong>iLO 4</strong>: v2.40 or later (for UEFI HTTP boot support)</li><li><strong>System ROM</strong>: P89 v2.60 or later (recommended)</li></ul><p><strong>Verification</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># Check iLO firmware version via REST API</span>
</span></span><span class=line><span class=cl>curl -k -u admin:password https://ilo-address/redfish/v1/Managers/1/ <span class=p>|</span> jq <span class=s1>&#39;.FirmwareVersion&#39;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Expected output: &#34;2.40&#34; or higher</span>
</span></span></code></pre></div><h3 id=architectural-implications>Architectural Implications</h3><p><strong>TFTP Elimination Impact</strong>:</p><ol><li><strong>Deployment</strong>: Cloud Run becomes viable (no UDP/TFTP requirement)</li><li><strong>Cost</strong>: Reduced infrastructure costs (~$5-8/month vs $8-17/month)</li><li><strong>Complexity</strong>: Simplified networking (TCP-only firewall rules)</li><li><strong>Development</strong>: Reduced effort (no TFTP library, testing, edge cases)</li><li><strong>Scalability</strong>: Cloud Run autoscaling vs fixed VM capacity</li><li><strong>Maintenance</strong>: Serverless reduces operational overhead</li></ol><p><strong>Decision Impact</strong>:</p><p>The removal of TFTP complexity fundamentally shifts the cost/benefit analysis:</p><ul><li><strong>Custom Implementation</strong>: More attractive (Cloud Run, reduced development time)</li><li><strong>Matchbox</strong>: Still valid but cost/complexity advantage reduced</li><li><strong>TCO Gap</strong>: Narrowed from ~$8,000-12,000 to ~$4,000-8,000 (Year 1)</li><li><strong>Development Gap</strong>: Reduced from 2-3 weeks to 1-2 weeks</li></ul><h2 id=detailed-comparison>Detailed Comparison</h2><h3 id=feature-comparison>Feature Comparison</h3><table><thead><tr><th>Feature</th><th>Custom Implementation</th><th>Matchbox</th></tr></thead><tbody><tr><td><strong>UEFI HTTP Boot</strong></td><td>✅ Native (standard HTTP)</td><td>✅ Built-in</td></tr><tr><td><strong>HTTP/HTTPS Boot</strong></td><td>✅ Via z5labs/humus</td><td>✅ Built-in</td></tr><tr><td><strong>Cloud Run Deployment</strong></td><td>✅ Preferred option</td><td>✅ Enabled by HTTP-only</td></tr><tr><td><strong>Boot Scripting</strong></td><td>✅ Custom templates</td><td>✅ Go templates</td></tr><tr><td><strong>Machine-to-Image Mapping</strong></td><td>✅ Firestore/JSON</td><td>✅ YAML groups with selectors</td></tr><tr><td><strong>Boot Profile Management</strong></td><td>✅ Custom API</td><td>✅ gRPC API + YAML</td></tr><tr><td><strong>Cloud-Init Support</strong></td><td>⚠️ Requires implementation</td><td>✅ Native support</td></tr><tr><td><strong>Ignition Support</strong></td><td>❌ Not planned</td><td>✅ Native support (Flatcar, CoreOS)</td></tr><tr><td><strong>Asset Versioning</strong></td><td>⚠️ Requires implementation</td><td>⚠️ Manual (via Cloud Storage versioning)</td></tr><tr><td><strong>Rollback Capability</strong></td><td>⚠️ Requires implementation</td><td>✅ Update group to previous profile</td></tr><tr><td><strong>OpenTelemetry Observability</strong></td><td>✅ Built-in</td><td>⚠️ Logs only (requires parsing)</td></tr><tr><td><strong>GCP Cloud Storage Integration</strong></td><td>✅ Native SDK</td><td>⚠️ Requires sync scripts</td></tr><tr><td><strong>HTTP REST Admin API</strong></td><td>✅ Native (z5labs/humus)</td><td>⚠️ gRPC only</td></tr><tr><td><strong>Multi-Environment Support</strong></td><td>⚠️ Requires implementation</td><td>✅ Groups + metadata</td></tr></tbody></table><h3 id=development-effort-comparison>Development Effort Comparison</h3><table><thead><tr><th>Task</th><th>Custom Implementation</th><th>Matchbox</th></tr></thead><tbody><tr><td><strong>Initial Setup</strong></td><td>1-2 days (project scaffolding)</td><td>4-8 hours (deployment + config)</td></tr><tr><td><strong>UEFI HTTP Boot</strong></td><td>1-2 days (standard HTTP endpoints)</td><td>✅ Included</td></tr><tr><td><strong>HTTP Boot API</strong></td><td>2-3 days (z5labs/humus endpoints)</td><td>✅ Included</td></tr><tr><td><strong>Machine Matching Logic</strong></td><td>2-3 days (database queries, selectors)</td><td>✅ Included</td></tr><tr><td><strong>Boot Script Templates</strong></td><td>2-3 days (boot script templating)</td><td>✅ Included</td></tr><tr><td><strong>Cloud-Init Support</strong></td><td>3-5 days (parsing, injection)</td><td>✅ Included</td></tr><tr><td><strong>Asset Management</strong></td><td>2-3 days (upload, storage)</td><td>✅ Included</td></tr><tr><td><strong>HTTP REST Admin API</strong></td><td>2-3 days (OpenAPI endpoints)</td><td>✅ Included (gRPC)</td></tr><tr><td><strong>Cloud Run Deployment</strong></td><td>1 day (Cloud Run config)</td><td>1 day (Cloud Run config)</td></tr><tr><td><strong>Testing</strong></td><td>3-5 days (unit, integration, E2E - simplified)</td><td>2-3 days (integration only)</td></tr><tr><td><strong>Documentation</strong></td><td>2-3 days</td><td>1 day (reference existing docs)</td></tr><tr><td><strong>Total Effort</strong></td><td><strong>2-3 weeks</strong></td><td><strong>1 week</strong></td></tr></tbody></table><h3 id=operational-complexity>Operational Complexity</h3><table><thead><tr><th>Aspect</th><th>Custom Implementation</th><th>Matchbox</th></tr></thead><tbody><tr><td><strong>Deployment</strong></td><td>Docker container on Compute Engine</td><td>Docker container on Compute Engine</td></tr><tr><td><strong>Configuration Updates</strong></td><td>API calls or Terraform updates</td><td>YAML file updates + API/filesystem sync</td></tr><tr><td><strong>Monitoring</strong></td><td>OpenTelemetry metrics to Cloud Monitoring</td><td>Log parsing + custom metrics</td></tr><tr><td><strong>Troubleshooting</strong></td><td>Full access to code, custom logging</td><td>Matchbox logs + gRPC API inspection</td></tr><tr><td><strong>Security Patches</strong></td><td>Manual code updates</td><td>Upstream container image updates</td></tr><tr><td><strong>Dependency Updates</strong></td><td>Manual Go module updates</td><td>Upstream Matchbox updates</td></tr><tr><td><strong>Backup/Restore</strong></td><td>Cloud Storage + Firestore backups</td><td>Sync <code>/var/lib/matchbox</code> to Cloud Storage</td></tr></tbody></table><h3 id=cost-comparison-summary>Cost Comparison Summary</h3><p><strong>Comparing Cloud Run Deployments</strong> (Preferred for both options):</p><table><thead><tr><th>Item</th><th>Custom (Cloud Run)</th><th>Matchbox (Cloud Run)</th><th>Difference</th></tr></thead><tbody><tr><td><strong>Compute</strong></td><td>Cloud Run ($3.50/month)</td><td>Cloud Run ($7/month)</td><td>+$3.50/month</td></tr><tr><td><strong>Storage</strong></td><td>Cloud Storage ($1/month)</td><td>Cloud Storage ($1/month)</td><td>$0</td></tr><tr><td><strong>Development</strong></td><td>2-3 weeks @ $100/hour = $8,000-12,000</td><td>1 week @ $100/hour = $4,000</td><td>-$4,000-8,000</td></tr><tr><td><strong>Annual Infrastructure</strong></td><td>~$54</td><td>~$96</td><td>+$42/year</td></tr><tr><td><strong>TCO (Year 1)</strong></td><td>~$8,054-12,054</td><td>~$4,096</td><td><strong>-$3,958-7,958</strong></td></tr><tr><td><strong>TCO (Year 3)</strong></td><td>~$8,162-12,162</td><td>~$4,288</td><td><strong>-$3,874-7,874</strong></td></tr></tbody></table><p><strong>Key Insights</strong>:</p><ul><li>UEFI HTTP boot enables Cloud Run deployment for both options, dramatically reducing infrastructure costs</li><li>Custom implementation TCO gap narrowed from $7,895-11,895 to $3,958-7,958 (Year 1)</li><li>Both options now cost ~$5-8/month for infrastructure (vs $8-17/month with TFTP)</li><li>Development time difference reduced from 2-3 weeks to 1-2 weeks</li><li>Decision is much closer than originally assessed</li></ul><h3 id=risk-analysis>Risk Analysis</h3><table><thead><tr><th>Risk</th><th>Custom Implementation</th><th>Matchbox</th><th>Mitigation</th></tr></thead><tbody><tr><td><strong>Security Vulnerabilities</strong></td><td>Medium (standard HTTP code, well-understood)</td><td>Medium (upstream dependency)</td><td>Both: Monitor for security updates, automated deployments</td></tr><tr><td><strong>Boot Failures</strong></td><td>Medium (HTTP-only reduces complexity)</td><td>Low (battle-tested)</td><td>Custom: Comprehensive E2E testing with real hardware</td></tr><tr><td><strong>Cloud Run Cold Starts</strong></td><td>Medium (needs validation)</td><td>Medium (needs validation)</td><td>Both: Min instances = 1 (always-on)</td></tr><tr><td><strong>Maintenance Burden</strong></td><td>Medium (ongoing code maintenance)</td><td>Low (upstream handles updates)</td><td>Both: Automated deployment pipelines</td></tr><tr><td><strong>GCP Integration Issues</strong></td><td>Low (native SDK)</td><td>Medium (sync scripts)</td><td>Matchbox: Robust sync with error handling</td></tr><tr><td><strong>Scalability Limits</strong></td><td>Low (Cloud Run autoscaling)</td><td>Low (handles thousands of nodes)</td><td>Both: Monitor boot request latency</td></tr><tr><td><strong>Dependency Abandonment</strong></td><td>N/A (no external deps)</td><td>Low (Red Hat backing)</td><td>Matchbox: Can fork if necessary</td></tr></tbody></table><h2 id=implementation-plan>Implementation Plan</h2><h3 id=phase-1-machine-management-service-week-1>Phase 1: Machine Management Service (Week 1)</h3><ol><li><p><strong>Project Setup</strong> (1-2 days)</p><ul><li>Create Go project with <code>z5labs/humus</code> framework</li><li>Set up OpenAPI specification for HTTP REST admin API</li><li>Configure Cloud Storage and Firestore clients</li><li>Implement basic health check endpoints</li></ul></li><li><p><strong>Profile Management API</strong> (2-3 days)</p><ul><li>Boot profile upload endpoint (kernel, initrd, metadata)</li><li>Profile listing and retrieval endpoints</li><li>Kernel/initrd streaming endpoints (<code>GET /api/v1/profiles/{id}/kernel</code>, <code>GET /api/v1/profiles/{id}/initrd</code>)</li><li>Cloud Storage integration for blob management</li></ul></li><li><p><strong>Machine Management API</strong> (2-3 days)</p><ul><li>Machine registration endpoints</li><li>MAC address to profile mapping</li><li>Machine listing and updates</li><li>Firestore integration for machine mappings</li></ul></li></ol><h3 id=phase-2-boot-service-week-2>Phase 2: Boot Service (Week 2)</h3><ol><li><p><strong>UEFI HTTP Boot Endpoints</strong> (2-3 days)</p><ul><li>HTTP endpoint serving boot scripts (iPXE format)</li><li>Kernel and initrd streaming endpoints (proxy to Machine Management Service)</li><li>MAC-based machine matching via Machine Management Service API</li><li>Boot script templating with machine-specific parameters</li></ul></li><li><p><strong>Testing & Deployment</strong> (2-3 days)</p><ul><li>Deploy both services to Cloud Run with min instances = 1</li><li>Configure WireGuard VPN connectivity</li><li>Test UEFI HTTP boot from HP DL360 Gen 9 (iLO 4 v2.40+)</li><li>Validate boot latency and service-to-service communication</li></ul></li></ol><h3 id=phase-3-integration--observability-week-2-3>Phase 3: Integration & Observability (Week 2-3)</h3><ol><li><p><strong>Service Integration</strong> (1-2 days)</p><ul><li>Configure service-to-service authentication (if needed)</li><li>Optimize streaming performance between services</li><li>Handle error scenarios and fallbacks</li></ul></li><li><p><strong>Observability & Documentation</strong> (2-3 days)</p><ul><li>OpenTelemetry metrics integration (both services)</li><li>Distributed tracing across services</li><li>Cloud Monitoring dashboards</li><li>API documentation</li><li>Operational runbooks</li></ul></li></ol><h3 id=success-criteria>Success Criteria</h3><ul><li>✅ Successfully boot HP DL360 Gen 9 via UEFI HTTP boot through WireGuard VPN</li><li>✅ Boot latency &lt; 100ms for HTTP requests to Boot Service</li><li>✅ Service-to-service latency &lt; 50ms (Boot Service → Machine Management Service)</li><li>✅ Cloud Run cold start latency &lt; 100ms (with min instances = 1 for both services)</li><li>✅ Machine-to-profile mapping works correctly based on MAC address</li><li>✅ Cloud Storage integration functional (upload, retrieve boot assets)</li><li>✅ HTTP REST API fully functional for boot configuration management</li><li>✅ Firestore stores machine mappings and boot profiles correctly</li><li>✅ OpenTelemetry distributed tracing works across both services</li><li>✅ Configuration update workflow clear and documented</li><li>✅ Firmware compatibility confirmed (no TFTP fallback needed)</li></ul><h2 id=more-information>More Information</h2><h3 id=related-resources>Related Resources</h3><ul><li><a href=https://matchbox.psdn.io/>Matchbox Documentation</a></li><li><a href=https://github.com/poseidon/matchbox>Matchbox GitHub Repository</a></li><li><a href=https://ipxe.org/howto/chainloading>iPXE Boot Process</a></li><li><a href=https://en.wikipedia.org/wiki/Preboot_Execution_Environment>PXE Boot Specification</a></li><li><a href=https://www.flatcar.org/docs/latest/provisioning/network-boot/>Flatcar Linux Provisioning with Matchbox</a></li><li><a href=https://coreos.github.io/ignition/>CoreOS Ignition Specification</a></li><li><a href=https://cloudinit.readthedocs.io/>Cloud-Init Documentation</a></li></ul><h3 id=related-adrs>Related ADRs</h3><ul><li><a href=./0002-network-boot-architecture/>ADR-0002: Network Boot Architecture</a> - Established cloud-hosted boot server with VPN</li><li><a href=./0003-cloud-provider-selection/>ADR-0003: Cloud Provider Selection</a> - Selected GCP as hosting provider</li><li><a href=./0001-use-madr-for-architecture-decision-records/>ADR-0001: Use MADR for Architecture Decision Records</a> - MADR format</li></ul><h3 id=future-considerations>Future Considerations</h3><ol><li><strong>High Availability</strong>: If boot server uptime becomes critical, evaluate multi-region deployment or failover strategies</li><li><strong>Multi-Cloud</strong>: If multi-cloud strategy emerges, custom implementation provides better portability</li><li><strong>Enterprise Features</strong>: If advanced provisioning workflows required (bare metal Kubernetes, Ignition support, etc.), evaluate adding features to custom implementation</li><li><strong>Asset Versioning</strong>: Implement comprehensive boot image versioning and rollback capabilities beyond basic Cloud Storage versioning</li><li><strong>Multi-Environment Support</strong>: Add support for multiple environments (dev, staging, prod) with environment-specific boot profiles</li></ol><h3 id=related-issues>Related Issues</h3><ul><li><a href=https://github.com/Zaba505/infra/issues/601>Issue #601</a> - story(docs): create adr for network boot infrastructure on google cloud</li><li><a href=https://github.com/Zaba505/infra/issues/595>Issue #595</a> - story(docs): create adr for network boot architecture</li><li><a href=https://github.com/Zaba505/infra/issues/597>Issue #597</a> - story(docs): create adr for cloud provider selection</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-d234efec001fb7b9899c0a45bea1ae5d>6 - [0006] Universal Resource Identifier Standard</h1><div class=lead>Select a consistent resource identifier format to be used across all types of resources in the system for tracking, linking, and reference.</div><h2 id=context-and-problem-statement>Context and Problem Statement</h2><p>As the infrastructure and services grow, we need a consistent way to identify resources across different systems, databases, APIs, and logs. Resources include servers, VMs, containers, services, configurations, and any other trackable entities. Without a standardized identifier format, we risk inconsistencies in references, difficulty correlating logs and traces, and challenges in cross-system integration.</p><p>What identifier format should we standardize on for all resources to ensure uniqueness, readability, and compatibility across our entire infrastructure?</p><h2 id=decision-drivers>Decision Drivers</h2><ul><li>Global uniqueness across all resources and systems</li><li>Human readability for debugging and operational tasks</li><li>Compatibility with various systems (databases, URLs, file systems, logs)</li><li>Sortability and ability to extract creation time information</li><li>Low collision probability without coordination</li><li>Performance considerations for generation and storage</li><li>Industry standard practices and ecosystem tooling support</li></ul><h2 id=considered-options>Considered Options</h2><ul><li>UUIDv4 (Random UUID)</li><li>UUIDv7 (Time-ordered UUID)</li><li>ULID (Universally Unique Lexicographically Sortable Identifier)</li><li>Custom sequential IDs</li><li>Snowflake IDs</li></ul><h2 id=decision-outcome>Decision Outcome</h2><p>Chosen option: &ldquo;UUIDv7 (Time-ordered UUID)&rdquo;, because it provides the best balance of standardization, performance, and functionality. UUIDv7 maintains full UUID compatibility while addressing the major weaknesses of UUIDv4 (poor database index performance and lack of temporal ordering). As a new IETF standard (RFC 9562), it has growing ecosystem support and is becoming available in standard libraries, making it a future-proof choice.</p><h3 id=consequences>Consequences</h3><ul><li>Good, because time-ordered IDs improve database index performance and reduce fragmentation</li><li>Good, because sortability by creation time simplifies debugging and operational tasks</li><li>Good, because extractable timestamps provide valuable metadata without additional storage</li><li>Good, because full UUID compatibility ensures broad system and tool support</li><li>Good, because no coordination needed between distributed systems for generation</li><li>Good, because growing standard library support reduces dependency burden</li><li>Neutral, because migration from existing UUIDv4 identifiers will need to be managed incrementally</li><li>Bad, because not all systems have UUIDv7 support yet (requires library updates or polyfills)</li><li>Bad, because IDs are still 36 characters, less compact than alternatives like ULID</li></ul><h3 id=confirmation>Confirmation</h3><p>Implementation compliance will be confirmed by:</p><ul><li>Code reviews ensuring all new resource types use the standardized identifier format</li><li>Linting rules or code generation templates that enforce the identifier format</li><li>Documentation updates reflecting the standard identifier format</li><li>Database schema reviews to verify proper column types and indexing</li></ul><h2 id=pros-and-cons-of-the-options>Pros and Cons of the Options</h2><h3 id=uuidv4-random-uuid>UUIDv4 (Random UUID)</h3><p>Standard 128-bit random identifier following RFC 4122, format: <code>550e8400-e29b-41d4-a716-446655440000</code></p><ul><li>Good, because widely supported across all languages, databases, and systems</li><li>Good, because cryptographically random with extremely low collision probability</li><li>Good, because no coordination needed between systems for generation</li><li>Good, because standard library support in most languages</li><li>Neutral, because 36 characters (with hyphens) or 32 hex characters</li><li>Bad, because not time-ordered, leading to poor database index performance</li><li>Bad, because not sortable by creation time</li><li>Bad, because difficult for humans to read or verify</li><li>Bad, because provides no temporal information</li></ul><h3 id=uuidv7-time-ordered-uuid>UUIDv7 (Time-ordered UUID)</h3><p>Latest UUID standard (RFC 9562) with millisecond timestamp prefix, format: <code>018c7dbd-9265-7000-8000-123456789abc</code></p><ul><li>Good, because maintains UUID compatibility while adding time-ordering</li><li>Good, because better database index performance than UUIDv4 due to ordering</li><li>Good, because sortable by creation time (millisecond precision)</li><li>Good, because extractable timestamp for debugging</li><li>Good, because growing ecosystem support and standard library adoption</li><li>Good, because globally unique without coordination</li><li>Neutral, because same 36/32 character length as UUIDv4</li><li>Neutral, because newer standard, not yet universally supported in all systems</li><li>Bad, because still not particularly human-readable</li></ul><h3 id=ulid-universally-unique-lexicographically-sortable-identifier>ULID (Universally Unique Lexicographically Sortable Identifier)</h3><p>26-character base32 encoded identifier with millisecond timestamp, format: <code>01ARZ3NDEKTSV4RRFFQ69G5FAV</code></p><ul><li>Good, because lexicographically sortable by creation time</li><li>Good, because shorter than UUID (26 chars vs 36) making it more readable</li><li>Good, because case-insensitive base32 encoding avoids ambiguous characters</li><li>Good, because extractable millisecond timestamp</li><li>Good, because URL-safe without escaping</li><li>Good, because better database index performance due to ordering</li><li>Neutral, because requires library support (not in standard libraries yet)</li><li>Neutral, because growing adoption but not as universal as UUID</li><li>Bad, because not an official IETF standard (though widely used)</li><li>Bad, because 26 characters still not trivially human-memorable</li></ul><h3 id=custom-sequential-ids>Custom sequential IDs</h3><p>System-specific sequential numbering (e.g., <code>SRV-00001</code>, <code>VM-12345</code>)</p><ul><li>Good, because very human-readable and memorable</li><li>Good, because short and compact</li><li>Good, because naturally sortable</li><li>Bad, because requires centralized coordination for uniqueness</li><li>Bad, because difficult to merge or sync across distributed systems</li><li>Bad, because exposes information about total resource count</li><li>Bad, because not globally unique without namespace management</li><li>Bad, because potential security concerns (predictability, enumeration)</li></ul><h3 id=snowflake-ids>Snowflake IDs</h3><p>64-bit IDs with timestamp, datacenter, and sequence components (Twitter Snowflake pattern)</p><ul><li>Good, because compact 64-bit integer format</li><li>Good, because time-ordered and sortable</li><li>Good, because efficient storage and indexing</li><li>Good, because high performance generation</li><li>Neutral, because requires coordination for datacenter/worker IDs</li><li>Bad, because not human-readable (large integers)</li><li>Bad, because requires infrastructure for ID generation service</li><li>Bad, because 64-bit limit may be reached for very high-volume systems</li><li>Bad, because less portable across systems than string-based identifiers</li></ul><h2 id=more-information>More Information</h2><ul><li>UUIDv7 specification: RFC 9562 - <a href=https://datatracker.ietf.org/doc/html/rfc9562>https://datatracker.ietf.org/doc/html/rfc9562</a></li><li>UUIDv4 specification: RFC 4122 - <a href=https://datatracker.ietf.org/doc/html/rfc4122>https://datatracker.ietf.org/doc/html/rfc4122</a></li><li>ULID specification: <a href=https://github.com/ulid/spec>https://github.com/ulid/spec</a></li><li>Considerations for database indexing performance with different ID types</li><li>Impact on API design, URL structure, and client implementation</li><li>Migration strategy for existing resources using different identifier formats</li></ul></div></main></div></div><footer class="td-footer row d-print-none"><div class=container-fluid><div class="row mx-md-2"><div class="td-footer__left col-6 col-sm-4 order-sm-1"></div><div class="td-footer__right col-6 col-sm-4 order-sm-3"><ul class=td-footer__links-list><li class=td-footer__links-item data-bs-toggle=tooltip title=GitHub aria-label=GitHub><a target=_blank rel=noopener href=https://github.com/Zaba505/infra aria-label=GitHub><i class="fab fa-github"></i></a></li></ul></div><div class="td-footer__center col-12 col-sm-4 py-2 order-sm-2"><span class=td-footer__copyright>&copy;
2024&ndash;2025
<span class=td-footer__authors>Zaba505 | <a href=https://creativecommons.org/licenses/by/4.0>CC BY 4.0</a> |</span></span><span class=td-footer__all_rights_reserved>All Rights Reserved</span></div></div></div></footer></div><script src=/infra/pr-preview/pr-626/js/main.min.eb40505784d893e4b5c8dbd67b59c353e735d847f4ffbfe9d6921dec08dbacba.js integrity="sha256-60BQV4TYk+S1yNvWe1nDU+c12Ef0/7/p1pId7AjbrLo=" crossorigin=anonymous></script><script defer src=/infra/pr-preview/pr-626/js/click-to-copy.min.73478a7d4807698aed7e355eb23f9890ca18fea3158604c8471746d046702bad.js integrity="sha256-c0eKfUgHaYrtfjVesj+YkMoY/qMVhgTIRxdG0EZwK60=" crossorigin=anonymous></script><script src=/infra/pr-preview/pr-626/js/tabpane-persist.js></script></body></html>