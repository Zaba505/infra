<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><link rel=canonical type=text/html href=https://zaba505.github.io/infra/pr-preview/pr-600/rd/adrs/><link rel=alternate type=application/rss+xml href=https://zaba505.github.io/infra/pr-preview/pr-600/rd/adrs/index.xml><meta name=robots content="noindex, nofollow"><link rel="shortcut icon" href=/infra/pr-preview/pr-600/favicons/favicon.ico><link rel=apple-touch-icon href=/infra/pr-preview/pr-600/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=icon type=image/png href=/infra/pr-preview/pr-600/favicons/favicon-16x16.png sizes=16x16><link rel=icon type=image/png href=/infra/pr-preview/pr-600/favicons/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/infra/pr-preview/pr-600/favicons/android-36x36.png sizes=36x36><link rel=icon type=image/png href=/infra/pr-preview/pr-600/favicons/android-48x48.png sizes=48x48><link rel=icon type=image/png href=/infra/pr-preview/pr-600/favicons/android-72x72.png sizes=72x72><link rel=icon type=image/png href=/infra/pr-preview/pr-600/favicons/android-96x96.png sizes=96x96><link rel=icon type=image/png href=/infra/pr-preview/pr-600/favicons/android-144x144.png sizes=144x144><link rel=icon type=image/png href=/infra/pr-preview/pr-600/favicons/android-192x192.png sizes=192x192><title>Architecture Decision Records | Zaba505's Home Lab</title><meta name=description content="Documentation of architectural decisions made using MADR 4.0.0 standard"><meta property="og:url" content="https://zaba505.github.io/infra/pr-preview/pr-600/rd/adrs/"><meta property="og:site_name" content="Zaba505's Home Lab"><meta property="og:title" content="Architecture Decision Records"><meta property="og:description" content="Documentation of architectural decisions made using MADR 4.0.0 standard"><meta property="og:locale" content="en_us"><meta property="og:type" content="website"><meta itemprop=name content="Architecture Decision Records"><meta itemprop=description content="Documentation of architectural decisions made using MADR 4.0.0 standard"><meta itemprop=dateModified content="2025-11-17T04:41:28+00:00"><meta itemprop=wordCount content="195"><meta name=twitter:card content="summary"><meta name=twitter:title content="Architecture Decision Records"><meta name=twitter:description content="Documentation of architectural decisions made using MADR 4.0.0 standard"><link rel=preload href=/infra/pr-preview/pr-600/scss/main.min.74eef40c5172b0e2f11bd9c3ea40dba66c2dc642ac5294c208f5dc9ff772c0e9.css as=style integrity="sha256-dO70DFFysOLxG9nD6kDbpmwtxkKsUpTCCPXcn/dywOk=" crossorigin=anonymous><link href=/infra/pr-preview/pr-600/scss/main.min.74eef40c5172b0e2f11bd9c3ea40dba66c2dc642ac5294c208f5dc9ff772c0e9.css rel=stylesheet integrity="sha256-dO70DFFysOLxG9nD6kDbpmwtxkKsUpTCCPXcn/dywOk=" crossorigin=anonymous><script src=https://code.jquery.com/jquery-3.7.1.min.js integrity="sha512-v2CJ7UaYy4JwqLDIrZUI/4hqeoQieOmAZNXBeQyjo21dadnwR+8ZaIJVT8EE2iyI61OV8e6M8PP2/4hpQINQ/g==" crossorigin=anonymous></script><script defer src=https://unpkg.com/lunr@2.3.9/lunr.min.js integrity=sha384-203J0SNzyqHby3iU6hzvzltrWi/M41wOP5Gu+BiJMz5nwKykbkUx8Kp7iti0Lpli crossorigin=anonymous></script></head><body class=td-section><header><nav class="td-navbar js-navbar-scroll" data-bs-theme=dark><div class="container-fluid flex-column flex-md-row"><a class=navbar-brand href=/infra/pr-preview/pr-600/><span class="navbar-brand__logo navbar-logo"></span><span class=navbar-brand__name>Zaba505's Home Lab</span></a><div class="td-navbar-nav-scroll ms-md-auto" id=main_navbar><ul class=navbar-nav><li class="td-light-dark-menu nav-item dropdown"><svg class="d-none"><symbol id="check2" viewBox="0 0 16 16"><path d="M13.854 3.646a.5.5.0 010 .708l-7 7a.5.5.0 01-.708.0l-3.5-3.5a.5.5.0 11.708-.708L6.5 10.293l6.646-6.647a.5.5.0 01.708.0z"/></symbol><symbol id="circle-half" viewBox="0 0 16 16"><path d="M8 15A7 7 0 108 1v14zm0 1A8 8 0 118 0a8 8 0 010 16z"/></symbol><symbol id="moon-stars-fill" viewBox="0 0 16 16"><path d="M6 .278a.768.768.0 01.08.858 7.208 7.208.0 00-.878 3.46c0 4.021 3.278 7.277 7.318 7.277.527.0 1.04-.055 1.533-.16a.787.787.0 01.81.316.733.733.0 01-.031.893A8.349 8.349.0 018.344 16C3.734 16 0 12.286.0 7.71.0 4.266 2.114 1.312 5.124.06A.752.752.0 016 .278z"/><path d="M10.794 3.148a.217.217.0 01.412.0l.387 1.162c.173.518.579.924 1.097 1.097l1.162.387a.217.217.0 010 .412l-1.162.387A1.734 1.734.0 0011.593 7.69l-.387 1.162a.217.217.0 01-.412.0l-.387-1.162A1.734 1.734.0 009.31 6.593l-1.162-.387a.217.217.0 010-.412l1.162-.387a1.734 1.734.0 001.097-1.097l.387-1.162zM13.863.099a.145.145.0 01.274.0l.258.774c.115.346.386.617.732.732l.774.258a.145.145.0 010 .274l-.774.258a1.156 1.156.0 00-.732.732l-.258.774a.145.145.0 01-.274.0l-.258-.774a1.156 1.156.0 00-.732-.732l-.774-.258a.145.145.0 010-.274l.774-.258c.346-.115.617-.386.732-.732L13.863.1z"/></symbol><symbol id="sun-fill" viewBox="0 0 16 16"><path d="M8 12a4 4 0 100-8 4 4 0 000 8zM8 0a.5.5.0 01.5.5v2a.5.5.0 01-1 0v-2A.5.5.0 018 0zm0 13a.5.5.0 01.5.5v2a.5.5.0 01-1 0v-2A.5.5.0 018 13zm8-5a.5.5.0 01-.5.5h-2a.5.5.0 010-1h2a.5.5.0 01.5.5zM3 8a.5.5.0 01-.5.5h-2a.5.5.0 010-1h2A.5.5.0 013 8zm10.657-5.657a.5.5.0 010 .707l-1.414 1.415a.5.5.0 11-.707-.708l1.414-1.414a.5.5.0 01.707.0zm-9.193 9.193a.5.5.0 010 .707L3.05 13.657a.5.5.0 01-.707-.707l1.414-1.414a.5.5.0 01.707.0zm9.193 2.121a.5.5.0 01-.707.0l-1.414-1.414a.5.5.0 01.707-.707l1.414 1.414a.5.5.0 010 .707zM4.464 4.465a.5.5.0 01-.707.0L2.343 3.05a.5.5.0 11.707-.707l1.414 1.414a.5.5.0 010 .708z"/></symbol></svg>
<button class="btn btn-link nav-link dropdown-toggle d-flex align-items-center" id=bd-theme type=button aria-expanded=false data-bs-toggle=dropdown data-bs-display=static aria-label="Toggle theme (auto)">
<svg class="bi my-1 theme-icon-active"><use href="#circle-half"/></svg></button><ul class="dropdown-menu dropdown-menu-end" aria-labelledby=bd-theme-text><li><button type=button class="dropdown-item d-flex align-items-center" data-bs-theme-value=light aria-pressed=false>
<svg class="bi me-2 opacity-50"><use href="#sun-fill"/></svg>
Light
<svg class="bi ms-auto d-none"><use href="#check2"/></svg></button></li><li><button type=button class="dropdown-item d-flex align-items-center" data-bs-theme-value=dark aria-pressed=false>
<svg class="bi me-2 opacity-50"><use href="#moon-stars-fill"/></svg>
Dark
<svg class="bi ms-auto d-none"><use href="#check2"/></svg></button></li><li><button type=button class="dropdown-item d-flex align-items-center active" data-bs-theme-value=auto aria-pressed=true>
<svg class="bi me-2 opacity-50"><use href="#circle-half"/></svg>
Auto
<svg class="bi ms-auto d-none"><use href="#check2"/></svg></button></li></ul></li></ul></div><div class="d-none d-lg-block"><div class="td-search td-search--offline"><div class=td-search__icon></div><input type=search class="td-search__input form-control" placeholder="Search this site…" aria-label="Search this site…" autocomplete=off data-offline-search-index-json-src=/infra/pr-preview/pr-600/offline-search-index.d9df6274dae3ea2f1c39072258a1d19e.json data-offline-search-base-href=/ data-offline-search-max-results=10></div></div></div></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><main class="col-12 col-md-9 col-xl-8 ps-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>This is the multi-page printable view of this section.
<a href=# onclick="return print(),!1">Click here to print</a>.</p><p><a href=/infra/pr-preview/pr-600/rd/adrs/>Return to the regular view of this page</a>.</p></div><h1 class=title>Architecture Decision Records</h1><div class=lead>Documentation of architectural decisions made using MADR 4.0.0 standard</div><ul><li>1: <a href=#pg-be14bb13e79709979af80fa8e61452c5>[0001] Use MADR for Architecture Decision Records</a></li><li>2: <a href=#pg-79e3b41ba15ba3a179ad4b5df5f3f2fe>[0002] Network Boot Architecture for Home Lab</a></li><li>3: <a href=#pg-853f5ec590d44937c1ebd528cd046965>[0003] Cloud Provider Selection for Network Boot Infrastructure</a></li><li>4: <a href=#pg-dab026fd06f98a03a459f97073d21662>[0004] Server Operating System Selection</a></li></ul><div class=content><h2 id=architecture-decision-records-adrs>Architecture Decision Records (ADRs)</h2><p>This section contains architectural decision records that document the key design choices made. Each ADR follows the MADR 4.0.0 format and includes:</p><ul><li>Context and problem statement</li><li>Decision drivers and constraints</li><li>Considered options with pros and cons</li><li>Decision outcome and rationale</li><li>Consequences (positive and negative)</li><li>Confirmation methods</li></ul><h3 id=adr-categories>ADR Categories</h3><p>ADRs are classified into three categories:</p><ul><li><strong>Strategic</strong> - High-level architectural decisions affecting the entire system (frameworks, authentication strategies, cross-cutting patterns). Use for foundational technology choices.</li><li><strong>User Journey</strong> - Decisions solving specific user journey problems. More tactical than strategic, but still architectural. Use when evaluating approaches to implement user-facing features.</li><li><strong>API Design</strong> - API endpoint implementation decisions (pagination, filtering, bulk operations). Use for significant API design trade-offs that warrant documentation.</li></ul><h3 id=status-values>Status Values</h3><p>Each ADR has a status that reflects its current state:</p><ul><li><code>proposed</code> - Decision is under consideration</li><li><code>accepted</code> - Decision has been approved and should be implemented</li><li><code>rejected</code> - Decision was considered but not approved</li><li><code>deprecated</code> - Decision is no longer relevant or has been superseded</li><li><code>superseded by ADR-XXXX</code> - Decision has been replaced by a newer ADR</li></ul><p>These records provide historical context for architectural decisions and help ensure consistency across the platform.</p></div></div><div class=td-content style=page-break-before:always><h1 id=pg-be14bb13e79709979af80fa8e61452c5>1 - [0001] Use MADR for Architecture Decision Records</h1><div class=lead>Adopt Markdown Architectural Decision Records (MADR) as the standard format for documenting architectural decisions in the project.</div><h2 id=context-and-problem-statement>Context and Problem Statement</h2><p>As the project grows, architectural decisions are made that have long-term impacts on the system&rsquo;s design, maintainability, and scalability. Without a structured way to document these decisions, we risk losing the context and rationale behind important choices, making it difficult for current and future team members to understand why certain approaches were taken.</p><p>How should we document architectural decisions in a way that is accessible, maintainable, and provides sufficient context for future reference?</p><h2 id=decision-drivers>Decision Drivers</h2><ul><li>Need for clear documentation of architectural decisions and their rationale</li><li>Easy accessibility and searchability of past decisions</li><li>Low barrier to entry for creating and maintaining decision records</li><li>Integration with existing documentation workflow</li><li>Version control friendly format</li><li>Industry-standard approach that team members may already be familiar with</li></ul><h2 id=considered-options>Considered Options</h2><ul><li>MADR (Markdown Architectural Decision Records)</li><li>ADR using custom format</li><li>Wiki-based documentation</li><li>No formal ADR process</li></ul><h2 id=decision-outcome>Decision Outcome</h2><p>Chosen option: &ldquo;MADR (Markdown Architectural Decision Records)&rdquo;, because it provides a well-established, standardized format that is lightweight, version-controlled, and integrates seamlessly with our existing documentation structure. MADR 4.0.0 offers a clear template that captures all necessary information while remaining flexible enough for different types of decisions.</p><h3 id=consequences>Consequences</h3><ul><li>Good, because MADR is a widely adopted standard with clear documentation and examples</li><li>Good, because markdown files are easy to create, edit, and review through pull requests</li><li>Good, because ADRs will be version-controlled alongside code, maintaining historical context</li><li>Good, because the format is flexible enough to accommodate strategic, user-journey, and API design decisions</li><li>Good, because team members can easily search and reference past decisions</li><li>Neutral, because requires discipline to maintain and update ADR status as decisions evolve</li><li>Bad, because team members need to learn and follow the MADR format conventions</li></ul><h3 id=confirmation>Confirmation</h3><p>Compliance will be confirmed through:</p><ul><li>Code reviews ensuring new architectural decisions are documented as ADRs</li><li>ADRs are stored in <code>docs/content/r&amp;d/adrs/</code> following the naming convention <code>NNNN-title-with-dashes.md</code></li><li>Regular reviews during architecture discussions to reference and update existing ADRs</li></ul><h2 id=pros-and-cons-of-the-options>Pros and Cons of the Options</h2><h3 id=madr-markdown-architectural-decision-records>MADR (Markdown Architectural Decision Records)</h3><p>MADR 4.0.0 is a standardized format for documenting architectural decisions using markdown.</p><ul><li>Good, because it&rsquo;s a well-established standard with extensive documentation</li><li>Good, because markdown is simple, portable, and version-control friendly</li><li>Good, because it provides a clear structure while remaining flexible</li><li>Good, because it integrates with static site generators and documentation tools</li><li>Good, because it&rsquo;s lightweight and doesn&rsquo;t require special tools</li><li>Neutral, because it requires some initial learning of the format</li><li>Neutral, because maintaining consistency requires discipline</li></ul><h3 id=adr-using-custom-format>ADR using custom format</h3><p>Create our own custom format for architectural decision records.</p><ul><li>Good, because we can tailor it exactly to our needs</li><li>Bad, because it requires defining and maintaining our own standard</li><li>Bad, because new team members won&rsquo;t be familiar with the format</li><li>Bad, because we lose the benefits of community knowledge and tooling</li><li>Bad, because it may evolve inconsistently over time</li></ul><h3 id=wiki-based-documentation>Wiki-based documentation</h3><p>Use a wiki system (like Confluence, Notion, or GitHub Wiki) to document decisions.</p><ul><li>Good, because wikis provide easy editing and hyperlinking</li><li>Good, because some team members may be familiar with wiki tools</li><li>Neutral, because it may or may not integrate with version control</li><li>Bad, because content may not be version-controlled alongside code</li><li>Bad, because it creates a separate system to maintain</li><li>Bad, because it&rsquo;s harder to review changes through standard PR process</li><li>Bad, because portability and long-term accessibility may be concerns</li></ul><h3 id=no-formal-adr-process>No formal ADR process</h3><p>Continue without a structured approach to documenting architectural decisions.</p><ul><li>Good, because it requires no additional overhead</li><li>Bad, because context and rationale for decisions are lost over time</li><li>Bad, because new team members struggle to understand why decisions were made</li><li>Bad, because it leads to repeated discussions of previously settled questions</li><li>Bad, because it makes it difficult to track when decisions should be revisited</li></ul><h2 id=more-information>More Information</h2><ul><li>MADR 4.0.0 specification: <a href=https://adr.github.io/madr/>https://adr.github.io/madr/</a></li><li>ADRs will be categorized as: strategic, user-journey, or api-design</li><li>ADR status values: proposed | accepted | rejected | deprecated | superseded by ADR-XXXX</li><li>All ADRs are stored in <code>docs/content/r&amp;d/adrs/</code> directory</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-79e3b41ba15ba3a179ad4b5df5f3f2fe>2 - [0002] Network Boot Architecture for Home Lab</h1><div class=lead>Evaluate options for network booting servers in a home lab environment, considering local vs cloud-hosted boot servers.</div><h2 id=context-and-problem-statement>Context and Problem Statement</h2><p>When setting up a home lab infrastructure, servers need to be provisioned and booted over the network using PXE (Preboot Execution Environment). This requires a TFTP/HTTP server to serve boot files to requesting machines. The question is: where should this boot server be hosted to balance security, reliability, cost, and operational complexity?</p><h2 id=decision-drivers>Decision Drivers</h2><ul><li><strong>Security</strong>: Minimize attack surface and ensure only authorized servers receive boot files</li><li><strong>Reliability</strong>: Boot process should be resilient and not dependent on external network connectivity</li><li><strong>Cost</strong>: Minimize ongoing infrastructure costs</li><li><strong>Complexity</strong>: Keep the operational burden manageable</li><li><strong>Trust Model</strong>: Clear verification of requesting server identity</li></ul><h2 id=considered-options>Considered Options</h2><ul><li>Option 1: TFTP/HTTP server locally on home lab network</li><li>Option 2: TFTP/HTTP server on public cloud (without VPN)</li><li>Option 3: TFTP/HTTP server on public cloud (with VPN)</li></ul><h2 id=decision-outcome>Decision Outcome</h2><p>Chosen option: &ldquo;Option 3: TFTP/HTTP server on public cloud (with VPN)&rdquo;, because:</p><ol><li><strong>No local machine management</strong>: Unlike Option 1, this avoids the need to maintain dedicated local hardware for the boot server, reducing operational overhead</li><li><strong>Secure protocol support</strong>: The VPN tunnel encrypts all traffic, allowing unsecured protocols like TFTP to be used without risk of data exposure over public internet routes (unlike Option 2)</li><li><strong>Cost-effective VPN</strong>: The UDM Pro natively supports WireGuard, enabling a self-managed VPN solution that avoids expensive managed VPN services (~$180-300/year vs ~$540-900/year)</li></ol><h3 id=consequences>Consequences</h3><ul><li>Good, because all traffic is encrypted through WireGuard VPN tunnel</li><li>Good, because boot server is not exposed to public internet (no public attack surface)</li><li>Good, because trust model is simple - subnet validation similar to local option</li><li>Good, because centralized cloud management reduces local maintenance burden</li><li>Good, because boot server remains available even if home lab storage fails</li><li>Good, because UDM Pro&rsquo;s native WireGuard support keeps costs at ~$180-300/year</li><li>Bad, because boot process depends on both internet connectivity and VPN availability</li><li>Bad, because VPN adds latency to boot file transfers</li><li>Bad, because VPN gateway becomes an additional failure point</li><li>Bad, because higher ongoing cost compared to local-only option (~$180-300/year vs ~$10/year)</li></ul><h3 id=confirmation>Confirmation</h3><p>The implementation will be confirmed by:</p><ul><li>Successfully network booting a test server using the chosen architecture</li><li>Validating the trust model prevents unauthorized boot requests</li><li>Measuring actual costs against estimates</li></ul><h2 id=pros-and-cons-of-the-options>Pros and Cons of the Options</h2><h3 id=option-1-tftphttp-server-locally-on-home-lab-network>Option 1: TFTP/HTTP server locally on home lab network</h3><p>Run the boot server on local infrastructure (e.g., Raspberry Pi, dedicated VM, or container) within the home lab network.</p><h4 id=boot-flow-sequence>Boot Flow Sequence</h4><pre class=mermaid>sequenceDiagram
    participant Server as Home Lab Server
    participant DHCP as Local DHCP Server
    participant Boot as Local TFTP/HTTP Server

    Server-&gt;&gt;DHCP: PXE Boot Request (DHCP Discover)
    DHCP-&gt;&gt;Server: DHCP Offer with Boot Server IP
    Server-&gt;&gt;Boot: TFTP Request for Boot File
    Boot-&gt;&gt;Boot: Verify MAC/IP against allowlist
    Boot-&gt;&gt;Server: Send iPXE/Boot Loader
    Server-&gt;&gt;Boot: HTTP Request for Kernel/Initrd
    Boot-&gt;&gt;Server: Send Boot Files
    Server-&gt;&gt;Server: Boot into OS</pre><h4 id=trust-model>Trust Model</h4><ul><li><strong>MAC Address Allowlist</strong>: Maintain a list of known server MAC addresses</li><li><strong>Network Isolation</strong>: Boot server only accessible from home lab VLAN</li><li><strong>No external exposure</strong>: Traffic never leaves local network</li><li><strong>Physical security</strong>: Relies on physical access control to home lab</li></ul><h4 id=cost-estimate>Cost Estimate</h4><ul><li><strong>Hardware</strong>: ~$50-100 one-time (Raspberry Pi or repurposed hardware)</li><li><strong>Power</strong>: ~$5-10/year (low power consumption)</li><li><strong>Total</strong>: ~$55-110 initial + ~$10/year ongoing</li></ul><h4 id=pros-and-cons>Pros and Cons</h4><ul><li>Good, because no dependency on internet connectivity for booting</li><li>Good, because lowest latency for boot file transfers</li><li>Good, because all data stays within local network (maximum privacy)</li><li>Good, because lowest ongoing cost</li><li>Good, because simple trust model based on network isolation</li><li>Neutral, because requires dedicated local hardware or resources</li><li>Bad, because single point of failure if boot server goes down</li><li>Bad, because requires local maintenance and updates</li></ul><h3 id=option-2-tftphttp-server-on-public-cloud-without-vpn>Option 2: TFTP/HTTP server on public cloud (without VPN)</h3><p>Host the boot server on a cloud provider (AWS, GCP, Azure) and expose it directly to the internet.</p><h4 id=boot-flow-sequence-1>Boot Flow Sequence</h4><pre class=mermaid>sequenceDiagram
    participant Server as Home Lab Server
    participant DHCP as Local DHCP Server
    participant Router as Home Router/NAT
    participant Internet as Internet
    participant Boot as Cloud TFTP/HTTP Server

    Server-&gt;&gt;DHCP: PXE Boot Request (DHCP Discover)
    DHCP-&gt;&gt;Server: DHCP Offer with Cloud Boot Server IP
    Server-&gt;&gt;Router: TFTP Request
    Router-&gt;&gt;Internet: NAT Translation
    Internet-&gt;&gt;Boot: TFTP Request from Home IP
    Boot-&gt;&gt;Boot: Verify source IP &#43; token/certificate
    Boot-&gt;&gt;Internet: Send iPXE/Boot Loader
    Internet-&gt;&gt;Router: Response
    Router-&gt;&gt;Server: Boot Loader
    Server-&gt;&gt;Router: HTTP Request for Kernel/Initrd
    Router-&gt;&gt;Internet: NAT Translation
    Internet-&gt;&gt;Boot: HTTP Request with auth headers
    Boot-&gt;&gt;Boot: Validate request authenticity
    Boot-&gt;&gt;Internet: Send Boot Files
    Internet-&gt;&gt;Router: Response
    Router-&gt;&gt;Server: Boot Files
    Server-&gt;&gt;Server: Boot into OS</pre><h4 id=trust-model-1>Trust Model</h4><ul><li><strong>Source IP Validation</strong>: Restrict to home lab&rsquo;s public IP (dynamic IP is problematic)</li><li><strong>Certificate/Token Authentication</strong>: Embed certificates in initial bootloader</li><li><strong>TLS for HTTP</strong>: All HTTP traffic encrypted</li><li><strong>Challenge-Response</strong>: Boot server can challenge requesting server</li><li><strong>Risk</strong>: TFTP typically unencrypted, vulnerable to interception</li></ul><h4 id=cost-estimate-1>Cost Estimate</h4><ul><li><strong>Cloud VM (t3.micro or equivalent)</strong>: ~$10-15/month</li><li><strong>Data Transfer</strong>: ~$1-5/month (boot files are typically small)</li><li><strong>Static IP</strong>: ~$3-5/month</li><li><strong>Total</strong>: ~$170-300/year</li></ul><h4 id=pros-and-cons-1>Pros and Cons</h4><ul><li>Good, because boot server remains available even if home lab has issues</li><li>Good, because centralized management in cloud console</li><li>Good, because easy to scale or replicate</li><li>Neutral, because requires internet connectivity for every boot</li><li>Bad, because significantly higher ongoing cost</li><li>Bad, because TFTP protocol is inherently insecure over public internet</li><li>Bad, because complex trust model required (IP validation, certificates)</li><li>Bad, because boot process depends on internet availability</li><li>Bad, because higher latency for boot file transfers</li><li>Bad, because public exposure increases attack surface</li></ul><h3 id=option-3-tftphttp-server-on-public-cloud-with-vpn>Option 3: TFTP/HTTP server on public cloud (with VPN)</h3><p>Host the boot server in the cloud but connect the home lab to the cloud via a site-to-site VPN tunnel.</p><h4 id=boot-flow-sequence-2>Boot Flow Sequence</h4><pre class=mermaid>sequenceDiagram
    participant Server as Home Lab Server
    participant DHCP as Local DHCP Server
    participant VPN as VPN Gateway (Home)
    participant CloudVPN as VPN Gateway (Cloud)
    participant Boot as Cloud TFTP/HTTP Server

    Note over VPN,CloudVPN: Site-to-Site VPN Tunnel Established

    Server-&gt;&gt;DHCP: PXE Boot Request (DHCP Discover)
    DHCP-&gt;&gt;Server: DHCP Offer with Boot Server Private IP
    Server-&gt;&gt;VPN: TFTP Request to Private IP
    VPN-&gt;&gt;CloudVPN: Encrypted VPN Tunnel
    CloudVPN-&gt;&gt;Boot: TFTP Request (appears local)
    Boot-&gt;&gt;Boot: Verify source IP from home lab subnet
    Boot-&gt;&gt;CloudVPN: Send iPXE/Boot Loader
    CloudVPN-&gt;&gt;VPN: Encrypted Response
    VPN-&gt;&gt;Server: Boot Loader
    Server-&gt;&gt;VPN: HTTP Request for Kernel/Initrd
    VPN-&gt;&gt;CloudVPN: Encrypted VPN Tunnel
    CloudVPN-&gt;&gt;Boot: HTTP Request
    Boot-&gt;&gt;Boot: Validate subnet membership
    Boot-&gt;&gt;CloudVPN: Send Boot Files
    CloudVPN-&gt;&gt;VPN: Encrypted Response
    VPN-&gt;&gt;Server: Boot Files
    Server-&gt;&gt;Server: Boot into OS</pre><h4 id=trust-model-2>Trust Model</h4><ul><li><strong>VPN Tunnel Encryption</strong>: All traffic encrypted end-to-end</li><li><strong>Private IP Addressing</strong>: Boot server only accessible via VPN</li><li><strong>Subnet Validation</strong>: Verify requests come from trusted home lab subnet</li><li><strong>VPN Authentication</strong>: Strong auth at tunnel level (certificates, pre-shared keys)</li><li><strong>No public exposure</strong>: Boot server has no public IP</li></ul><h4 id=cost-estimate-2>Cost Estimate</h4><ul><li><strong>Cloud VM (t3.micro or equivalent)</strong>: ~$10-15/month</li><li><strong>Data Transfer (VPN)</strong>: ~$5-10/month</li><li><strong>VPN Gateway Service (if using managed)</strong>: ~$30-50/month OR</li><li><strong>Self-managed VPN (WireGuard/OpenVPN)</strong>: ~$0 additional</li><li><strong>Total (self-managed VPN)</strong>: ~$180-300/year</li><li><strong>Total (managed VPN)</strong>: ~$540-900/year</li></ul><h4 id=pros-and-cons-2>Pros and Cons</h4><ul><li>Good, because all traffic encrypted through VPN tunnel</li><li>Good, because boot server not exposed to public internet</li><li>Good, because trust model similar to local option (subnet validation)</li><li>Good, because centralized cloud management benefits</li><li>Good, because boot server available if home lab storage fails</li><li>Neutral, because moderate complexity (VPN setup and maintenance)</li><li>Bad, because higher cost than local option</li><li>Bad, because boot process still depends on internet + VPN availability</li><li>Bad, because VPN adds latency to boot process</li><li>Bad, because VPN gateway becomes additional failure point</li><li>Bad, because most expensive option if using managed VPN service</li></ul><h2 id=more-information>More Information</h2><h3 id=related-resources>Related Resources</h3><ul><li><a href=https://en.wikipedia.org/wiki/Preboot_Execution_Environment>PXE Boot Specification</a></li><li><a href=https://ipxe.org/>iPXE - Open Source Boot Firmware</a></li><li><a href=https://tools.ietf.org/html/rfc1350>TFTP Protocol (RFC 1350)</a></li></ul><h3 id=key-questions-for-decision>Key Questions for Decision</h3><ol><li>How critical is boot availability during internet outages?</li><li>Is the home lab public IP static or dynamic?</li><li>What is the acceptable boot time latency?</li><li>How many servers need to be supported?</li><li>Is there existing VPN infrastructure?</li></ol><h3 id=related-issues>Related Issues</h3><ul><li><a href=https://github.com/Zaba505/infra/issues/595>Issue #595</a> - story(docs): create adr for network boot architecture</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-853f5ec590d44937c1ebd528cd046965>3 - [0003] Cloud Provider Selection for Network Boot Infrastructure</h1><div class=lead>Evaluate Google Cloud Platform vs Amazon Web Services for hosting network boot server infrastructure as required by ADR-0002.</div><h2 id=context-and-problem-statement>Context and Problem Statement</h2><p><a href=./0002-network-boot-architecture/>ADR-0002</a> established that network boot infrastructure will be hosted on a cloud provider and accessed via VPN (specifically WireGuard from the UDM Pro). The decision to use cloud hosting provides resilience against local hardware failures while maintaining security through encrypted VPN tunnels.</p><p>The question now is: <strong>Which cloud provider should host the network boot infrastructure?</strong></p><p>This decision will affect:</p><ul><li><strong>Cost</strong>: Ongoing monthly/annual infrastructure costs</li><li><strong>Protocol Support</strong>: Ability to serve TFTP, HTTP, and HTTPS boot files</li><li><strong>VPN Integration</strong>: Ease of WireGuard deployment and management</li><li><strong>Operational Complexity</strong>: Management overhead and maintenance burden</li><li><strong>Performance</strong>: Boot file transfer latency and throughput</li><li><strong>Vendor Lock-in</strong>: Future flexibility to migrate or multi-cloud</li></ul><h2 id=decision-drivers>Decision Drivers</h2><ul><li><strong>Cost Efficiency</strong>: Minimize ongoing infrastructure costs for home lab scale</li><li><strong>Protocol Support</strong>: Must support TFTP (UDP/69), HTTP (TCP/80), and HTTPS (TCP/443) for network boot workflows</li><li><strong>WireGuard Compatibility</strong>: Must support self-managed WireGuard VPN with reasonable effort</li><li><strong>UDM Pro Integration</strong>: Should work seamlessly with UniFi Dream Machine Pro&rsquo;s native WireGuard client</li><li><strong>Simplicity</strong>: Minimize operational complexity for a single-person home lab</li><li><strong>Existing Expertise</strong>: Leverage existing team knowledge and infrastructure</li><li><strong>Performance</strong>: Sufficient throughput and low latency for boot file transfers (50-200MB per boot)</li></ul><h2 id=considered-options>Considered Options</h2><ul><li><strong>Option 1</strong>: Google Cloud Platform (GCP)</li><li><strong>Option 2</strong>: Amazon Web Services (AWS)</li></ul><h2 id=decision-outcome>Decision Outcome</h2><p>Chosen option: &ldquo;<strong>Option 1: Google Cloud Platform (GCP)</strong>&rdquo;, because:</p><ol><li><strong>Existing Infrastructure</strong>: The home lab already uses GCP extensively (Cloud Run services, load balancers, mTLS infrastructure per existing codebase), reducing operational overhead and leveraging existing expertise</li><li><strong>Comparable Costs</strong>: Both providers offer similar costs for the required infrastructure (~$6-12/month for compute + VPN), with GCP&rsquo;s e2-micro being sufficient</li><li><strong>Equivalent Protocol Support</strong>: Both support TFTP/HTTP/HTTPS via direct VM access (load balancers unnecessary for single boot server), meeting all protocol requirements</li><li><strong>WireGuard Compatibility</strong>: Both require self-managed WireGuard deployment (neither has native WireGuard support), with nearly identical implementation complexity</li><li><strong>Unified Management</strong>: Consolidating all cloud infrastructure on GCP simplifies monitoring, billing, IAM, and operational workflows</li></ol><p>While AWS would be a viable alternative (especially with t4g.micro ARM instances offering slightly better price/performance), the <strong>existing GCP investment</strong> makes it the pragmatic choice to avoid multi-cloud complexity.</p><h3 id=consequences>Consequences</h3><ul><li>Good, because consolidates all cloud infrastructure on a single provider (reduced operational complexity)</li><li>Good, because leverages existing GCP expertise and IAM configurations</li><li>Good, because unified Cloud Monitoring/Logging across all services</li><li>Good, because single cloud bill simplifies cost tracking</li><li>Good, because existing Terraform modules and patterns can be reused</li><li>Good, because GCP&rsquo;s e2-micro instances (~$6.50/month) are cost-effective for the workload</li><li>Good, because self-managed WireGuard provides flexibility and low cost (~$10/month total)</li><li>Neutral, because both providers have comparable protocol support (TFTP/HTTP/HTTPS via VM)</li><li>Neutral, because both require self-managed WireGuard (no native support)</li><li>Bad, because creates vendor lock-in to GCP (migration would require relearning and reconfiguration)</li><li>Bad, because foregoes AWS&rsquo;s slightly cheaper t4g.micro ARM instances (~$6/month vs GCP&rsquo;s ~$6.50/month)</li><li>Bad, because multi-cloud strategy could provide redundancy (accepted trade-off for simplicity)</li></ul><h3 id=confirmation>Confirmation</h3><p>The implementation will be confirmed by:</p><ul><li>Successfully deploying WireGuard VPN gateway on GCP Compute Engine</li><li>Establishing site-to-site VPN tunnel between UDM Pro and GCP</li><li>Network booting a test server via VPN using TFTP and HTTP protocols</li><li>Measuring actual costs against estimates (~$10-15/month)</li><li>Validating boot performance (transfer time &lt; 30 seconds for typical boot)</li></ul><h2 id=pros-and-cons-of-the-options>Pros and Cons of the Options</h2><h3 id=option-1-google-cloud-platform-gcp>Option 1: Google Cloud Platform (GCP)</h3><p>Host network boot infrastructure on Google Cloud Platform.</p><h4 id=architecture-overview>Architecture Overview</h4><pre class=mermaid>graph TB
    subgraph &#34;Home Lab Network&#34;
        A[Home Lab Servers]
        B[UDM Pro - WireGuard Client]
    end
    
    subgraph &#34;GCP VPC&#34;
        C[WireGuard Gateway VM&lt;br/&gt;e2-micro]
        D[Boot Server VM&lt;br/&gt;e2-micro]
        C --&gt;|VPC Routing| D
    end
    
    A --&gt;|PXE Boot Request| B
    B --&gt;|WireGuard Tunnel| C
    C --&gt;|TFTP/HTTP/HTTPS| D
    D --&gt;|Boot Files| C
    C --&gt;|Encrypted Response| B
    B --&gt;|Boot Files| A</pre><h4 id=implementation-details>Implementation Details</h4><p><strong>Compute</strong>:</p><ul><li><strong>WireGuard Gateway</strong>: e2-micro VM (~$6.50/month) running Ubuntu 22.04<ul><li>Self-managed WireGuard server</li><li>IP forwarding enabled</li><li>Static external IP (~$3.50/month if VM ever stops)</li></ul></li><li><strong>Boot Server</strong>: e2-micro VM (same or consolidated with gateway)<ul><li>TFTP server (<code>tftpd-hpa</code>)</li><li>HTTP server (nginx or simple Python server)</li><li>Optional HTTPS with self-signed cert or Let&rsquo;s Encrypt</li></ul></li></ul><p><strong>Networking</strong>:</p><ul><li><strong>VPC</strong>: Default VPC or custom VPC with private subnets</li><li><strong>Firewall Rules</strong>:<ul><li>Allow UDP/51820 from home lab public IP (WireGuard)</li><li>Allow UDP/69, TCP/80, TCP/443 from VPN subnet (boot protocols)</li></ul></li><li><strong>Routes</strong>: Custom route to direct home lab subnet through WireGuard gateway</li><li><strong>Cloud VPN</strong>: Not used (self-managed WireGuard instead to save ~$65/month)</li></ul><p><strong>WireGuard Setup</strong>:</p><ul><li>Install WireGuard on Compute Engine VM</li><li>Configure <code>wg0</code> interface with PostUp/PostDown iptables rules</li><li>Store private key in Secret Manager</li><li>UDM Pro connects as WireGuard peer</li></ul><p><strong>Cost Breakdown</strong> (US regions):</p><table><thead><tr><th>Component</th><th>Monthly Cost</th></tr></thead><tbody><tr><td>e2-micro VM (WireGuard + Boot)</td><td>~$6.50</td></tr><tr><td>Static External IP (if attached)</td><td>~$3.50</td></tr><tr><td>Egress (10 boots × 150MB)</td><td>~$0.18</td></tr><tr><td><strong>Total</strong></td><td><strong>~$10.18</strong></td></tr><tr><td><strong>Annual</strong></td><td><strong>~$122</strong></td></tr></tbody></table><h4 id=pros-and-cons>Pros and Cons</h4><ul><li>Good, because existing home lab infrastructure already uses GCP extensively</li><li>Good, because consolidates all cloud resources on single provider (unified billing, IAM, monitoring)</li><li>Good, because leverages existing GCP expertise and Terraform modules</li><li>Good, because Cloud Monitoring/Logging already configured for other services</li><li>Good, because Secret Manager integration for WireGuard key storage</li><li>Good, because e2-micro instance size is sufficient for network boot workload</li><li>Good, because low cost (~$10/month for self-managed WireGuard)</li><li>Good, because VPC networking is familiar and well-documented</li><li>Neutral, because requires self-managed WireGuard (no native support, same as AWS)</li><li>Neutral, because TFTP/HTTP/HTTPS served directly from VM (no special GCP features needed)</li><li>Bad, because slightly more expensive than AWS t4g.micro (~$6.50/month vs ~$6/month)</li><li>Bad, because creates vendor lock-in to GCP ecosystem</li><li>Bad, because Cloud VPN (managed IPsec) is expensive (~$73/month), so must use self-managed WireGuard</li></ul><h3 id=option-2-amazon-web-services-aws>Option 2: Amazon Web Services (AWS)</h3><p>Host network boot infrastructure on Amazon Web Services.</p><h4 id=architecture-overview-1>Architecture Overview</h4><pre class=mermaid>graph TB
    subgraph &#34;Home Lab Network&#34;
        A[Home Lab Servers]
        B[UDM Pro - WireGuard Client]
    end
    
    subgraph &#34;AWS VPC&#34;
        C[WireGuard Gateway EC2&lt;br/&gt;t4g.micro]
        D[Boot Server EC2&lt;br/&gt;t4g.micro]
        C --&gt;|VPC Routing| D
    end
    
    A --&gt;|PXE Boot Request| B
    B --&gt;|WireGuard Tunnel| C
    C --&gt;|TFTP/HTTP/HTTPS| D
    D --&gt;|Boot Files| C
    C --&gt;|Encrypted Response| B
    B --&gt;|Boot Files| A</pre><h4 id=implementation-details-1>Implementation Details</h4><p><strong>Compute</strong>:</p><ul><li><strong>WireGuard Gateway</strong>: t4g.micro EC2 (~$6/month, ARM-based Graviton)<ul><li>Self-managed WireGuard server</li><li>Source/Dest check disabled for IP forwarding</li><li>Elastic IP (free when attached to running instance)</li></ul></li><li><strong>Boot Server</strong>: t4g.micro EC2 (same or consolidated with gateway)<ul><li>TFTP server (<code>tftpd-hpa</code>)</li><li>HTTP server (nginx)</li><li>Optional HTTPS with Let&rsquo;s Encrypt or self-signed cert</li></ul></li></ul><p><strong>Networking</strong>:</p><ul><li><strong>VPC</strong>: Default VPC or custom VPC with private subnets</li><li><strong>Security Groups</strong>:<ul><li>WireGuard SG: Allow UDP/51820 from home lab public IP</li><li>Boot Server SG: Allow UDP/69, TCP/80, TCP/443 from WireGuard SG</li></ul></li><li><strong>Route Table</strong>: Add route for home lab subnet via WireGuard instance</li><li><strong>Site-to-Site VPN</strong>: Not used (self-managed WireGuard saves ~$30/month)</li></ul><p><strong>WireGuard Setup</strong>:</p><ul><li>Install WireGuard on Ubuntu 22.04 or Amazon Linux 2023 EC2</li><li>Configure <code>wg0</code> with iptables MASQUERADE</li><li>Store private key in Secrets Manager</li><li>UDM Pro connects as WireGuard peer</li></ul><p><strong>Cost Breakdown</strong> (US East):</p><table><thead><tr><th>Component</th><th>Monthly Cost</th></tr></thead><tbody><tr><td>t4g.micro EC2 (WireGuard + Boot)</td><td>~$6.00</td></tr><tr><td>Elastic IP (attached)</td><td>$0.00</td></tr><tr><td>Egress (10 boots × 150MB)</td><td>~$0.09</td></tr><tr><td><strong>Total (On-Demand)</strong></td><td><strong>~$6.09</strong></td></tr><tr><td><strong>Total (1-yr Reserved)</strong></td><td><strong>~$3.59</strong></td></tr><tr><td><strong>Annual (On-Demand)</strong></td><td><strong>~$73</strong></td></tr><tr><td><strong>Annual (Reserved)</strong></td><td><strong>~$43</strong></td></tr></tbody></table><h4 id=pros-and-cons-1>Pros and Cons</h4><ul><li>Good, because t4g.micro ARM instances offer best price/performance (~$6/month on-demand)</li><li>Good, because Reserved Instances provide significant savings (~40% with 1-year commitment)</li><li>Good, because Elastic IP is free when attached to running instance</li><li>Good, because AWS has extensive documentation and community support</li><li>Good, because potential for future multi-cloud strategy</li><li>Good, because ACM provides free SSL certificates (if public domain used)</li><li>Good, because Secrets Manager for WireGuard key storage</li><li>Good, because low cost (~$6/month on-demand, ~$3.50/month with RI)</li><li>Neutral, because requires self-managed WireGuard (no native support, same as GCP)</li><li>Neutral, because TFTP/HTTP/HTTPS served directly from EC2 (no special AWS features)</li><li>Bad, because introduces multi-cloud complexity (separate billing, IAM, monitoring)</li><li>Bad, because no existing AWS infrastructure in home lab (new learning curve)</li><li>Bad, because requires separate monitoring/logging setup (CloudWatch vs Cloud Monitoring)</li><li>Bad, because separate Terraform state and modules needed</li><li>Bad, because Site-to-Site VPN is expensive (~$36/month), so must use self-managed WireGuard</li></ul><h2 id=more-information>More Information</h2><h3 id=detailed-analysis>Detailed Analysis</h3><p>For in-depth analysis of each provider&rsquo;s capabilities:</p><ul><li><p><a href=../analysis/google-cloud/><strong>Google Cloud Platform Analysis</strong></a></p><ul><li><a href=../analysis/google-cloud/network-boot/>Network Boot Protocol Support (TFTP, HTTP, HTTPS)</a></li><li><a href=../analysis/google-cloud/wireguard/>WireGuard VPN Support and Deployment</a></li></ul></li><li><p><a href=../analysis/aws/><strong>Amazon Web Services Analysis</strong></a></p><ul><li><a href=../analysis/aws/network-boot/>Network Boot Protocol Support (TFTP, HTTP, HTTPS)</a></li><li><a href=../analysis/aws/wireguard/>WireGuard VPN Support and Deployment</a></li></ul></li></ul><h3 id=key-findings-summary>Key Findings Summary</h3><p>Both providers offer:</p><ul><li>✅ <strong>TFTP Support</strong>: Via direct VM/EC2 access (load balancers don&rsquo;t support TFTP)</li><li>✅ <strong>HTTP/HTTPS Support</strong>: Full support via direct VM/EC2 or load balancers</li><li>✅ <strong>WireGuard Compatibility</strong>: Self-managed deployment on VM/EC2 (neither has native support)</li><li>✅ <strong>UDM Pro Integration</strong>: Native WireGuard client works with both</li><li>✅ <strong>Low Cost</strong>: $6-12/month for compute + VPN infrastructure</li><li>✅ <strong>Sufficient Performance</strong>: 100+ Mbps throughput on smallest instances</li></ul><p>Key differences:</p><ul><li><strong>GCP</strong>: Slightly higher cost (~$10/month), but consolidates with existing infrastructure</li><li><strong>AWS</strong>: Slightly lower cost (~$6/month on-demand, ~$3.50/month Reserved), but introduces multi-cloud complexity</li></ul><h3 id=cost-comparison-table>Cost Comparison Table</h3><table><thead><tr><th>Component</th><th>GCP (e2-micro)</th><th>AWS (t4g.micro On-Demand)</th><th>AWS (t4g.micro 1-yr RI)</th></tr></thead><tbody><tr><td>Compute</td><td>$6.50/month</td><td>$6.00/month</td><td>$3.50/month</td></tr><tr><td>Static IP</td><td>$3.50/month</td><td>$0.00 (Elastic IP free when attached)</td><td>$0.00</td></tr><tr><td>Egress (1.5GB)</td><td>$0.18/month</td><td>$0.09/month</td><td>$0.09/month</td></tr><tr><td><strong>Monthly</strong></td><td><strong>$10.18</strong></td><td><strong>$6.09</strong></td><td><strong>$3.59</strong></td></tr><tr><td><strong>Annual</strong></td><td><strong>$122</strong></td><td><strong>$73</strong></td><td><strong>$43</strong></td></tr></tbody></table><p><strong>Savings Analysis</strong>: AWS is ~$49-79/year cheaper, but introduces operational complexity.</p><h3 id=protocol-support-comparison>Protocol Support Comparison</h3><table><thead><tr><th>Protocol</th><th>GCP Support</th><th>AWS Support</th><th>Implementation</th></tr></thead><tbody><tr><td>TFTP (UDP/69)</td><td>⚠️ Via VM</td><td>⚠️ Via EC2</td><td>Direct VM/EC2 access (no LB support)</td></tr><tr><td>HTTP (TCP/80)</td><td>✅ Full</td><td>✅ Full</td><td>Direct VM/EC2 or Load Balancer</td></tr><tr><td>HTTPS (TCP/443)</td><td>✅ Full</td><td>✅ Full</td><td>Direct VM/EC2 or Load Balancer + cert</td></tr><tr><td>WireGuard</td><td>⚠️ Self-managed</td><td>⚠️ Self-managed</td><td>Install on VM/EC2</td></tr></tbody></table><h3 id=wireguard-deployment-comparison>WireGuard Deployment Comparison</h3><table><thead><tr><th>Aspect</th><th>GCP</th><th>AWS</th></tr></thead><tbody><tr><td><strong>Native Support</strong></td><td>❌ No (IPsec Cloud VPN only)</td><td>❌ No (IPsec Site-to-Site VPN only)</td></tr><tr><td><strong>Self-Managed</strong></td><td>✅ Compute Engine</td><td>✅ EC2</td></tr><tr><td><strong>Setup Complexity</strong></td><td>Similar (install, configure, firewall)</td><td>Similar (install, configure, SG)</td></tr><tr><td><strong>IP Forwarding</strong></td><td>Enable on VM</td><td>Disable Source/Dest check</td></tr><tr><td><strong>Firewall</strong></td><td>VPC Firewall rules</td><td>Security Groups</td></tr><tr><td><strong>Key Storage</strong></td><td>Secret Manager</td><td>Secrets Manager</td></tr><tr><td><strong>Cost</strong></td><td>~$10/month total</td><td>~$6/month total</td></tr></tbody></table><h3 id=trade-offs-analysis>Trade-offs Analysis</h3><p><strong>Choosing GCP</strong>:</p><ul><li><strong>Wins</strong>: Operational simplicity, unified infrastructure, existing expertise</li><li><strong>Loses</strong>: ~$50-80/year higher cost, vendor lock-in</li></ul><p><strong>Choosing AWS</strong>:</p><ul><li><strong>Wins</strong>: Lower cost, Reserved Instance savings, multi-cloud optionality</li><li><strong>Loses</strong>: Multi-cloud complexity, separate monitoring/billing, new tooling</li></ul><p>For a home lab prioritizing <strong>simplicity over cost optimization</strong>, GCP&rsquo;s consolidation benefits outweigh the modest cost difference.</p><h3 id=related-adrs>Related ADRs</h3><ul><li><a href=./0002-network-boot-architecture/>ADR-0002: Network Boot Architecture</a> - Established requirement for cloud-hosted boot server with VPN</li><li><a href=./0001-use-madr-for-architecture-decision-records/>ADR-0001: Use MADR for Architecture Decision Records</a> - MADR format used for this ADR</li></ul><h3 id=future-considerations>Future Considerations</h3><ol><li><strong>Cost Reevaluation</strong>: If annual costs become significant, reconsider AWS Reserved Instances</li><li><strong>Multi-Cloud</strong>: If multi-cloud strategy emerges, migrate boot server to AWS</li><li><strong>Managed WireGuard</strong>: If GCP or AWS adds native WireGuard support, reevaluate managed option</li><li><strong>High Availability</strong>: If HA required, evaluate multi-region deployment costs on both providers</li></ol><h3 id=related-issues>Related Issues</h3><ul><li><a href=https://github.com/Zaba505/infra/issues/597>Issue #597</a> - story(docs): create adr for cloud provider selection</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-dab026fd06f98a03a459f97073d21662>4 - [0004] Server Operating System Selection</h1><div class=lead>Evaluate operating systems for homelab server infrastructure with focus on Kubernetes cluster setup and maintenance.</div><h2 id=context-and-problem-statement>Context and Problem Statement</h2><p>The homelab infrastructure requires a server operating system to run Kubernetes clusters for container workloads. The choice of operating system significantly impacts ease of cluster initialization, ongoing maintenance burden, security posture, and operational complexity.</p><p>The question is: <strong>Which operating system should be used for homelab Kubernetes servers?</strong></p><p>This decision will affect:</p><ul><li><strong>Cluster Initialization</strong>: Complexity and time required to bootstrap Kubernetes</li><li><strong>Maintenance Burden</strong>: Frequency and complexity of OS updates, Kubernetes upgrades, and patching</li><li><strong>Security Posture</strong>: Attack surface, built-in security features, and hardening requirements</li><li><strong>Resource Efficiency</strong>: RAM, CPU, and disk overhead</li><li><strong>Operational Complexity</strong>: Day-to-day management, troubleshooting, and debugging</li><li><strong>Learning Curve</strong>: Time required for team to become proficient</li></ul><h2 id=decision-drivers>Decision Drivers</h2><ul><li><strong>Ease of Kubernetes Setup</strong>: Minimize steps and complexity for cluster initialization</li><li><strong>Maintenance Simplicity</strong>: Reduce ongoing operational burden for updates and upgrades</li><li><strong>Security-First Design</strong>: Minimal attack surface and strong security defaults</li><li><strong>Resource Efficiency</strong>: Low RAM/CPU/disk overhead for cost-effective homelab</li><li><strong>Learning Curve</strong>: Reasonable adoption time for single-person homelab</li><li><strong>Community Support</strong>: Strong documentation and active community</li><li><strong>Immutability</strong>: Prefer declarative, version-controlled configuration (GitOps-friendly)</li><li><strong>Purpose-Built</strong>: OS optimized specifically for Kubernetes vs general-purpose</li></ul><h2 id=considered-options>Considered Options</h2><ul><li><strong>Option 1</strong>: Ubuntu Server with k3s</li><li><strong>Option 2</strong>: Fedora Server with kubeadm</li><li><strong>Option 3</strong>: Talos Linux (purpose-built Kubernetes OS)</li><li><strong>Option 4</strong>: Harvester HCI (hyperconverged platform)</li></ul><h2 id=decision-outcome>Decision Outcome</h2><p>Chosen option: &ldquo;<strong>Option 3: Talos Linux</strong>&rdquo;, because:</p><ol><li><strong>Minimal Attack Surface</strong>: No SSH, shell, or package manager eliminates entire classes of vulnerabilities, providing the strongest security posture</li><li><strong>Built-in Kubernetes</strong>: No separate installation or configuration complexity - Kubernetes is included and optimized</li><li><strong>Declarative Configuration</strong>: API-driven, immutable infrastructure aligns with GitOps principles and prevents configuration drift</li><li><strong>Lowest Resource Overhead</strong>: ~768MB RAM vs 1-2GB+ for traditional distros, maximizing homelab hardware efficiency</li><li><strong>Simplified Maintenance</strong>: Declarative upgrades (<code>talosctl upgrade</code>) for both OS and Kubernetes reduce operational burden</li><li><strong>Security by Default</strong>: Immutable filesystem, no shell, KSPP compliance - secure without manual hardening</li></ol><p>While the learning curve is steeper than traditional Linux distributions, the benefits of purpose-built Kubernetes infrastructure, minimal maintenance, and superior security outweigh the initial learning investment for a dedicated Kubernetes homelab.</p><h3 id=consequences>Consequences</h3><ul><li>Good, because minimal attack surface (no SSH/shell) provides strongest security posture</li><li>Good, because declarative configuration enables GitOps workflows and prevents drift</li><li>Good, because lowest resource overhead (~768MB RAM) maximizes homelab efficiency</li><li>Good, because built-in Kubernetes eliminates installation complexity</li><li>Good, because immutable infrastructure prevents configuration drift</li><li>Good, because simplified upgrades (single command for OS + K8s) reduce maintenance burden</li><li>Good, because smallest disk footprint (~500MB) vs 10GB+ for traditional distros</li><li>Good, because secure by default (no manual hardening required)</li><li>Good, because purpose-built design optimized specifically for Kubernetes</li><li>Good, because API-driven management (talosctl) enables automation</li><li>Neutral, because steeper learning curve (paradigm shift from shell-based management)</li><li>Neutral, because smaller community than Ubuntu/Fedora (but active and helpful)</li><li>Bad, because limited to Kubernetes workloads only (not general-purpose)</li><li>Bad, because no shell access requires different troubleshooting approach</li><li>Bad, because newer platform (less mature than Ubuntu/Fedora)</li><li>Bad, because no escape hatch for manual intervention when needed</li></ul><h3 id=confirmation>Confirmation</h3><p>The implementation will be confirmed by:</p><ul><li>Successfully bootstrapping a Talos cluster using talosctl</li><li>Deploying test workloads and validating functionality</li><li>Performing declarative OS and Kubernetes upgrades</li><li>Measuring actual resource usage (RAM &lt; 1GB per node)</li><li>Validating security posture (no SSH/shell, immutable filesystem)</li><li>Testing GitOps workflow (machine configs in version control)</li></ul><h2 id=pros-and-cons-of-the-options>Pros and Cons of the Options</h2><h3 id=option-1-ubuntu-server-with-k3s>Option 1: Ubuntu Server with k3s</h3><p>Host Kubernetes using Ubuntu Server 24.04 LTS with k3s lightweight Kubernetes distribution.</p><h4 id=architecture-overview>Architecture Overview</h4><pre class=mermaid>sequenceDiagram
    participant Admin
    participant Server as Ubuntu Server
    participant K3s as k3s Components
    
    Admin-&gt;&gt;Server: Install Ubuntu 24.04 LTS
    Server-&gt;&gt;Server: Configure network (static IP)
    Admin-&gt;&gt;Server: Update system
    Admin-&gt;&gt;Server: curl -sfL https://get.k3s.io | sh -
    Server-&gt;&gt;K3s: Download k3s binary
    K3s-&gt;&gt;Server: Configure containerd
    K3s-&gt;&gt;Server: Start k3s service
    K3s-&gt;&gt;Server: Initialize etcd (embedded)
    K3s-&gt;&gt;Server: Start API server
    K3s-&gt;&gt;Server: Deploy built-in CNI (Flannel)
    K3s--&gt;&gt;Admin: Control plane ready
    Admin-&gt;&gt;Server: Retrieve node token
    Admin-&gt;&gt;Server: Install k3s agent on workers
    K3s-&gt;&gt;Server: Join workers to cluster
    K3s--&gt;&gt;Admin: Cluster ready (5-10 minutes)</pre><h4 id=implementation-details>Implementation Details</h4><p><strong>Installation</strong>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Single-command k3s install</span>
</span></span><span style=display:flex><span>curl -sfL https://get.k3s.io <span style=color:#000;font-weight:700>|</span> sh -
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Get token for workers</span>
</span></span><span style=display:flex><span>sudo cat /var/lib/rancher/k3s/server/node-token
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Install on workers</span>
</span></span><span style=display:flex><span>curl -sfL https://get.k3s.io <span style=color:#000;font-weight:700>|</span> <span style=color:#000>K3S_URL</span><span style=color:#ce5c00;font-weight:700>=</span>https://control-plane:6443 <span style=color:#000>K3S_TOKEN</span><span style=color:#ce5c00;font-weight:700>=</span>&lt;token&gt; sh -
</span></span></code></pre></div><p><strong>Resource Requirements</strong>:</p><ul><li><strong>RAM</strong>: 1GB total (512MB OS + 512MB k3s)</li><li><strong>CPU</strong>: 1-2 cores</li><li><strong>Disk</strong>: 20GB (10GB OS + 10GB containers)</li></ul><p><strong>Maintenance</strong>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># OS updates</span>
</span></span><span style=display:flex><span>sudo apt update <span style=color:#ce5c00;font-weight:700>&amp;&amp;</span> sudo apt upgrade
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># k3s upgrade</span>
</span></span><span style=display:flex><span>curl -sfL https://get.k3s.io <span style=color:#000;font-weight:700>|</span> <span style=color:#000>INSTALL_K3S_VERSION</span><span style=color:#ce5c00;font-weight:700>=</span>v1.32.0+k3s1 sh -
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Or automatic via system-upgrade-controller</span>
</span></span></code></pre></div><h4 id=pros-and-cons>Pros and Cons</h4><ul><li>Good, because most familiar Linux distribution (easy adoption)</li><li>Good, because 5-year LTS support (10 years with Ubuntu Pro)</li><li>Good, because k3s provides single-command setup</li><li>Good, because extensive documentation and community support</li><li>Good, because compatible with all Kubernetes tooling</li><li>Good, because automatic security updates available</li><li>Good, because general-purpose (can run non-K8s workloads)</li><li>Good, because low learning curve</li><li>Neutral, because moderate resource overhead (1GB RAM)</li><li>Bad, because general-purpose OS has larger attack surface</li><li>Bad, because requires manual OS updates and reboots</li><li>Bad, because managing OS + Kubernetes lifecycle separately</li><li>Bad, because imperative configuration (not GitOps-native)</li><li>Bad, because mutable filesystem (configuration drift possible)</li></ul><h3 id=option-2-fedora-server-with-kubeadm>Option 2: Fedora Server with kubeadm</h3><p>Host Kubernetes using Fedora Server with kubeadm (official Kubernetes tool) and CRI-O container runtime.</p><h4 id=architecture-overview-1>Architecture Overview</h4><pre class=mermaid>sequenceDiagram
    participant Admin
    participant Server as Fedora Server
    participant K8s as Kubernetes Components
    
    Admin-&gt;&gt;Server: Install Fedora 41
    Server-&gt;&gt;Server: Configure network
    Admin-&gt;&gt;Server: Update system (dnf update)
    Admin-&gt;&gt;Server: Install CRI-O
    Server-&gt;&gt;Server: Configure CRI-O runtime
    Admin-&gt;&gt;Server: Install kubeadm/kubelet/kubectl
    Server-&gt;&gt;Server: Disable swap, load kernel modules
    Server-&gt;&gt;Server: Configure SELinux
    Admin-&gt;&gt;K8s: kubeadm init --cri-socket=unix:///var/run/crio/crio.sock
    K8s-&gt;&gt;Server: Generate certificates
    K8s-&gt;&gt;Server: Start etcd
    K8s-&gt;&gt;Server: Start API server
    K8s--&gt;&gt;Admin: Control plane ready
    Admin-&gt;&gt;K8s: kubectl apply CNI
    K8s-&gt;&gt;Server: Deploy CNI pods
    Admin-&gt;&gt;K8s: kubeadm join (workers)
    K8s--&gt;&gt;Admin: Cluster ready (15-20 minutes)</pre><h4 id=implementation-details-1>Implementation Details</h4><p><strong>Installation</strong>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Install CRI-O</span>
</span></span><span style=display:flex><span>sudo dnf install -y cri-o
</span></span><span style=display:flex><span>sudo systemctl <span style=color:#204a87>enable</span> --now crio
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Install kubeadm components</span>
</span></span><span style=display:flex><span>sudo dnf install -y kubelet kubeadm kubectl
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Initialize cluster</span>
</span></span><span style=display:flex><span>sudo kubeadm init --pod-network-cidr<span style=color:#ce5c00;font-weight:700>=</span>10.244.0.0/16 --cri-socket<span style=color:#ce5c00;font-weight:700>=</span>unix:///var/run/crio/crio.sock
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Install CNI</span>
</span></span><span style=display:flex><span>kubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.28.0/manifests/calico.yaml
</span></span></code></pre></div><p><strong>Resource Requirements</strong>:</p><ul><li><strong>RAM</strong>: 2.2GB total (700MB OS + 1.5GB Kubernetes)</li><li><strong>CPU</strong>: 2+ cores</li><li><strong>Disk</strong>: 35GB (15GB OS + 20GB containers)</li></ul><p><strong>Maintenance</strong>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># OS updates (every 13 months major upgrade)</span>
</span></span><span style=display:flex><span>sudo dnf update -y
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Kubernetes upgrade</span>
</span></span><span style=display:flex><span>sudo dnf update -y kubeadm
</span></span><span style=display:flex><span>sudo kubeadm upgrade apply v1.32.0
</span></span><span style=display:flex><span>sudo dnf update -y kubelet kubectl
</span></span></code></pre></div><h4 id=pros-and-cons-1>Pros and Cons</h4><ul><li>Good, because SELinux enabled by default (stronger than AppArmor)</li><li>Good, because latest kernel and packages (bleeding edge)</li><li>Good, because native CRI-O support (OpenShift compatibility)</li><li>Good, because upstream for RHEL (enterprise patterns)</li><li>Good, because kubeadm provides full control over cluster</li><li>Neutral, because faster release cycle (latest features, but more upgrades)</li><li>Bad, because short support cycle (13 months per release)</li><li>Bad, because bleeding-edge can introduce instability</li><li>Bad, because complex kubeadm setup (many manual steps)</li><li>Bad, because higher resource overhead (2.2GB RAM)</li><li>Bad, because SELinux configuration for Kubernetes is complex</li><li>Bad, because frequent OS upgrades required (every 13 months)</li><li>Bad, because managing OS + Kubernetes separately</li><li>Bad, because imperative configuration (not GitOps-native)</li></ul><h3 id=option-3-talos-linux-purpose-built-kubernetes-os>Option 3: Talos Linux (purpose-built Kubernetes OS)</h3><p>Use Talos Linux, an immutable, API-driven operating system designed specifically for Kubernetes with built-in cluster management.</p><h4 id=architecture-overview-2>Architecture Overview</h4><pre class=mermaid>sequenceDiagram
    participant Admin
    participant Server as Bare Metal Server
    participant Talos as Talos Linux
    participant K8s as Kubernetes Components
    
    Admin-&gt;&gt;Server: Boot Talos ISO (PXE or USB)
    Server-&gt;&gt;Talos: Start in maintenance mode
    Talos--&gt;&gt;Admin: API endpoint ready
    Admin-&gt;&gt;Admin: Generate configs (talosctl gen config)
    Admin-&gt;&gt;Talos: talosctl apply-config (controlplane.yaml)
    Talos-&gt;&gt;Server: Install Talos to disk
    Server-&gt;&gt;Server: Reboot from disk
    Talos-&gt;&gt;K8s: Start kubelet
    Talos-&gt;&gt;K8s: Start etcd
    Talos-&gt;&gt;K8s: Start API server
    Admin-&gt;&gt;Talos: talosctl bootstrap
    Talos-&gt;&gt;K8s: Initialize cluster
    K8s-&gt;&gt;Talos: Start controller-manager
    K8s--&gt;&gt;Admin: Control plane ready
    Admin-&gt;&gt;K8s: Apply CNI
    Admin-&gt;&gt;Talos: Apply worker configs
    Talos-&gt;&gt;K8s: Join workers
    K8s--&gt;&gt;Admin: Cluster ready (10-15 minutes)</pre><h4 id=implementation-details-2>Implementation Details</h4><p><strong>Installation</strong>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Generate machine configs</span>
</span></span><span style=display:flex><span>talosctl gen config homelab https://192.168.1.10:6443
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Apply config to control plane (booted from ISO)</span>
</span></span><span style=display:flex><span>talosctl apply-config --insecure --nodes 192.168.1.10 --file controlplane.yaml
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Bootstrap Kubernetes</span>
</span></span><span style=display:flex><span>talosctl bootstrap --nodes 192.168.1.10 --endpoints 192.168.1.10
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Get kubeconfig</span>
</span></span><span style=display:flex><span>talosctl kubeconfig --nodes 192.168.1.10
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Add workers</span>
</span></span><span style=display:flex><span>talosctl apply-config --insecure --nodes 192.168.1.11 --file worker.yaml
</span></span></code></pre></div><p><strong>Machine Configuration</strong> (declarative YAML):</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#204a87;font-weight:700>version</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline> </span><span style=color:#000>v1alpha1</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline></span><span style=color:#204a87;font-weight:700>machine</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>  </span><span style=color:#204a87;font-weight:700>type</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline> </span><span style=color:#000>controlplane</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>  </span><span style=color:#204a87;font-weight:700>install</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>    </span><span style=color:#204a87;font-weight:700>disk</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline> </span><span style=color:#000>/dev/sda</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>  </span><span style=color:#204a87;font-weight:700>network</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>    </span><span style=color:#204a87;font-weight:700>hostname</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline> </span><span style=color:#000>control-plane-1</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>    </span><span style=color:#204a87;font-weight:700>interfaces</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>      </span>- <span style=color:#204a87;font-weight:700>interface</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline> </span><span style=color:#000>eth0</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>        </span><span style=color:#204a87;font-weight:700>addresses</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>          </span>- <span style=color:#0000cf;font-weight:700>192.168.1.10</span><span style=color:#000>/24</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline></span><span style=color:#204a87;font-weight:700>cluster</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>  </span><span style=color:#204a87;font-weight:700>clusterName</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline> </span><span style=color:#000>homelab</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>  </span><span style=color:#204a87;font-weight:700>controlPlane</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>    </span><span style=color:#204a87;font-weight:700>endpoint</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline> </span><span style=color:#000>https://192.168.1.10:6443</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>  </span><span style=color:#204a87;font-weight:700>network</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>    </span><span style=color:#204a87;font-weight:700>cni</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>      </span><span style=color:#204a87;font-weight:700>name</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline> </span><span style=color:#000>custom</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>      </span><span style=color:#204a87;font-weight:700>urls</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>        </span>- <span style=color:#000>https://raw.githubusercontent.com/projectcalico/calico/v3.28.0/manifests/calico.yaml</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span></code></pre></div><p><strong>Resource Requirements</strong>:</p><ul><li><strong>RAM</strong>: 768MB total (256MB OS + 512MB Kubernetes)</li><li><strong>CPU</strong>: 1-2 cores</li><li><strong>Disk</strong>: 10-15GB (500MB OS + 10GB containers)</li></ul><p><strong>Maintenance</strong>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Upgrade Talos (OS + Kubernetes)</span>
</span></span><span style=display:flex><span>talosctl upgrade --nodes 192.168.1.10 --image ghcr.io/siderolabs/installer:v1.9.0
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Upgrade Kubernetes version</span>
</span></span><span style=display:flex><span>talosctl upgrade-k8s --nodes 192.168.1.10 --to 1.32.0
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Apply config changes</span>
</span></span><span style=display:flex><span>talosctl apply-config --nodes 192.168.1.10 --file controlplane.yaml
</span></span></code></pre></div><h4 id=pros-and-cons-2>Pros and Cons</h4><ul><li>Good, because Kubernetes built-in (no separate installation)</li><li>Good, because minimal attack surface (no SSH, shell, package manager)</li><li>Good, because immutable infrastructure (config drift impossible)</li><li>Good, because API-driven management (GitOps-friendly)</li><li>Good, because lowest resource overhead (~768MB RAM)</li><li>Good, because declarative configuration (YAML in version control)</li><li>Good, because secure by default (no manual hardening)</li><li>Good, because smallest disk footprint (~500MB OS)</li><li>Good, because designed specifically for Kubernetes</li><li>Good, because simple declarative upgrades (OS + K8s)</li><li>Good, because UEFI Secure Boot support</li><li>Neutral, because smaller community (but active and helpful)</li><li>Bad, because steep learning curve (paradigm shift)</li><li>Bad, because limited to Kubernetes workloads only</li><li>Bad, because troubleshooting without shell requires different approach</li><li>Bad, because relatively new (less mature than Ubuntu/Fedora)</li><li>Bad, because no escape hatch for manual intervention</li></ul><h3 id=option-4-harvester-hci-hyperconverged-platform>Option 4: Harvester HCI (hyperconverged platform)</h3><p>Use Harvester, a hyperconverged infrastructure platform built on K3s and KubeVirt for unified VM + container management.</p><h4 id=architecture-overview-3>Architecture Overview</h4><pre class=mermaid>sequenceDiagram
    participant Admin
    participant Server as Bare Metal Server
    participant Harvester as Harvester HCI
    participant K3s as K3s / KubeVirt
    participant Storage as Longhorn Storage
    
    Admin-&gt;&gt;Server: Boot Harvester ISO
    Server-&gt;&gt;Harvester: Installation wizard
    Admin-&gt;&gt;Harvester: Configure cluster (VIP, storage)
    Harvester-&gt;&gt;Server: Install RancherOS 2.0
    Harvester-&gt;&gt;Server: Install K3s
    Server-&gt;&gt;Server: Reboot
    Harvester-&gt;&gt;K3s: Start K3s server
    K3s-&gt;&gt;Storage: Deploy Longhorn
    K3s-&gt;&gt;Server: Deploy KubeVirt
    K3s-&gt;&gt;Server: Deploy multus CNI
    Harvester--&gt;&gt;Admin: Web UI ready
    Admin-&gt;&gt;Harvester: Add nodes
    Harvester-&gt;&gt;K3s: Join cluster
    K3s--&gt;&gt;Admin: Cluster ready (20-30 minutes)</pre><h4 id=implementation-details-3>Implementation Details</h4><p><strong>Installation</strong>: Interactive ISO wizard or cloud-init config</p><p><strong>Resource Requirements</strong>:</p><ul><li><strong>RAM</strong>: 8GB minimum per node (16GB+ recommended)</li><li><strong>CPU</strong>: 4+ cores per node</li><li><strong>Disk</strong>: 250GB+ per node (100GB OS + 150GB storage)</li><li><strong>Nodes</strong>: 3+ for production HA</li></ul><p><strong>Features</strong>:</p><ul><li>Web UI management</li><li>Built-in storage (Longhorn)</li><li>VM support (KubeVirt)</li><li>Live migration</li><li>Rancher integration</li></ul><h4 id=pros-and-cons-3>Pros and Cons</h4><ul><li>Good, because unified VM + container platform</li><li>Good, because built-in K3s (Kubernetes included)</li><li>Good, because web UI simplifies management</li><li>Good, because built-in persistent storage (Longhorn)</li><li>Good, because VM live migration</li><li>Good, because Rancher integration</li><li>Neutral, because immutable OS layer</li><li>Bad, because very heavy resource requirements (8GB+ RAM)</li><li>Bad, because complex architecture (KubeVirt, Longhorn, multus)</li><li>Bad, because overkill for container-only workloads</li><li>Bad, because larger attack surface (web UI, VM layer)</li><li>Bad, because requires 3+ nodes for HA (not single-node friendly)</li><li>Bad, because steep learning curve for full feature set</li></ul><h2 id=more-information>More Information</h2><h3 id=detailed-analysis>Detailed Analysis</h3><p>For in-depth analysis of each operating system:</p><ul><li><p><a href=../analysis/server-os/ubuntu/><strong>Ubuntu Server Analysis</strong></a></p><ul><li>Installation methods (kubeadm, k3s, MicroK8s)</li><li>Cluster initialization sequences</li><li>Maintenance requirements and upgrade procedures</li><li>Resource overhead and security posture</li></ul></li><li><p><a href=../analysis/server-os/fedora/><strong>Fedora Server Analysis</strong></a></p><ul><li>kubeadm with CRI-O installation</li><li>SELinux configuration for Kubernetes</li><li>Rapid release cycle implications</li><li>RHEL ecosystem compatibility</li></ul></li><li><p><a href=../analysis/server-os/talos-linux/><strong>Talos Linux Analysis</strong></a></p><ul><li>API-driven, immutable architecture</li><li>Declarative configuration model</li><li>Security-first design principles</li><li>Production readiness and advanced features</li></ul></li><li><p><a href=../analysis/server-os/harvester/><strong>Harvester HCI Analysis</strong></a></p><ul><li>Hyperconverged infrastructure capabilities</li><li>VM + container unified platform</li><li>KubeVirt and Longhorn integration</li><li>Multi-node cluster requirements</li></ul></li></ul><h3 id=key-findings-summary>Key Findings Summary</h3><p>Resource efficiency comparison:</p><ul><li>✅ <strong>Talos</strong>: 768MB RAM, 500MB disk (most efficient)</li><li>✅ <strong>Ubuntu + k3s</strong>: 1GB RAM, 20GB disk (efficient)</li><li>⚠️ <strong>Fedora + kubeadm</strong>: 2.2GB RAM, 35GB disk (moderate)</li><li>❌ <strong>Harvester</strong>: 8GB+ RAM, 250GB+ disk (heavy)</li></ul><p>Security posture comparison:</p><ul><li>✅ <strong>Talos</strong>: Minimal attack surface (no SSH/shell, immutable)</li><li>✅ <strong>Fedora</strong>: SELinux by default (strong MAC)</li><li>⚠️ <strong>Ubuntu</strong>: AppArmor (moderate security)</li><li>⚠️ <strong>Harvester</strong>: Larger attack surface (web UI, VM layer)</li></ul><p>Operational complexity comparison:</p><ul><li>✅ <strong>Ubuntu + k3s</strong>: Single command install, familiar management</li><li>✅ <strong>Talos</strong>: Declarative, automated (after learning curve)</li><li>⚠️ <strong>Fedora + kubeadm</strong>: Manual kubeadm steps, frequent OS upgrades</li><li>❌ <strong>Harvester</strong>: Complex HCI architecture, heavy requirements</li></ul><h3 id=decision-matrix>Decision Matrix</h3><table><thead><tr><th>Criterion</th><th>Ubuntu + k3s</th><th>Fedora + kubeadm</th><th>Talos Linux</th><th>Harvester</th></tr></thead><tbody><tr><td><strong>Setup Simplicity</strong></td><td>⭐⭐⭐⭐⭐</td><td>⭐⭐⭐</td><td>⭐⭐⭐⭐</td><td>⭐⭐⭐</td></tr><tr><td><strong>Maintenance Burden</strong></td><td>⭐⭐⭐⭐</td><td>⭐⭐</td><td>⭐⭐⭐⭐⭐</td><td>⭐⭐⭐⭐</td></tr><tr><td><strong>Security Posture</strong></td><td>⭐⭐⭐</td><td>⭐⭐⭐⭐</td><td>⭐⭐⭐⭐⭐</td><td>⭐⭐⭐</td></tr><tr><td><strong>Resource Efficiency</strong></td><td>⭐⭐⭐⭐</td><td>⭐⭐⭐</td><td>⭐⭐⭐⭐⭐</td><td>⭐</td></tr><tr><td><strong>Learning Curve</strong></td><td>⭐⭐⭐⭐⭐</td><td>⭐⭐⭐⭐</td><td>⭐⭐</td><td>⭐⭐⭐</td></tr><tr><td><strong>Community Support</strong></td><td>⭐⭐⭐⭐⭐</td><td>⭐⭐⭐⭐</td><td>⭐⭐⭐</td><td>⭐⭐⭐⭐</td></tr><tr><td><strong>Immutability</strong></td><td>⭐</td><td>⭐</td><td>⭐⭐⭐⭐⭐</td><td>⭐⭐⭐⭐</td></tr><tr><td><strong>GitOps-Friendly</strong></td><td>⭐⭐</td><td>⭐⭐</td><td>⭐⭐⭐⭐⭐</td><td>⭐⭐⭐</td></tr><tr><td><strong>Purpose-Built</strong></td><td>⭐⭐</td><td>⭐⭐</td><td>⭐⭐⭐⭐⭐</td><td>⭐⭐⭐⭐</td></tr><tr><td><strong>Overall Score</strong></td><td>29/45</td><td>24/45</td><td>38/45</td><td>28/45</td></tr></tbody></table><p><strong>Talos Linux scores highest</strong> for Kubernetes-dedicated homelab infrastructure prioritizing security, efficiency, and GitOps workflows.</p><h3 id=trade-offs-analysis>Trade-offs Analysis</h3><p><strong>Choosing Talos Linux</strong>:</p><ul><li><strong>Wins</strong>: Best security, lowest overhead, declarative configuration, minimal maintenance</li><li><strong>Loses</strong>: Steeper learning curve, no shell access, smaller community</li></ul><p><strong>Choosing Ubuntu + k3s</strong>:</p><ul><li><strong>Wins</strong>: Easiest adoption, largest community, general-purpose flexibility</li><li><strong>Loses</strong>: Higher attack surface, manual OS management, imperative config</li></ul><p><strong>Choosing Fedora + kubeadm</strong>:</p><ul><li><strong>Wins</strong>: Latest features, SELinux, enterprise compatibility</li><li><strong>Loses</strong>: Frequent OS upgrades, complex setup, higher overhead</li></ul><p><strong>Choosing Harvester</strong>:</p><ul><li><strong>Wins</strong>: VM + container unified platform, web UI</li><li><strong>Loses</strong>: Heavy resources, complex architecture, overkill for K8s-only</li></ul><p>For a <strong>Kubernetes-dedicated homelab prioritizing security and efficiency</strong>, Talos Linux&rsquo;s benefits outweigh the learning curve investment.</p><h3 id=related-adrs>Related ADRs</h3><ul><li><a href=./0001-use-madr-for-architecture-decision-records/>ADR-0001: Use MADR for Architecture Decision Records</a> - MADR format used for this ADR</li><li><a href=./0002-network-boot-architecture/>ADR-0002: Network Boot Architecture</a> - Server provisioning architecture</li><li><a href=./0003-cloud-provider-selection/>ADR-0003: Cloud Provider Selection</a> - Cloud infrastructure decisions</li></ul><h3 id=future-considerations>Future Considerations</h3><ol><li><strong>Team Growth</strong>: If team grows beyond single person, reassess Ubuntu for familiarity</li><li><strong>VM Requirements</strong>: If VM workloads emerge, consider Harvester or KubeVirt on Talos</li><li><strong>Enterprise Patterns</strong>: If RHEL compatibility needed, reconsider Fedora/CentOS Stream</li><li><strong>Maintenance Burden</strong>: If Talos learning curve proves too steep, fallback to k3s</li><li><strong>Talos Maturity</strong>: Monitor Talos ecosystem growth and production adoption</li></ol><h3 id=related-issues>Related Issues</h3><ul><li><a href=https://github.com/Zaba505/infra/issues/598>Issue #598</a> - story(docs): create adr for server operating system</li></ul></div></main></div></div><footer class="td-footer row d-print-none"><div class=container-fluid><div class="row mx-md-2"><div class="td-footer__left col-6 col-sm-4 order-sm-1"></div><div class="td-footer__right col-6 col-sm-4 order-sm-3"><ul class=td-footer__links-list><li class=td-footer__links-item data-bs-toggle=tooltip title=GitHub aria-label=GitHub><a target=_blank rel=noopener href=https://github.com/Zaba505/infra aria-label=GitHub><i class="fab fa-github"></i></a></li></ul></div><div class="td-footer__center col-12 col-sm-4 py-2 order-sm-2"><span class=td-footer__copyright>&copy;
2024&ndash;2025
<span class=td-footer__authors>Zaba505 | <a href=https://creativecommons.org/licenses/by/4.0>CC BY 4.0</a> |</span></span><span class=td-footer__all_rights_reserved>All Rights Reserved</span></div></div></div></footer></div><script src=/infra/pr-preview/pr-600/js/main.min.eb40505784d893e4b5c8dbd67b59c353e735d847f4ffbfe9d6921dec08dbacba.js integrity="sha256-60BQV4TYk+S1yNvWe1nDU+c12Ef0/7/p1pId7AjbrLo=" crossorigin=anonymous></script><script defer src=/infra/pr-preview/pr-600/js/click-to-copy.min.73478a7d4807698aed7e355eb23f9890ca18fea3158604c8471746d046702bad.js integrity="sha256-c0eKfUgHaYrtfjVesj+YkMoY/qMVhgTIRxdG0EZwK60=" crossorigin=anonymous></script><script src=/infra/pr-preview/pr-600/js/tabpane-persist.js></script></body></html>