[{"body":" Context and Problem Statement As the project grows, architectural decisions are made that have long-term impacts on the system’s design, maintainability, and scalability. Without a structured way to document these decisions, we risk losing the context and rationale behind important choices, making it difficult for current and future team members to understand why certain approaches were taken.\nHow should we document architectural decisions in a way that is accessible, maintainable, and provides sufficient context for future reference?\nDecision Drivers Need for clear documentation of architectural decisions and their rationale Easy accessibility and searchability of past decisions Low barrier to entry for creating and maintaining decision records Integration with existing documentation workflow Version control friendly format Industry-standard approach that team members may already be familiar with Considered Options MADR (Markdown Architectural Decision Records) ADR using custom format Wiki-based documentation No formal ADR process Decision Outcome Chosen option: “MADR (Markdown Architectural Decision Records)”, because it provides a well-established, standardized format that is lightweight, version-controlled, and integrates seamlessly with our existing documentation structure. MADR 4.0.0 offers a clear template that captures all necessary information while remaining flexible enough for different types of decisions.\nConsequences Good, because MADR is a widely adopted standard with clear documentation and examples Good, because markdown files are easy to create, edit, and review through pull requests Good, because ADRs will be version-controlled alongside code, maintaining historical context Good, because the format is flexible enough to accommodate strategic, user-journey, and API design decisions Good, because team members can easily search and reference past decisions Neutral, because requires discipline to maintain and update ADR status as decisions evolve Bad, because team members need to learn and follow the MADR format conventions Confirmation Compliance will be confirmed through:\nCode reviews ensuring new architectural decisions are documented as ADRs ADRs are stored in docs/content/r\u0026d/adrs/ following the naming convention NNNN-title-with-dashes.md Regular reviews during architecture discussions to reference and update existing ADRs Pros and Cons of the Options MADR (Markdown Architectural Decision Records) MADR 4.0.0 is a standardized format for documenting architectural decisions using markdown.\nGood, because it’s a well-established standard with extensive documentation Good, because markdown is simple, portable, and version-control friendly Good, because it provides a clear structure while remaining flexible Good, because it integrates with static site generators and documentation tools Good, because it’s lightweight and doesn’t require special tools Neutral, because it requires some initial learning of the format Neutral, because maintaining consistency requires discipline ADR using custom format Create our own custom format for architectural decision records.\nGood, because we can tailor it exactly to our needs Bad, because it requires defining and maintaining our own standard Bad, because new team members won’t be familiar with the format Bad, because we lose the benefits of community knowledge and tooling Bad, because it may evolve inconsistently over time Wiki-based documentation Use a wiki system (like Confluence, Notion, or GitHub Wiki) to document decisions.\nGood, because wikis provide easy editing and hyperlinking Good, because some team members may be familiar with wiki tools Neutral, because it may or may not integrate with version control Bad, because content may not be version-controlled alongside code Bad, because it creates a separate system to maintain Bad, because it’s harder to review changes through standard PR process Bad, because portability and long-term accessibility may be concerns No formal ADR process Continue without a structured approach to documenting architectural decisions.\nGood, because it requires no additional overhead Bad, because context and rationale for decisions are lost over time Bad, because new team members struggle to understand why decisions were made Bad, because it leads to repeated discussions of previously settled questions Bad, because it makes it difficult to track when decisions should be revisited More Information MADR 4.0.0 specification: https://adr.github.io/madr/ ADRs will be categorized as: strategic, user-journey, or api-design ADR status values: proposed | accepted | rejected | deprecated | superseded by ADR-XXXX All ADRs are stored in docs/content/r\u0026d/adrs/ directory ","categories":"","description":"Adopt Markdown Architectural Decision Records (MADR) as the standard format for documenting architectural decisions in the project.\n","excerpt":"Adopt Markdown Architectural Decision Records (MADR) as the standard …","ref":"/infra/pr-preview/pr-592/rd/adrs/0001-use-madr-for-architecture-decision-records/","tags":"","title":"[0001] Use MADR for Architecture Decision Records"},{"body":"Overview The HP ProLiant DL360 Gen9 provides robust network boot capabilities through multiple protocols and firmware interfaces. This makes it particularly well-suited for diskless deployments, automated provisioning, and infrastructure-as-code workflows.\nSupported Network Boot Protocols PXE (Preboot Execution Environment) The DL360 Gen9 fully supports PXE boot via both legacy BIOS and UEFI firmware modes:\nLegacy BIOS PXE: Traditional PXE implementation using TFTP\nProtocol: PXEv2 (PXE 2.1) Network Stack: IPv4 only in legacy mode Boot files: pxelinux.0, undionly.kpxe, or custom NBP DHCP options: Standard options 66 (TFTP server) and 67 (boot filename) UEFI PXE: Modern UEFI network boot implementation\nProtocol: PXEv2 with UEFI extensions Network Stack: IPv4 and IPv6 support Boot files: bootx64.efi, grubx64.efi, shimx64.efi Architecture: x64 (EFI BC) DHCP Architecture ID: 0x0007 (EFI BC) or 0x0009 (EFI x86-64) iPXE Support The DL360 Gen9 can boot iPXE, enabling advanced features:\nChainloading: Boot standard PXE, then chainload iPXE for enhanced capabilities HTTP/HTTPS Boot: Download kernels and images over HTTP(S) instead of TFTP SAN Boot: iSCSI and AoE (ATA over Ethernet) support Scripting: Conditional boot logic and dynamic configuration Embedded Scripts: iPXE can be compiled with embedded boot scripts Implementation Methods:\nChainload from standard PXE: DHCP points to undionly.kpxe or ipxe.efi Flash iPXE to FlexibleLOM option ROM (advanced, requires care) Boot iPXE from USB, then continue network boot UEFI HTTP Boot Native UEFI HTTP boot is supported on Gen9 servers with recent firmware:\nProtocol: RFC 7230 HTTP/1.1 Requirements: UEFI firmware version 2.40 or later (check via iLO) DHCP option 60 (vendor class identifier) = “HTTPClient” DHCP option 67 pointing to HTTP(S) URL Advantages: No TFTP server required Faster transfers than TFTP Support for HTTPS with certificate validation Better suited for large images (kernels, initramfs) Limitations: UEFI mode only (not available in legacy BIOS) Requires DHCP server with HTTP URL support HTTP(S) Boot Configuration For UEFI HTTP boot on DL360 Gen9:\n# Example ISC DHCP configuration for UEFI HTTP boot class \"httpclients\" { match if substring(option vendor-class-identifier, 0, 10) = \"HTTPClient\"; } pool { allow members of \"httpclients\"; option vendor-class-identifier \"HTTPClient\"; # Point to HTTP boot URI filename \"http://boot.example.com/boot/efi/bootx64.efi\"; } Network Interface Options The DL360 Gen9 supports multiple network adapter configurations for boot:\nFlexibleLOM (LOM = LAN on Motherboard) HPE FlexibleLOM slot supports:\nHPE 366FLR: Quad-port 1GbE (Broadcom BCM5719) HPE 560FLR-SFP+: Dual-port 10GbE (Intel X710) HPE 361i: Dual-port 1GbE (Intel I350) All FlexibleLOM adapters support PXE and UEFI network boot. The option ROM can be configured via BIOS/UEFI settings.\nPCIe Network Adapters Standard PCIe network cards with PXE/UEFI boot ROM support:\nIntel X520, X710 series (10GbE) Broadcom NetXtreme series Mellanox ConnectX-3/4 (with appropriate firmware) Boot Priority: Configure via System ROM \u003e Network Boot Options to select which NIC boots first.\nFirmware Configuration Accessing Boot Configuration RBSU (ROM-Based Setup Utility): Press F9 during POST iLO 4 Remote Console: Access via network, then virtual F9 UEFI System Utilities: Modern interface for UEFI firmware settings Key Settings Navigate to: System Configuration \u003e BIOS/Platform Configuration (RBSU) \u003e Network Boot Options\nNetwork Boot: Enable/Disable Boot Mode: UEFI or Legacy BIOS IPv4/IPv6: Enable protocol support Boot Retry: Number of attempts before falling back to next boot device Boot Order: Prioritize network boot in boot sequence Per-NIC Configuration In RBSU \u003e Network Options:\nOption ROM: Enable/Disable per adapter Link Speed: Force speed/duplex or auto-negotiate VLAN: VLAN tagging for boot (if supported by DHCP/PXE environment) PXE Menu: Enable interactive PXE menu (Ctrl+S during PXE boot) iLO 4 Integration The DL360 Gen9’s iLO 4 provides additional network boot features:\nVirtual Media Network Boot Mount ISO images remotely via iLO Virtual Media Boot from network-attached ISO without physical media Useful for OS installation or diagnostics Workflow:\nUpload ISO to HTTP/HTTPS server or use SMB/NFS share iLO Remote Console \u003e Virtual Devices \u003e Image File CD-ROM/DVD Set boot order to prioritize virtual optical drive Reboot server Scripted Deployment via iLO iLO 4 RESTful API allows:\nSetting one-time boot to network via API call Automating PXE boot for provisioning pipelines Integration with tools like Terraform, Ansible Example using iLO RESTful API:\ncurl -k -u admin:password -X PATCH \\ https://ilo-hostname/redfish/v1/Systems/1/ \\ -d '{\"Boot\":{\"BootSourceOverrideTarget\":\"Pxe\",\"BootSourceOverrideEnabled\":\"Once\"}}' Boot Process Flow Legacy BIOS PXE Boot Server powers on, initializes NICs NIC sends DHCPDISCOVER with PXE vendor options DHCP server responds with IP, TFTP server (option 66), boot file (option 67) NIC downloads NBP (Network Bootstrap Program) via TFTP NBP executes (e.g., pxelinux.0 loads syslinux menu) User selects boot target or automated script continues Kernel and initramfs download and boot UEFI PXE Boot UEFI firmware initializes network stack UEFI PXE driver sends DHCPv4/v6 DISCOVER DHCP responds with boot file (e.g., bootx64.efi) UEFI downloads boot file via TFTP UEFI loads and executes boot loader (GRUB2, systemd-boot, iPXE) Boot loader may download additional files (kernel, initrd, config) OS boots UEFI HTTP Boot UEFI firmware with HTTP Boot support enabled DHCP request includes “HTTPClient” vendor class DHCP responds with HTTP(S) URL in option 67 UEFI HTTP client downloads boot file over HTTP(S) Execution continues as with UEFI PXE Performance Considerations TFTP vs HTTP TFTP: Slow for large files (typical: 1-5 MB/s) Use for small boot loaders only Chainload to iPXE or HTTP boot for better performance HTTP: 10-100x faster depending on network and server Recommended for kernels, initramfs, live OS images iPXE or UEFI HTTP boot required Network Speed Impact DL360 Gen9 boot performance by NIC speed:\n1GbE: Adequate for most PXE deployments (100-125 MB/s theoretical max) 10GbE: Significant improvement for large image downloads (1-2 GB/s) Bonding/Teaming: Not typically used for boot (single NIC boots) Recommendation: For production diskless nodes or frequent re-provisioning, 10GbE with HTTP boot provides best performance.\nCommon Use Cases 1. Automated OS Provisioning Boot into installer via PXE:\nKickstart (RHEL/CentOS/Rocky) Preseed (Debian/Ubuntu) Ignition (Fedora CoreOS, Flatcar) 2. Diskless Boot Boot OS entirely from network/RAM:\nNetwork root: NFS or iSCSI root filesystem Overlay: Persistent storage via network overlay Stateless: Boot identical image, no local state 3. Rescue and Diagnostics Boot live environments:\nSystemRescue Clonezilla Memtest86+ Hardware diagnostics (HPE Service Pack for ProLiant) 4. Kubernetes/Container Hosts PXE boot immutable OS images:\nTalos Linux: API-driven, diskless k8s nodes Flatcar Container Linux: Automated updates k3OS: Lightweight k8s OS Troubleshooting PXE Boot Fails Symptoms: “PXE-E51: No DHCP or proxy DHCP offers received” or timeout\nChecks:\nVerify NIC link light and switch port status Confirm DHCP server is responding (check DHCP logs) Ensure DHCP options 66 and 67 are set correctly Test TFTP server accessibility (tftp -i \u003cserver\u003e GET \u003cfile\u003e) Check BIOS/UEFI network boot is enabled Verify boot order prioritizes network boot Disable Secure Boot if using unsigned boot files UEFI Network Boot Not Available Symptoms: Network boot option missing in UEFI boot menu\nResolution:\nEnter RBSU (F9), navigate to Network Options Ensure at least one NIC has “Option ROM” enabled Verify Boot Mode is set to UEFI (not Legacy) Update System ROM to latest version if option is missing Some FlexibleLOM cards require firmware update for UEFI boot support HTTP Boot Fails Symptoms: UEFI HTTP boot option present but fails to download\nChecks:\nVerify firmware version supports HTTP boot (\u003e=2.40) Ensure DHCP option 67 contains valid HTTP(S) URL Test URL accessibility from another client Check DNS resolution if using hostname in URL For HTTPS: Verify certificate is trusted (or disable cert validation in test) Slow PXE Boot Symptoms: Boot process takes minutes instead of seconds\nOptimizations:\nSwitch from TFTP to HTTP (chainload iPXE or use UEFI HTTP boot) Increase TFTP server block size (tftp-hpa --blocksize 1468) Tune DHCP response times (reduce lease query delays) Use local network segment for boot server (avoid WAN/VPN) Enable NIC interrupt coalescing in BIOS for 10GbE Security Considerations Secure Boot DL360 Gen9 supports UEFI Secure Boot:\nValidates signed boot loaders (shim, GRUB, kernel) Prevents unsigned code execution during boot Required for some compliance scenarios Configuration: RBSU \u003e Boot Options \u003e Secure Boot = Enabled\nImplications for Network Boot:\nMust use signed boot loaders (e.g., shim.efi signed by Microsoft/vendor) Custom kernels require signing or disabling Secure Boot iPXE must be signed or chainloaded from signed shim Network Security Risks:\nPXE/TFTP is unencrypted and unauthenticated Attacker on network can serve malicious boot images DHCP spoofing can redirect to malicious boot server Mitigations:\nNetwork Segmentation: Isolate PXE boot to management VLAN DHCP Snooping: Prevent rogue DHCP servers on switch HTTPS Boot: Use UEFI HTTP boot with TLS and certificate validation iPXE with HTTPS: Chainload iPXE, then use HTTPS for all downloads Signed Images: Use Secure Boot with signed boot chain 802.1X: Require network authentication before DHCP (complex for PXE) iLO Security Change default iLO password immediately Use TLS for iLO web interface and API Restrict iLO network access (firewall, separate VLAN) Disable iLO Virtual Media if not needed Enable iLO Security Override for extra security during boot Firmware and Driver Resources Required Firmware Versions For optimal network boot support:\nSystem ROM: v2.60 or later (latest recommended) iLO 4 Firmware: v2.80 or later NIC Firmware: Latest for specific FlexibleLOM/PCIe card Check current versions: iLO web interface \u003e Information \u003e Firmware Information\nUpdating Firmware Methods:\nHPE Service Pack for ProLiant (SPP): Comprehensive update bundle\nBoot from SPP ISO (via iLO Virtual Media or USB) Runs Smart Update Manager (SUM) in Linux environment Updates all firmware, drivers, system ROM automatically iLO Web Interface: Individual component updates\nSystem ROM: Administration \u003e Firmware \u003e Update Firmware Upload .fwpkg or .bin files from HPE support site Online Flash Component: Linux Online ROM Flash utility\nInstall hp-firmware-* packages Run updates while OS is running (requires reboot to apply) Download Source: https://support.hpe.com/connect/s/product?language=en_US\u0026kmpmoid=1010026910 (requires HPE Passport account, free registration)\nBest Practices Use UEFI Mode: Better security, IPv6 support, larger disk support Enable HTTP Boot: Faster and more reliable than TFTP for large files Chainload iPXE: Flexibility of iPXE with standard PXE infrastructure Update Firmware: Keep System ROM and iLO current for bug fixes and features Isolate Boot Network: Use dedicated management VLAN for PXE/provisioning Test Failover: Configure multiple DHCP servers and boot mirrors for redundancy Document Configuration: Record BIOS settings, DHCP config, and boot infrastructure Monitor iLO Logs: Track boot failures and hardware issues via iLO event log References HPE ProLiant DL360 Gen9 Server User Guide HPE UEFI System Utilities User Guide iLO 4 User Guide (firmware version 2.80) Intel PXE Specification v2.1 UEFI Specification v2.8 (HTTP Boot) iPXE Documentation: https://ipxe.org/ Conclusion The HP ProLiant DL360 Gen9 provides enterprise-grade network boot capabilities suitable for both traditional PXE deployments and modern UEFI HTTP boot scenarios. Its flexible configuration options, mature firmware support, and iLO integration make it an excellent platform for automated provisioning, diskless computing, and infrastructure-as-code workflows in home lab environments.\nFor home lab use, the recommended configuration is:\nUEFI boot mode with Secure Boot disabled (unless required) iPXE chainloading for flexibility and HTTP performance iLO 4 configured for remote management and scripted provisioning Latest firmware for stability and feature support ","categories":"","description":"Comprehensive analysis of network boot support on HP ProLiant DL360 Gen9\n","excerpt":"Comprehensive analysis of network boot support on HP ProLiant DL360 …","ref":"/infra/pr-preview/pr-592/rd/analysis/hp_dl360_gen9/network-boot/","tags":"","title":"Network Boot Capabilities"},{"body":"System Overview The HP ProLiant DL360 Gen9 is a dual-socket 1U rack server designed for data center and enterprise deployments, also popular in home lab environments due to its performance and manageability.\nGeneration: Gen9 (2014-2017 product cycle)\nForm Factor: 1U rack-mountable (19-inch standard rack)\nDimensions: 43.46 x 67.31 x 4.29 cm (17.1 x 26.5 x 1.69 in)\nProcessor Support Supported CPU Families The DL360 Gen9 supports Intel Xeon E5-2600 v3 and v4 series processors:\nE5-2600 v3 (Haswell-EP): Released Q3 2014\nProcess: 22nm Cores: 4-18 per socket TDP: 55W-145W Max Memory Speed: DDR4-2133 E5-2600 v4 (Broadwell-EP): Released Q1 2016\nProcess: 14nm Cores: 4-22 per socket TDP: 55W-145W Max Memory Speed: DDR4-2400 Popular CPU Options Value: E5-2620 v3/v4 (6 cores, 15MB cache, 85W)\nBalanced: E5-2650 v3/v4 (10-12 cores, 25-30MB cache, 105W)\nPerformance: E5-2680 v3/v4 (12-14 cores, 30-35MB cache, 120W)\nHigh Core Count: E5-2699 v4 (22 cores, 55MB cache, 145W)\nConfiguration Options Single Processor: One CPU socket populated (budget option) Dual Processor: Both sockets populated (full performance) Note: Memory and I/O performance scales with processor count. Single-CPU configuration limits memory channels and PCIe lanes.\nMemory Architecture Memory Specifications Type: DDR4 RDIMM or LRDIMM Speed: DDR4-2133 (v3) or DDR4-2400 (v4) Slots: 24 DIMM slots (12 per processor) Maximum Capacity: 768GB with 32GB RDIMMs 1.5TB with 64GB LRDIMMs (v4 processors) Minimum: 8GB (1x 8GB DIMM) Memory Configuration Rules Channels per CPU: 4 channels, 3 DIMMs per channel Population: Populate channels evenly for optimal bandwidth Mixing: Do not mix RDIMM and LRDIMM types Speed: All DIMMs run at speed of slowest DIMM Recommended Configurations Basic Home Lab (Single CPU):\n4x 16GB = 64GB (one DIMM per channel on both memory boards) Standard (Dual CPU):\n8x 16GB = 128GB (one DIMM per channel) 12x 16GB = 192GB (two DIMMs per channel on primary channels) High Capacity (Dual CPU):\n24x 32GB = 768GB (all slots populated, RDIMM) Performance Priority: Populate all channels before adding second DIMM per channel\nStorage Options Drive Bay Configurations The DL360 Gen9 offers multiple drive bay configurations:\n8 SFF (2.5-inch): Most common configuration 10 SFF: Extended bay version 4 LFF (3.5-inch): Less common in 1U form factor Drive Types Supported SAS: 12Gb/s, 6Gb/s (enterprise-grade) SATA: 6Gb/s, 3Gb/s (value option) SSD: SAS/SATA SSD, NVMe (with appropriate controller) Storage Controllers Smart Array Controllers (HPE proprietary RAID):\nP440ar: Entry-level, 2GB FBWC (Flash-Backed Write Cache), RAID 0/1/5/6/10 P840ar: High-performance, 4GB FBWC, RAID 0/1/5/6/10/50/60 P440: PCIe card version, 2GB FBWC P840: PCIe card version, 4GB FBWC HBA Mode (non-RAID pass-through):\nSmart Array controllers in HBA mode for software RAID (ZFS, mdadm) Limited support; check firmware version Alternative Controllers:\nLSI/Broadcom HBA controllers in PCIe slots H240ar (12Gb/s HBA mode) Boot Drive Options For network-focused deployments:\nMinimal Local Storage: 2x SSD in RAID 1 for hypervisor/OS USB/SD Boot: iLO supports USB boot, SD card (internal USB) Diskless: Pure network boot (subject of network-boot.md) Network Connectivity Integrated FlexibleLOM The DL360 Gen9 includes a FlexibleLOM slot for swappable network adapters:\nCommon FlexibleLOM Options:\nHPE 366FLR: 4x 1GbE (Broadcom BCM5719)\nMost common, good for general use Supports PXE, UEFI network boot, SR-IOV HPE 560FLR-SFP+: 2x 10GbE SFP+ (Intel X710)\nHigh performance, fiber or DAC Supports PXE, UEFI boot, SR-IOV, RDMA (RoCE) HPE 361i: 2x 1GbE (Intel I350)\nEntry-level, good driver support PCIe Expansion Slots Slot Configuration:\nSlot 1: PCIe 3.0 x16 (low-profile) Slot 2: PCIe 3.0 x8 (low-profile) Slot 3: PCIe 3.0 x8 (low-profile) - optional, depends on riser Network Card Options:\nIntel X520/X710 (10GbE) Mellanox ConnectX-3/ConnectX-4 (10/25/40GbE, InfiniBand) Broadcom NetXtreme (1/10/25GbE) Note: Ensure cards are low-profile for 1U chassis compatibility\nPower Supply PSU Options 500W: Single PSU, non-redundant (not recommended) 800W: Common, supports dual CPU + moderate expansion 1400W: High-power, dual CPU with high TDP + GPUs Redundancy: 1+1 redundant hot-plug recommended Power Configuration Platinum Efficiency: 94%+ at 50% load Hot-Plug: Replace without powering down Auto-Switching: 100-240V AC, 50/60Hz Home Lab Power Draw (typical):\nIdle (dual E5-2650 v3, 128GB RAM): 100-130W Load: 200-350W depending on CPU and drive configuration Power Management HPE Dynamic Power Capping: Limit max power via iLO Collaborative Power: Share power budget across chassis in blade environments Energy Efficient Ethernet (EEE): Reduce NIC power during low utilization Cooling and Acoustics Fan Configuration 6x Hot-Plug Fans: Front-mounted, redundant (N+1) Variable Speed: Controlled by System ROM based on thermal sensors iLO Management: Monitor fan speed, temperature via iLO Thermal Management Temperature Range: 10-35°C (50-95°F) operating Altitude: Up to 3,050m (10,000 ft) at reduced temperature Airflow: Front-to-back, ensure clear intake and exhaust Noise Level Idle: ~45 dBA (quiet for 1U server) Load: 55-70 dBA depending on thermal demand Home Lab Consideration: Audible but acceptable in dedicated space; louder than desktop workstation Noise Reduction:\nRun lower TDP CPUs (e.g., E5-2620 series) Maintain ambient temperature \u003c25°C Ensure adequate airflow (not in enclosed cabinet without ventilation) Management - iLO 4 iLO 4 Features The Integrated Lights-Out 4 (iLO 4) provides out-of-band management:\nWeb Interface: HTTPS management console Remote Console: HTML5 or Java-based KVM Virtual Media: Mount ISOs/images remotely Power Control: Power on/off, reset, cold boot Monitoring: Sensors, event logs, hardware health Alerting: Email alerts, SNMP traps, syslog Scripting: RESTful API (Redfish standard) iLO Licensing iLO Standard (included): Basic management, remote console iLO Advanced (license required): Virtual media Remote console performance improvements Directory integration (LDAP/AD) Graphical remote console iLO Advanced Premium (license required): Insight Remote Support Federation Jitter smoothing Home Lab: iLO Advanced license highly recommended for virtual media and full remote console features\niLO Network Configuration Dedicated iLO Port: Separate 1GbE management port (recommended) Shared LOM: Share FlexibleLOM port with OS (not recommended for isolation) Security: Isolate iLO on dedicated management VLAN, disable if not needed\nBIOS and Firmware System ROM (BIOS/UEFI) Firmware Type: UEFI 2.31 or later Boot Modes: UEFI, Legacy BIOS, or hybrid Configuration: RBSU (ROM-Based Setup Utility) accessible via F9 Firmware Update Methods Service Pack for ProLiant (SPP): Comprehensive bundle of all firmware iLO Online Flash: Update via web interface Online ROM Flash: Linux utility for online updates USB Flash: Boot from USB with firmware update utility Recommended Practice: Update to latest SPP for security patches and feature improvements\nSecure Boot UEFI Secure Boot: Supported, validates boot loader signatures TPM: Optional Trusted Platform Module 1.2 or 2.0 Boot Order Protection: Prevent unauthorized boot device changes Expansion and Modularity GPU Support Limited GPU support due to 1U form factor and power constraints:\nLow-Profile GPUs: Nvidia T4, AMD Instinct MI25 (may require custom cooling) Power: Consider 1400W PSU for high-power GPUs Not Ideal: For GPU-heavy workloads, consider 2U+ servers (e.g., DL380 Gen9) USB Ports Front: 1x USB 3.0 Rear: 2x USB 3.0 Internal: 1x USB 2.0 (for SD/USB boot device) Serial Port Rear serial port for legacy console access Useful for network equipment serial console, debug Home Lab Considerations Pros for Home Lab Density: 1U form factor saves rack space iLO Management: Enterprise remote management without KVM Network Boot: Excellent PXE/UEFI boot support (see network-boot.md) Serviceability: Hot-swap drives, PSU, fans Documentation: Extensive HPE documentation and community support Parts Availability: Common on secondary market, affordable Cons for Home Lab Noise: Louder than tower servers or workstations Power: Higher idle power than consumer hardware (100-130W idle) 1U Limitations: Limited GPU, PCIe expansion vs 2U/4U chassis Firmware: Requires HPE account for SPP downloads (free but registration required) Recommended Home Lab Configuration Budget (~$500-800 used):\nDual E5-2620 v3 or v4 (6 cores each, 85W TDP) 128GB RAM (8x 16GB DDR4) 2x SSD (boot), 4-6x HDD/SSD (data) HPE 366FLR (4x 1GbE) Dual 500W or 800W PSU (redundant) iLO Advanced license Performance (~$1000-1500 used):\nDual E5-2680 v4 (14 cores each, 120W TDP) 256GB RAM (16x 16GB DDR4) 2x NVMe SSD (boot/cache), 6-8x SSD (data) HPE 560FLR-SFP+ (2x 10GbE) + PCIe 4x1GbE card Dual 800W PSU iLO Advanced license Comparison with Other Generations vs Gen8 (Previous) Gen9 Advantages:\nDDR4 vs DDR3 (lower power, higher capacity) Better UEFI support and HTTP boot Newer processor architecture (Haswell/Broadwell vs Sandy Bridge/Ivy Bridge) iLO 4 vs iLO 3 (better HTML5 console) Gen8 Advantages:\nLower cost on secondary market Adequate for light workloads vs Gen10 (Next) Gen10 Advantages:\nNewer CPUs (Skylake-SP/Cascade Lake) More PCIe lanes Better UEFI firmware and security features DDR4-2666/2933 support Gen9 Advantages:\nLower cost (mature product cycle) Excellent value for performance/dollar Still well-supported by modern OS and firmware Technical Resources QuickSpecs: HPE ProLiant DL360 Gen9 Server QuickSpecs User Guide: HPE ProLiant DL360 Gen9 Server User Guide Maintenance and Service Guide: Detailed disassembly and part replacement Firmware Downloads: HPE Support Portal (requires free account) Summary The HP ProLiant DL360 Gen9 remains an excellent choice for home labs and small deployments in 2024-2025. Its balance of performance (dual Xeon v4, 768GB RAM capacity), manageability (iLO 4), and network boot capabilities make it particularly well-suited for virtualization, container hosting, and infrastructure automation workflows. While not the latest generation, it offers strong value with robust firmware support and wide secondary market availability.\nBest For:\nVirtualization hosts (ESXi, Proxmox, Hyper-V) Kubernetes/container platforms Network boot/diskless deployments Storage servers (with appropriate controller) General compute workloads Avoid For:\nGPU-intensive workloads (1U constraints) Noise-sensitive environments (unless isolated) Extreme low-power requirements (100W+ idle) ","categories":"","description":"Detailed hardware specifications and configuration options for HP ProLiant DL360 Gen9\n","excerpt":"Detailed hardware specifications and configuration options for HP …","ref":"/infra/pr-preview/pr-592/rd/analysis/hp_dl360_gen9/specifications/","tags":"","title":"Hardware Specifications"},{"body":"Initial Setup Hardware Assembly Install Processors:\nUse thermal paste (HPE thermal grease recommended) Align CPU carefully with socket (LGA 2011-3) Secure heatsink with proper torque (hand-tighten screws in cross pattern) Install both CPUs for dual-socket configuration Install Memory:\nPopulate channels evenly (see Memory Configuration below) Seat DIMMs firmly until retention clips engage Verify all DIMMs recognized in POST Install Storage:\nInsert drives into hot-swap caddies Label drives clearly for identification Configure RAID controller (see Storage Configuration below) Install Network Cards:\nFlexibleLOM: Slide into dedicated slot until seated PCIe cards: Ensure low-profile brackets, secure with screw Note MAC addresses for DHCP reservations Connect Power:\nInstall PSUs (both for redundancy) Connect power cords Verify PSU LEDs indicate proper operation Initial Power-On:\nPress power button Monitor POST on screen or via iLO remote console Address any POST errors before proceeding iLO 4 Initial Configuration Physical iLO Connection Connect Ethernet cable to dedicated iLO port (not FlexibleLOM) Default iLO IP: Obtains via DHCP, or use temporary address via RBSU Check DHCP server logs for iLO MAC and assigned IP First Login Access iLO web interface: https://\u003cilo-ip\u003e Default credentials: Username: Administrator Password: On label on server pull-out tab (or rear label) Immediately change default password (Administration \u003e Access Settings) Essential iLO Settings Network Configuration (Administration \u003e Network):\nSet static IP or DHCP reservation Configure DNS servers Set hostname (e.g., ilo-dl360-01) Enable SNTP time sync Security (Administration \u003e Security):\nEnforce HTTPS only (disable HTTP) Configure SSH key authentication if using CLI Set strong password policy Enable iLO Security features Access (Administration \u003e Access Settings):\nConfigure iLO username/password for automation Create additional user accounts (separation of duties) Set session timeout (default: 30 minutes) Date and Time (Administration \u003e Date and Time):\nSet NTP servers for accurate timestamps Configure timezone Licenses (Administration \u003e Licensing):\nInstall iLO Advanced license key (required for full virtual media) License can be purchased or acquired from secondary market iLO Firmware Update Before production use, update iLO to latest version:\nDownload latest iLO 4 firmware from HPE Support Portal Administration \u003e Firmware \u003e Update Firmware Upload .bin file, apply update iLO will reboot automatically (system stays running) System ROM (BIOS/UEFI) Configuration Accessing RBSU Local: Press F9 during POST Remote: iLO Remote Console \u003e Power \u003e Momentary Press \u003e Press F9 when prompted Boot Mode Selection System Configuration \u003e BIOS/Platform Configuration (RBSU) \u003e Boot Mode:\nUEFI Mode (recommended for modern OS):\nSupports GPT partitions (\u003e2TB disks) Required for Secure Boot Better UEFI HTTP boot support IPv6 PXE boot support Legacy BIOS Mode:\nFor older OS or compatibility MBR partition tables only Traditional PXE boot Recommendation: Use UEFI Mode unless legacy compatibility required\nBoot Order Configuration System Configuration \u003e BIOS/Platform Configuration (RBSU) \u003e Boot Options \u003e UEFI Boot Order:\nRecommended order for network boot deployment:\nNetwork Boot: FlexibleLOM or PCIe NIC Internal Storage: RAID controller or disk Virtual Media: iLO virtual CD/DVD (for installation media) USB: For rescue/recovery Enable Network Boot:\nSystem Configuration \u003e BIOS/Platform Configuration (RBSU) \u003e Network Options \u003e Network Boot Set to “Enabled” Performance and Power Settings System Configuration \u003e BIOS/Platform Configuration (RBSU) \u003e Power Management:\nPower Regulator Mode:\nHP Dynamic Power Savings: Balanced power/performance (recommended for home lab) HP Static High Performance: Maximum performance, higher power draw HP Static Low Power: Minimize power, reduced performance OS Control: Let OS manage (e.g., Linux cpufreq) Collaborative Power Control: Disabled (for standalone servers)\nMinimum Processor Idle Power Core C-State: C6 (lower idle power)\nEnergy/Performance Bias: Balanced Performance (or Maximum Performance for compute workloads)\nRecommendation: Start with “Dynamic Power Savings” and adjust based on workload\nMemory Configuration Optimal Population (dual-CPU configuration):\nFor maximum performance, populate all channels before adding second DIMM per channel:\n64GB (8x 8GB):\nCPU1: Slots 1, 4, 7, 10 and CPU2: Slots 1, 4, 7, 10 Result: 4 channels per CPU, 1 DIMM per channel 128GB (8x 16GB):\nSame as above with 16GB DIMMs 192GB (12x 16GB):\nCPU1: Slots 1, 4, 7, 10, 2, 5 and CPU2: Slots 1, 4, 7, 10, 2, 5 Result: 4 channels per CPU, some with 2 DIMMs per channel 768GB (24x 32GB):\nAll slots populated Check Configuration: RBSU \u003e System Information \u003e Memory Information\nProcessor Options System Configuration \u003e BIOS/Platform Configuration (RBSU) \u003e Processor Options:\nIntel Hyperthreading: Enabled (recommended for most workloads)\nDoubles logical cores (e.g., 12-core CPU shows as 24 cores) Benefits most virtualization and multi-threaded workloads Disable only for specific security compliance (e.g., some cloud providers) Intel Virtualization Technology (VT-x): Enabled (required for hypervisors)\nIntel VT-d (IOMMU): Enabled (required for PCI passthrough, SR-IOV)\nTurbo Boost: Enabled (allows CPU to exceed base clock)\nCores Enabled: All (or reduce to lower power/heat if needed)\nIntegrated Devices System Configuration \u003e BIOS/Platform Configuration (RBSU) \u003e System Options \u003e Integrated Devices:\nEmbedded SATA Controller: Enabled (if using SATA drives) Embedded RAID Controller: Enabled (for Smart Array controllers) SR-IOV: Enabled (if using virtual network interfaces with VMs) Network Controller Options For each NIC (FlexibleLOM, PCIe):\nSystem Configuration \u003e BIOS/Platform Configuration (RBSU) \u003e Network Options \u003e [Adapter]:\nNetwork Boot: Enabled (for network boot on that NIC) PXE/iSCSI: Select PXE for standard network boot Link Speed: Auto-Negotiation (recommended) or force 1G/10G IPv4: Enabled (for IPv4 PXE boot) IPv6: Enabled (if using IPv6 PXE boot) Boot Order: Configure which NIC boots first if multiple are enabled\nSecure Boot Configuration System Configuration \u003e BIOS/Platform Configuration (RBSU) \u003e Boot Options \u003e Secure Boot:\nSecure Boot: Disabled (for unsigned boot loaders, custom kernels) Secure Boot: Enabled (for signed boot loaders, Windows, some Linux distros) Note: If using PXE with unsigned images (e.g., custom iPXE), Secure Boot must be disabled\nFirmware Updates Update System ROM to latest version:\nVia iLO:\niLO web \u003e Administration \u003e Firmware \u003e Update Firmware Upload System ROM .fwpkg or .bin file Server reboots automatically to apply Via Service Pack for ProLiant (SPP):\nDownload SPP ISO from HPE Support Portal Mount via iLO Virtual Media Boot server from SPP ISO Smart Update Manager (SUM) runs in Linux environment Select components to update (System ROM, iLO, controller firmware, NIC firmware) Apply updates, reboot Recommendation: Use SPP for comprehensive updates on initial setup, then iLO for individual component updates\nStorage Configuration Smart Array Controller Setup Access Smart Array Configuration During POST: Press F5 when “Smart Array Configuration Utility” message appears Via RBSU: System Configuration \u003e BIOS/Platform Configuration (RBSU) \u003e System Options \u003e ROM-Based Setup Utility \u003e Smart Array Configuration Create RAID Arrays Delete Existing Arrays (if reconfiguring):\nSelect controller \u003e Configuration \u003e Delete Array Confirm deletion (data loss warning) Create New Array:\nSelect controller \u003e Configuration \u003e Create Array Select physical drives to include Choose RAID level: RAID 0: Striping, no redundancy (maximum performance, maximum capacity) RAID 1: Mirroring (redundancy, half capacity, good for boot drives) RAID 5: Striping + parity (redundancy, n-1 capacity, balanced) RAID 6: Striping + double parity (dual-drive failure tolerance, n-2 capacity) RAID 10: Mirror + stripe (high performance + redundancy, half capacity) Configure spare drives (hot spares for automatic rebuild) Create logical drive Set bootable flag if boot drive Recommended Configurations:\nBoot/OS: 2x SSD in RAID 1 (redundancy, fast boot) Data (performance): 4-6x SSD in RAID 10 (fast, redundant) Data (capacity): 4-8x HDD in RAID 6 (capacity, dual-drive tolerance) Controller Settings Cache Settings:\nWrite Cache: Enabled (requires battery/flash-backed cache) Read Cache: Enabled No-Battery Write Cache: Disabled (data safety) or Enabled (performance, risk) Rebuild Priority: Medium or High (faster rebuild, may impact performance)\nSurface Scan Delay: 3-7 days (periodic integrity check)\nHBA Mode (Non-RAID) For software RAID (ZFS, mdadm, Ceph):\nAccess Smart Array Configuration (F5 during POST) Controller \u003e Configuration \u003e Enable HBA Mode Confirm (RAID arrays will be deleted) Reboot Note: Not all Smart Array controllers support HBA mode. Check compatibility. Alternative: Use separate LSI HBA in PCIe slot.\nNetwork Configuration for Boot DHCP Server Setup For PXE/UEFI network boot, configure DHCP server with appropriate options:\nISC DHCP Example (/etc/dhcp/dhcpd.conf):\n# Define subnet subnet 192.168.10.0 netmask 255.255.255.0 { range 192.168.10.100 192.168.10.200; option routers 192.168.10.1; option domain-name-servers 192.168.10.1; # PXE boot options next-server 192.168.10.5; # TFTP server IP # Differentiate UEFI vs BIOS if exists user-class and option user-class = \"iPXE\" { # iPXE boot script filename \"http://boot.example.com/boot.ipxe\"; } elsif option arch = 00:07 or option arch = 00:09 { # UEFI (x86-64) filename \"bootx64.efi\"; } else { # Legacy BIOS filename \"undionly.kpxe\"; } } # Static reservation for DL360 host dl360-01 { hardware ethernet xx:xx:xx:xx:xx:xx; # FlexibleLOM MAC fixed-address 192.168.10.50; option host-name \"dl360-01\"; } FlexibleLOM Configuration Configure FlexibleLOM NIC for network boot:\nRBSU \u003e Network Options \u003e FlexibleLOM Enable “Network Boot” Select PXE or iSCSI Configure IPv4/IPv6 as needed Set as first boot device in boot order Multi-NIC Boot Priority If multiple NICs have network boot enabled:\nRBSU \u003e Network Options \u003e Network Boot Order Drag/drop to prioritize NIC boot order First NIC in list attempts boot first Recommendation: Enable network boot on one NIC (typically FlexibleLOM port 1) to avoid confusion\nOperating System Installation Traditional Installation (Virtual Media) Download OS ISO (e.g., Ubuntu Server, ESXi, Proxmox) Upload ISO to HTTP/HTTPS server or local file iLO Remote Console \u003e Virtual Devices \u003e Image File CD-ROM/DVD Browse to ISO location, click “Insert Media” Set boot order to prioritize virtual media Reboot server, boot from virtual CD/DVD Proceed with OS installation Network Installation (PXE) See Network Boot Capabilities for detailed PXE/UEFI boot setup\nQuick workflow:\nConfigure DHCP server with PXE options Setup TFTP server with boot files Enable network boot in BIOS Reboot, server PXE boots Select OS installer from PXE menu Automated installation proceeds (Kickstart/Preseed/Ignition) Optimization for Specific Workloads Virtualization (ESXi, Proxmox, Hyper-V) BIOS Settings:\nHyperthreading: Enabled VT-x: Enabled VT-d: Enabled Power Management: Dynamic or OS Control Turbo Boost: Enabled Hardware:\nMaximum memory (384GB+ recommended) Fast storage (SSD RAID 10 for VM storage) 10GbE networking for VM traffic Configuration:\nPass through NICs to VMs (SR-IOV or PCI passthrough) Use storage controller in HBA mode for direct disk access to VM storage (ZFS, Ceph) Kubernetes/Container Platforms BIOS Settings:\nHyperthreading: Enabled VT-x/VT-d: Enabled (for nested virtualization, kata containers) Power Management: Dynamic or High Performance Hardware:\n128GB+ RAM for multi-tenant workloads Fast local NVMe/SSD for container image cache and ephemeral storage 10GbE for pod networking OS Recommendations:\nTalos Linux: Network-bootable, immutable k8s OS Flatcar Container Linux: Auto-updating, minimal OS Ubuntu Server: Broad compatibility, snap/docker native Storage Server (NAS, SAN) BIOS Settings:\nDisable Hyperthreading (slight performance improvement for ZFS) VT-d: Enabled (if passing through HBA to VM) Power Management: High Performance Hardware:\nMaximum drive bays (8-10 SFF) HBA mode or separate LSI HBA controller 10GbE or bonded 1GbE for network storage traffic ECC memory (critical for ZFS) Software:\nTrueNAS SCALE (Linux-based, k8s apps) OpenMediaVault (Debian-based, plugins) Ubuntu + ZFS (custom setup) Compute/HPC Workloads BIOS Settings:\nHyperthreading: Depends on workload (test both) Turbo Boost: Enabled Power Management: Maximum Performance C-States: Disabled (reduce latency) Hardware:\nHigh core count CPUs (E5-2680 v4, 2690 v4) Maximum memory bandwidth (populate all channels) Fast local scratch storage (NVMe) Monitoring and Maintenance iLO Health Monitoring Information \u003e System Information:\nCPU temperature and status Memory status Drive status (via controller) Fan speeds PSU status Overall system health LED status Alerting (Administration \u003e Alerting):\nConfigure email alerts for: Fan failures Temperature warnings Drive failures Memory errors PSU failures Set up SNMP traps for integration with monitoring systems (Nagios, Zabbix, Prometheus) Integrated Management Log (IML) Information \u003e Integrated Management Log:\nView hardware events and errors Filter by severity (Informational, Caution, Critical) Export log for troubleshooting Regular Checks:\nReview IML weekly for early warning signs Address caution-level events before they become critical Firmware Update Cadence Recommendation:\niLO: Update quarterly or when security advisories released System ROM: Update annually or for bug fixes Storage Controller: Update when issues arise or annually NIC Firmware: Update when issues arise Method: Use SPP for annual comprehensive updates, iLO web interface for individual component updates\nPhysical Maintenance Monthly:\nCheck fan noise (increased noise may indicate clogged air filters or failing fan) Verify PSU and drive LEDs (no amber lights) Check iLO for alerts Quarterly:\nClean air filters (if accessible, depends on rack airflow) Verify backup of iLO configuration Test iLO Virtual Media functionality Annually:\nUpdate all firmware via SPP Verify RAID battery/flash-backed cache status Review and update BIOS settings as workload evolves Troubleshooting Common Issues Server Won’t Power On Check PSU power cords connected Verify PSU LEDs indicate power Press iLO power button via web interface Check iLO IML for power-related errors Reseat PSUs, check for blown fuses POST Errors Memory Errors:\nReseat memory DIMMs Test with minimal configuration (1 DIMM per CPU) Replace failing DIMMs identified in POST CPU Errors:\nVerify heatsink properly seated Check thermal paste application Reseat CPU (careful with pins) Drive Errors:\nCheck drive connection to caddy Verify controller recognizes drive Replace failing drive No Network Boot See Network Boot Troubleshooting for detailed diagnostics\nQuick checks:\nVerify NIC link light Confirm network boot enabled in BIOS Check DHCP server logs for PXE request Test TFTP server accessibility iLO Not Accessible Check physical Ethernet connection to iLO port Verify switch port active Reset iLO: Press and hold iLO NMI button (rear) for 5 seconds Factory reset iLO via jumper (see maintenance guide) Check iLO firmware version, update if outdated High Fan Noise Check ambient temperature (\u003c25°C recommended) Verify airflow not blocked (front/rear clearance) Clean dust from intake (compressed air) Check iLO temperature sensors for elevated temps Lower CPU TDP if temperatures excessive (lower power CPUs) Verify all fans operational (replace failed fans) Security Hardening iLO Security Change Default Credentials: Immediately on first boot Disable Unused Services: SSH, IPMI if not needed Use HTTPS Only: Disable HTTP (Administration \u003e Network \u003e HTTP Port) Network Isolation: Dedicated management VLAN, firewall iLO access Update Firmware: Apply security patches promptly Account Management: Use separate accounts, least privilege BIOS/UEFI Security BIOS Password: Set administrator password (RBSU \u003e System Options \u003e BIOS Admin Password) Secure Boot: Enable if using signed boot loaders Boot Order Lock: Prevent unauthorized boot device changes TPM: Enable if using BitLocker or LUKS disk encryption Operating System Security Minimal Installation: Install only required packages Firewall: Enable host firewall (iptables, firewalld, ufw) SSH Hardening: Key-based auth, disable password auth, non-standard port Automatic Updates: Enable for security patches Monitoring: Deploy intrusion detection (fail2ban, OSSEC) Conclusion Proper configuration of the HP ProLiant DL360 Gen9 ensures optimal performance, reliability, and manageability for home lab and production deployments. The combination of UEFI boot capabilities, iLO remote management, and flexible hardware configuration makes the DL360 Gen9 a versatile platform for virtualization, containerization, storage, and compute workloads.\nKey takeaways:\nUpdate firmware early (iLO, System ROM, controllers) Configure iLO for remote management and monitoring Choose boot mode (UEFI recommended) and configure network boot appropriately Optimize BIOS settings for specific workload (virtualization, storage, compute) Implement security hardening (iLO, BIOS, OS) Establish monitoring and maintenance schedule For network boot-specific configuration, refer to the Network Boot Capabilities guide.\n","categories":"","description":"Setup, optimization, and configuration recommendations for HP ProLiant DL360 Gen9 in home lab environments\n","excerpt":"Setup, optimization, and configuration recommendations for HP ProLiant …","ref":"/infra/pr-preview/pr-592/rd/analysis/hp_dl360_gen9/configuration/","tags":"","title":"Configuration Guide"},{"body":"This section contains detailed analysis of the HP ProLiant DL360 Gen9 server platform, including hardware specifications, network boot capabilities, and configuration guidance for home lab deployments.\nOverview The HP ProLiant DL360 Gen9 is a 1U rack-mountable server released by HPE as part of their Generation 9 (Gen9) product line, introduced in 2014. It’s a popular choice for home labs due to its balance of performance, density, and relative power efficiency compared to earlier generations.\nKey Features Form Factor: 1U rack-mountable Processor Support: Dual Intel Xeon E5-2600 v3/v4 processors (Haswell/Broadwell) Memory: Up to 768GB DDR4 RAM (24 DIMM slots) Storage: Flexible SFF/LFF drive configurations Network: Integrated quad-port 1GbE or 10GbE FlexibleLOM options Management: iLO 4 (Integrated Lights-Out) with remote KVM and virtual media Boot Options: UEFI and Legacy BIOS support with extensive network boot capabilities Documentation Sections Network Boot Capabilities - Detailed analysis of PXE, iPXE, and UEFI HTTP boot support Hardware Specifications - Complete hardware configuration details Configuration Guide - Setup and optimization recommendations ","categories":"","description":"Technical analysis of HP ProLiant DL360 Gen9 server capabilities with focus on network boot support\n","excerpt":"Technical analysis of HP ProLiant DL360 Gen9 server capabilities with …","ref":"/infra/pr-preview/pr-592/rd/analysis/hp_dl360_gen9/","tags":"","title":"HP ProLiant DL360 Gen9 Analysis"},{"body":"Components Router Ubiquiti Dream Machine Pro\nSwitch Ubiquiti 48 port PoE\nServers hp-dl360 - HP ProLiant DL360 Gen 9 CPU: 2 x Intel(R) Xeon(R) CPU E5-2620 v3 Mem: 8 x 16GB DDR4-2133 RDIMM Boot: 1 x 250GB Samsung 970 Evo Plus M.2-2280 PCIe 3.0 NVME Storage: 4 x 1TB Samsung 870 Evo Power: 2 x HP 500W Flex Slot Platinum Power Supply Accelerator: N/A camd-ryzen5 - Custom PC CPU: 1 x Ryzen 5 5600X Mem: 2 x 8GB DDR4-3600 CL19 Boot: N/A Storage: 1 x 1TB Samsung 970 Evo Plus M.2-2280 PCIe 3.0 NVME Power: 1 x EVGA SuperNOVA 750 GT 750 W 80+ Gold Accelerator: 1 x Gigabyte EAGLE Radeon RX 6700 XT 12 GB Network Topology flowchart TB router@{ label: \"Router\", shape: circle } switch@{ label: \"Switch\", shape: circle } server_a@{ label: \"hp-dl360\", shape: rect } server_b@{ label: \"hp-dl360\", shape: rect } server_c@{ label: \"camd-ryzen5\", shape: rect } subgraph one [IPMI VLAN] server_a_ipmi@{ label: \"hp-dl360 IPMI Interface\", shape: terminal } server_b_ipmi@{ label: \"hp-dl360 IPMI Interface\", shape: terminal } end subgraph two [Homelab VLAN] server_a_port2@{ label: \"hp-dl360 Port 2\", shape: terminal } server_a_port3@{ label: \"hp-dl360 Port 3\", shape: terminal } server_a_port4@{ label: \"hp-dl360 Port 4\", shape: terminal } server_b_port2@{ label: \"hp-dl360 Port 2\", shape: terminal } server_b_port3@{ label: \"hp-dl360 Port 3\", shape: terminal } server_b_port4@{ label: \"hp-dl360 Port 4\", shape: terminal } server_c_port@{ label: \"camd-ryzen5 Port\", shape: terminal } end subgraph three [Network Boot VLAN] server_a_port1@{ label: \"hp-dl360 Port 1\", shape: terminal } server_b_port1@{ label: \"hp-dl360 Port 1\", shape: terminal } end router --- switch switch --- server_a_ipmi switch --- server_a_port1 switch --- server_a_port2 switch --- server_a_port3 switch --- server_a_port4 server_a_ipmi --- server_a server_a_port1 --- server_a server_a_port2 --- server_a server_a_port3 --- server_a server_a_port4 --- server_a switch --- server_b_ipmi switch --- server_b_port1 switch --- server_b_port2 switch --- server_b_port3 switch --- server_b_port4 server_b_ipmi --- server_b server_b_port1 --- server_b server_b_port2 --- server_b server_b_port3 --- server_b server_b_port4 --- server_b switch --- server_c_port server_c_port --- server_c IPMI VLAN This VLAN groups the IPMI or IPMI-like interfaces for all servers in the homelab so that each machine can be remotely accessed. Most importantly, this access can be restricted via VLAN firewall rules.\nNetwork Boot VLAN This an exclusive VLAN for configuring the servers to network boot. This VLAN is covered in more depth on the specific Network Boot documentation.\nHomelab VLAN This is the main VLAN for servers to communicate with one another, as well as, the external internet.\n","categories":"","description":"","excerpt":"Components Router Ubiquiti Dream Machine Pro\nSwitch Ubiquiti 48 port …","ref":"/infra/pr-preview/pr-592/physical_infrastructure/","tags":"","title":"Physical Infrastructure"},{"body":"Architecture Decision Records (ADRs) This section contains architectural decision records that document the key design choices made. Each ADR follows the MADR 4.0.0 format and includes:\nContext and problem statement Decision drivers and constraints Considered options with pros and cons Decision outcome and rationale Consequences (positive and negative) Confirmation methods ADR Categories ADRs are classified into three categories:\nStrategic - High-level architectural decisions affecting the entire system (frameworks, authentication strategies, cross-cutting patterns). Use for foundational technology choices. User Journey - Decisions solving specific user journey problems. More tactical than strategic, but still architectural. Use when evaluating approaches to implement user-facing features. API Design - API endpoint implementation decisions (pagination, filtering, bulk operations). Use for significant API design trade-offs that warrant documentation. Status Values Each ADR has a status that reflects its current state:\nproposed - Decision is under consideration accepted - Decision has been approved and should be implemented rejected - Decision was considered but not approved deprecated - Decision is no longer relevant or has been superseded superseded by ADR-XXXX - Decision has been replaced by a newer ADR These records provide historical context for architectural decisions and help ensure consistency across the platform.\n","categories":"","description":"Documentation of architectural decisions made using MADR 4.0.0 standard\n","excerpt":"Documentation of architectural decisions made using MADR 4.0.0 …","ref":"/infra/pr-preview/pr-592/rd/adrs/","tags":"","title":"Architecture Decision Records"},{"body":"","categories":"","description":"","excerpt":"","ref":"/infra/pr-preview/pr-592/categories/","tags":"","title":"Categories"},{"body":" Note Network booting only applies to following machines in the homelab:\nhp-dl360 Architecture flowchart LR subgraph homelab [Homelab] server@{ label: \"Server\", shape: rect } switch@{ label: \"Switch\", shape: circle } router@{ label: \"Router\", shape: circle } end subgraph cloud [Public Cloud] wiregaurd@{ label: \"Wiregaurd\", shape: rect } matchbox@{ label: \"Matchbox\", shape: rect } end server --- switch switch --- router router --- wiregaurd wiregaurd --- matchbox Homelab Setting up Network Boot VLAN We begin by creating a VLAN on the router and assign by MAC address the first port of each server to it.\nInitializing Wiregaurd node One feature of the Router is that it can act as either a Wiregaurd server or client. In this instance, the client implementation is initialized and all traffic on the Network Boot VLAN will be assigned to go through the Wiregaurd connection.\nPublic Cloud flowchart LR gateway@{ label: \"External Passthrough Network Load Balancer\", shape: rect } subgraph k8s [Kubernetes] wiregaurd@{ label: \"Wiregaurd\", shape: rect } matchbox@{ label: \"Matchbox\", shape: rect } end gateway --- wiregaurd wiregaurd --- matchbox Within a public cloud (e.g. AWS, Azure, GCP), a Kubernetes cluster will be instantiated.\nKubernetes Architecture flowchart LR subgraph ns [Network Boot Namespace] direction LR wiregaurd_service@{ label: \"Wiregaurd Service (LoadBalancer)\", shape: rect } wiregaurd@{ label: \"Wiregaurd Daemon Set\", shape: rect } wiregaurd_config@{ label: \"Wiregaurd Config Secret\", shape: cyl } matchbox@{ label: \"Matchbox Deployment\", shape: rect } matchbox_config@{ label: \"Matchbox Config Volume\", shape: cyl } end wiregaurd_service --- wiregaurd wiregaurd --- wiregaurd_config wiregaurd --- matchbox matchbox --- matchbox_config Boot Sequence sequenceDiagram participant server as Server participant router as Router participant wiregaurd as Wiregaurd participant matchbox as Matchbox server -\u003e\u003e router: UEFI HTTP boot request router -\u003e\u003e wiregaurd: Encrypted UEFI HTTP boot request wiregaurd -\u003e\u003e matchbox: UEFI HTTP boot request matchbox -\u003e\u003e wiregaurd: UEFI Executable Image wiregaurd -\u003e\u003e router: UEFI Executable Image router -\u003e\u003e server: UEFI Executable Image ","categories":"","description":"","excerpt":" Note Network booting only applies to following machines in the …","ref":"/infra/pr-preview/pr-592/physical_infrastructure/network_boot/","tags":"","title":"Network Boot"},{"body":"","categories":"","description":"","excerpt":"","ref":"/infra/pr-preview/pr-592/rd/","tags":"","title":"Research and Development"},{"body":"","categories":"","description":"","excerpt":"","ref":"/infra/pr-preview/pr-592/tags/","tags":"","title":"Tags"},{"body":"High Level Diagram flowchart LR internet[\"Internet\"] cloudflare[\"Cloudflare\"] homelab[\"Home Lab\"] cloud[\"Public Cloud\"] internet --- cloudflare cloudflare ---|mTLS| homelab homelab ---|Wiregaurd| cloud Cloudflare Cloudflare is leveraged as both a DNS provider and HTTP(s) proxy to services hosted on the Home Lab. The primary goals of leveraging Cloudflare are the following:\nDDoS protection for all services, especially those self-hosted via the Home Lab. Home IP address hiding via proxied records aka, proxy HTTP/HTTP(s) from Internet through Cloudflare before getting to Home Lab Home Lab Any and all computer hardware primarily dedicated to self-hosting services, content, and experiments is often referred to as a home lab or home server in the context of a single server.\nPublic Cloud A public cloud provider (e.g. AWS, GCP, Azure) is used to host services needed by the Home Lab for various tasks like network booting.\n","categories":"","description":"","excerpt":"High Level Diagram flowchart LR internet[\"Internet\"] …","ref":"/infra/pr-preview/pr-592/","tags":"","title":"Zaba505's Home Lab"}]